warning: CUDA version 12.1 is only partially supported
warning: CUDA version 12.1 is only partially supported
warning: we failed to emit call to builtin function __builtin_inff
[ict-debug] driver.cc: After return 5, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z33__device_stub__log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    call @_Z18log_softmax_kernelPKfPfii(%arg0, %arg1, %arg2, %arg3) : (memref<?xf32>, memref<?xf32>, i32, i32) -> ()
    return
  }
  func.func private @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = call @__builtin_inff() : () -> f32
    %3 = arith.negf %2 : f32
    %4 = arith.index_cast %arg3 : i32 to index
    %5 = arith.muli %1, %arg3 : i32
    %6 = arith.index_cast %5 : i32 to index
    %7 = affine.for %arg4 = 0 to %4 iter_args(%arg5 = %3) -> (f32) {
      %23 = affine.load %arg0[%arg4 + symbol(%6)] : memref<?xf32>
      %24 = arith.maxnumf %arg5, %23 : f32
      affine.yield %24 : f32
    }
    nvvm.barrier0
    %8 = arith.index_cast %arg3 : i32 to index
    %9 = arith.muli %1, %arg3 : i32
    %10 = arith.index_cast %9 : i32 to index
    %11 = affine.for %arg4 = 0 to %8 iter_args(%arg5 = %cst) -> (f32) {
      %23 = affine.load %arg0[%arg4 + symbol(%10)] : memref<?xf32>
      %24 = arith.subf %23, %7 : f32
      %25 = math.exp %24 : f32
      %26 = arith.addf %arg5, %25 : f32
      affine.yield %26 : f32
    }
    %12 = math.log %11 : f32
    nvvm.barrier0
    %13 = gpu.thread_id  x
    %14 = arith.muli %1, %arg3 : i32
    %15 = arith.index_cast %14 : i32 to index
    %16 = arith.index_cast %14 : i32 to index
    %17 = gpu.block_dim  x
    %18 = arith.index_cast %arg3 : i32 to index
    %19 = arith.subi %18, %13 : index
    %20 = arith.subi %17, %c1 : index
    %21 = arith.addi %20, %19 : index
    %22 = arith.divui %21, %17 : index
    affine.for %arg4 = 0 to %22 {
      %23 = affine.load %arg0[%arg4 * symbol(%17) + symbol(%15) + symbol(%13)] : memref<?xf32>
      %24 = arith.subf %23, %7 : f32
      %25 = arith.subf %24, %12 : f32
      affine.store %25, %arg1[%arg4 * symbol(%17) + symbol(%16) + symbol(%13)] : memref<?xf32>
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
[ict-debug] driver.cc: After return 5, module: end

[ict-debug] driver.cc: After return 6, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = call @__builtin_inff() : () -> f32
    %3 = arith.negf %2 : f32
    %4 = arith.index_cast %arg3 : i32 to index
    %5 = arith.muli %1, %arg3 : i32
    %6 = arith.index_cast %5 : i32 to index
    %7 = affine.for %arg4 = 0 to %4 iter_args(%arg5 = %3) -> (f32) {
      %16 = affine.load %arg0[%arg4 + symbol(%6)] : memref<?xf32>
      %17 = arith.maxnumf %arg5, %16 : f32
      affine.yield %17 : f32
    }
    nvvm.barrier0
    %8 = affine.for %arg4 = 0 to %4 iter_args(%arg5 = %cst) -> (f32) {
      %16 = affine.load %arg0[%arg4 + symbol(%6)] : memref<?xf32>
      %17 = arith.subf %16, %7 : f32
      %18 = math.exp %17 : f32
      %19 = arith.addf %arg5, %18 : f32
      affine.yield %19 : f32
    }
    %9 = math.log %8 : f32
    nvvm.barrier0
    %10 = gpu.thread_id  x
    %11 = gpu.block_dim  x
    %12 = arith.subi %4, %10 : index
    %13 = arith.subi %11, %c1 : index
    %14 = arith.addi %13, %12 : index
    %15 = arith.divui %14, %11 : index
    affine.for %arg4 = 0 to %15 {
      %16 = affine.load %arg0[%arg4 * symbol(%11) + symbol(%6) + symbol(%10)] : memref<?xf32>
      %17 = arith.subf %16, %7 : f32
      %18 = arith.subf %17, %9 : f32
      affine.store %18, %arg1[%arg4 * symbol(%11) + symbol(%6) + symbol(%10)] : memref<?xf32>
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
[ict-debug] driver.cc: After return 6, module: end

WrapAndReplaceBarrierPass::runOnOperation(): before execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = call @__builtin_inff() : () -> f32
    %3 = arith.negf %2 : f32
    %4 = arith.index_cast %arg3 : i32 to index
    %5 = arith.muli %1, %arg3 : i32
    %6 = arith.index_cast %5 : i32 to index
    %7 = scf.for %arg4 = %c0 to %4 step %c1 iter_args(%arg5 = %3) -> (f32) {
      %16 = arith.addi %arg4, %6 : index
      %17 = memref.load %arg0[%16] : memref<?xf32>
      %18 = arith.maxnumf %arg5, %17 : f32
      scf.yield %18 : f32
    }
    nvvm.barrier0
    %8 = scf.for %arg4 = %c0 to %4 step %c1 iter_args(%arg5 = %cst) -> (f32) {
      %16 = arith.addi %arg4, %6 : index
      %17 = memref.load %arg0[%16] : memref<?xf32>
      %18 = arith.subf %17, %7 : f32
      %19 = math.exp %18 : f32
      %20 = arith.addf %arg5, %19 : f32
      scf.yield %20 : f32
    }
    %9 = math.log %8 : f32
    nvvm.barrier0
    %10 = gpu.thread_id  x
    %11 = gpu.block_dim  x
    %12 = arith.subi %4, %10 : index
    %13 = arith.subi %11, %c1 : index
    %14 = arith.addi %13, %12 : index
    %15 = arith.divui %14, %11 : index
    scf.for %arg4 = %c0 to %15 step %c1 {
      %16 = arith.muli %arg4, %11 : index
      %17 = arith.addi %16, %6 : index
      %18 = arith.addi %17, %10 : index
      %19 = memref.load %arg0[%18] : memref<?xf32>
      %20 = arith.subf %19, %7 : f32
      %21 = arith.subf %20, %9 : f32
      memref.store %21, %arg1[%18] : memref<?xf32>
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
WrapAndReplaceBarrierPass::runOnOperation(): before execute: end
WrapAndReplaceBarrierPass::runOnOperation(): after execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %c1_1 = arith.constant 1 : index
      %cst = arith.constant 0.000000e+00 : f32
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = func.call @__builtin_inff() : () -> f32
      %3 = arith.negf %2 : f32
      %4 = arith.index_cast %arg3 : i32 to index
      %5 = arith.muli %1, %arg3 : i32
      %6 = arith.index_cast %5 : i32 to index
      %7 = scf.for %arg5 = %c0_0 to %4 step %c1_1 iter_args(%arg6 = %3) -> (f32) {
        %15 = arith.addi %arg5, %6 : index
        %16 = memref.load %arg0[%15] : memref<?xf32>
        %17 = arith.maxnumf %arg6, %16 : f32
        scf.yield %17 : f32
      }
      "polygeist.barrier"(%arg4) : (index) -> ()
      %8 = scf.for %arg5 = %c0_0 to %4 step %c1_1 iter_args(%arg6 = %cst) -> (f32) {
        %15 = arith.addi %arg5, %6 : index
        %16 = memref.load %arg0[%15] : memref<?xf32>
        %17 = arith.subf %16, %7 : f32
        %18 = math.exp %17 : f32
        %19 = arith.addf %arg6, %18 : f32
        scf.yield %19 : f32
      }
      %9 = math.log %8 : f32
      "polygeist.barrier"(%arg4) : (index) -> ()
      %10 = gpu.block_dim  x
      %11 = arith.subi %4, %arg4 : index
      %12 = arith.subi %10, %c1_1 : index
      %13 = arith.addi %12, %11 : index
      %14 = arith.divui %13, %10 : index
      scf.for %arg5 = %c0_0 to %14 step %c1_1 {
        %15 = arith.muli %arg5, %10 : index
        %16 = arith.addi %15, %6 : index
        %17 = arith.addi %16, %arg4 : index
        %18 = memref.load %arg0[%17] : memref<?xf32>
        %19 = arith.subf %18, %7 : f32
        %20 = arith.subf %19, %9 : f32
        memref.store %20, %arg1[%17] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
WrapAndReplaceBarrierPass::runOnOperation(): after execute: end
[ict-debug] driver.cc: After return 7, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %c1_1 = arith.constant 1 : index
      %cst = arith.constant 0.000000e+00 : f32
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = func.call @__builtin_inff() : () -> f32
      %3 = arith.negf %2 : f32
      %4 = arith.index_cast %arg3 : i32 to index
      %5 = arith.muli %1, %arg3 : i32
      %6 = arith.index_cast %5 : i32 to index
      %7 = scf.for %arg5 = %c0_0 to %4 step %c1_1 iter_args(%arg6 = %3) -> (f32) {
        %15 = arith.addi %arg5, %6 : index
        %16 = memref.load %arg0[%15] : memref<?xf32>
        %17 = arith.maxnumf %arg6, %16 : f32
        scf.yield %17 : f32
      }
      "polygeist.barrier"(%arg4) : (index) -> ()
      %8 = scf.for %arg5 = %c0_0 to %4 step %c1_1 iter_args(%arg6 = %cst) -> (f32) {
        %15 = arith.addi %arg5, %6 : index
        %16 = memref.load %arg0[%15] : memref<?xf32>
        %17 = arith.subf %16, %7 : f32
        %18 = math.exp %17 : f32
        %19 = arith.addf %arg6, %18 : f32
        scf.yield %19 : f32
      }
      %9 = math.log %8 : f32
      "polygeist.barrier"(%arg4) : (index) -> ()
      %10 = gpu.block_dim  x
      %11 = arith.subi %4, %arg4 : index
      %12 = arith.subi %10, %c1_1 : index
      %13 = arith.addi %12, %11 : index
      %14 = arith.divui %13, %10 : index
      scf.for %arg5 = %c0_0 to %14 step %c1_1 {
        %15 = arith.muli %arg5, %10 : index
        %16 = arith.addi %15, %6 : index
        %17 = arith.addi %16, %arg4 : index
        %18 = memref.load %arg0[%17] : memref<?xf32>
        %19 = arith.subf %18, %7 : f32
        %20 = arith.subf %19, %9 : f32
        memref.store %20, %arg1[%17] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
[ict-debug] driver.cc: After return 7, module: end

[ict-debug] driver.cc: Before my pass process:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<32xf32>
    %alloca_0 = memref.alloca() : memref<32xf32>
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.index_cast %arg3 : i32 to index
    %3 = arith.muli %1, %arg3 : i32
    %4 = arith.index_cast %3 : i32 to index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %17 = func.call @__builtin_inff() : () -> f32
      %18 = arith.negf %17 : f32
      %19 = scf.for %arg5 = %c0 to %2 step %c1 iter_args(%arg6 = %18) -> (f32) {
        %20 = arith.addi %arg5, %4 : index
        %21 = memref.load %arg0[%20] : memref<?xf32>
        %22 = arith.maxnumf %arg6, %21 : f32
        scf.yield %22 : f32
      }
      memref.store %19, %alloca[%arg4] : memref<32xf32>
      scf.yield
    }
    %5 = gpu.block_id  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %6, %arg3 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.index_cast %arg3 : i32 to index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %17 = memref.load %alloca[%arg4] : memref<32xf32>
      %18 = scf.for %arg5 = %c0 to %9 step %c1 iter_args(%arg6 = %cst) -> (f32) {
        %19 = arith.addi %arg5, %8 : index
        %20 = memref.load %arg0[%19] : memref<?xf32>
        %21 = arith.subf %20, %17 : f32
        %22 = math.exp %21 : f32
        %23 = arith.addf %arg6, %22 : f32
        scf.yield %23 : f32
      }
      memref.store %18, %alloca_0[%arg4] : memref<32xf32>
      scf.yield
    }
    %10 = arith.index_cast %arg3 : i32 to index
    %11 = gpu.block_id  x
    %12 = arith.index_cast %11 : index to i32
    %13 = arith.muli %12, %arg3 : i32
    %14 = arith.index_cast %13 : i32 to index
    %15 = gpu.block_dim  x
    %16 = arith.subi %15, %c1 : index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %17 = memref.load %alloca_0[%arg4] : memref<32xf32>
      %18 = math.log %17 : f32
      %19 = memref.load %alloca[%arg4] : memref<32xf32>
      %20 = arith.subi %10, %arg4 : index
      %21 = arith.addi %16, %20 : index
      %22 = arith.divui %21, %15 : index
      scf.for %arg5 = %c0 to %22 step %c1 {
        %23 = arith.muli %arg5, %15 : index
        %24 = arith.addi %23, %14 : index
        %25 = arith.addi %24, %arg4 : index
        %26 = memref.load %arg0[%25] : memref<?xf32>
        %27 = arith.subf %26, %19 : f32
        %28 = arith.subf %27, %18 : f32
        memref.store %28, %arg1[%25] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
}
[ict-debug] driver.cc: Before my pass process: end

[ict-debug] driver.cc: vectorizeSize = 1

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32>
      %alloca_0 = memref.alloca() : memref<32xf32>
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = arith.index_cast %arg3 : i32 to index
      %3 = arith.muli %1, %arg3 : i32
      %4 = arith.index_cast %3 : i32 to index
      scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
        %7 = func.call @__builtin_inff() : () -> f32
        %8 = arith.negf %7 : f32
        %9 = scf.for %arg5 = %c0 to %2 step %c1 iter_args(%arg6 = %8) -> (f32) {
          %10 = arith.addi %arg5, %4 : index
          %11 = memref.load %arg0[%10] : memref<?xf32>
          %12 = arith.maxnumf %arg6, %11 : f32
          scf.yield %12 : f32
        }
        memref.store %9, %alloca[%arg4] : memref<32xf32>
        scf.yield
      }
      scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
        %7 = memref.load %alloca[%arg4] : memref<32xf32>
        %8 = scf.for %arg5 = %c0 to %2 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %9 = arith.addi %arg5, %4 : index
          %10 = memref.load %arg0[%9] : memref<?xf32>
          %11 = arith.subf %10, %7 : f32
          %12 = math.exp %11 : f32
          %13 = arith.addf %arg6, %12 : f32
          scf.yield %13 : f32
        }
        memref.store %8, %alloca_0[%arg4] : memref<32xf32>
        scf.yield
      }
      %5 = gpu.block_dim  x
      %6 = arith.subi %5, %c1 : index
      scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
        %7 = memref.load %alloca_0[%arg4] : memref<32xf32>
        %8 = math.log %7 : f32
        %9 = memref.load %alloca[%arg4] : memref<32xf32>
        %10 = arith.subi %2, %arg4 : index
        %11 = arith.addi %6, %10 : index
        %12 = arith.divui %11, %5 : index
        scf.for %arg5 = %c0 to %12 step %c1 {
          %13 = arith.muli %arg5, %5 : index
          %14 = arith.addi %13, %4 : index
          %15 = arith.addi %14, %arg4 : index
          %16 = memref.load %arg0[%15] : memref<?xf32>
          %17 = arith.subf %16, %9 : f32
          %18 = arith.subf %17, %8 : f32
          memref.store %18, %arg1[%15] : memref<?xf32>
        }
        scf.yield
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute: end

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32>
      %alloca_0 = memref.alloca() : memref<32xf32>
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = arith.index_cast %arg3 : i32 to index
      %3 = arith.muli %1, %arg3 : i32
      %4 = arith.index_cast %3 : i32 to index
      %c1_1 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_1 {
        %6 = func.call @__builtin_inff() : () -> f32
        %7 = arith.negf %6 : f32
        %8 = scf.for %arg5 = %c0 to %2 step %c1 iter_args(%arg6 = %7) -> (f32) {
          %9 = arith.addi %arg5, %4 : index
          %10 = memref.load %arg0[%9] : memref<?xf32>
          %11 = arith.maxnumf %arg6, %10 : f32
          scf.yield %11 : f32
        }
        memref.store %8, %alloca[%arg4] : memref<32xf32>
      }
      %c1_2 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_2 {
        %6 = memref.load %alloca[%arg4] : memref<32xf32>
        %7 = scf.for %arg5 = %c0 to %2 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %8 = arith.addi %arg5, %4 : index
          %9 = memref.load %arg0[%8] : memref<?xf32>
          %10 = arith.subf %9, %6 : f32
          %11 = math.exp %10 : f32
          %12 = arith.addf %arg6, %11 : f32
          scf.yield %12 : f32
        }
        memref.store %7, %alloca_0[%arg4] : memref<32xf32>
      }
      %c32_3 = arith.constant 32 : index
      %5 = arith.subi %c32_3, %c1 : index
      %c1_4 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_4 {
        %6 = memref.load %alloca_0[%arg4] : memref<32xf32>
        %7 = math.log %6 : f32
        %8 = memref.load %alloca[%arg4] : memref<32xf32>
        %9 = arith.subi %2, %arg4 : index
        %10 = arith.addi %5, %9 : index
        %11 = arith.divui %10, %c32_3 : index
        scf.for %arg5 = %c0 to %11 step %c1 {
          %12 = arith.muli %arg5, %c32_3 : index
          %13 = arith.addi %12, %4 : index
          %14 = arith.addi %13, %arg4 : index
          %15 = memref.load %arg0[%14] : memref<?xf32>
          %16 = arith.subf %15, %8 : f32
          %17 = arith.subf %16, %7 : f32
          memref.store %17, %arg1[%14] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize: end

[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca = memref.alloca() : memref<32xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca = memref.alloca() : memref<32xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.index_cast %arg3 : i32 to index
      %4 = arith.muli %2, %arg3 : i32
      %5 = arith.index_cast %4 : i32 to index
      %c1_1 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_1 {
        %7 = func.call @__builtin_inff() : () -> f32
        %8 = arith.negf %7 : f32
        %9 = scf.for %arg5 = %c0 to %3 step %c1 iter_args(%arg6 = %8) -> (f32) {
          %10 = arith.addi %arg5, %5 : index
          %11 = memref.load %arg0[%10] : memref<?xf32>
          %12 = arith.maxnumf %arg6, %11 : f32
          scf.yield %12 : f32
        }
        memref.store %9, %alloca[%arg4] : memref<32xf32, 5>
      }
      %c1_2 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_2 {
        %7 = memref.load %alloca[%arg4] : memref<32xf32, 5>
        %8 = scf.for %arg5 = %c0 to %3 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %9 = arith.addi %arg5, %5 : index
          %10 = memref.load %arg0[%9] : memref<?xf32>
          %11 = arith.subf %10, %7 : f32
          %12 = math.exp %11 : f32
          %13 = arith.addf %arg6, %12 : f32
          scf.yield %13 : f32
        }
        memref.store %8, %alloca_0[%arg4] : memref<32xf32, 5>
      }
      %c32_3 = arith.constant 32 : index
      %6 = arith.subi %c32_3, %c1 : index
      %c1_4 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_4 {
        %7 = memref.load %alloca_0[%arg4] : memref<32xf32, 5>
        %8 = math.log %7 : f32
        %9 = memref.load %alloca[%arg4] : memref<32xf32, 5>
        %10 = arith.subi %3, %arg4 : index
        %11 = arith.addi %6, %10 : index
        %12 = arith.divui %11, %c32_3 : index
        scf.for %arg5 = %c0 to %12 step %c1 {
          %13 = arith.muli %arg5, %c32_3 : index
          %14 = arith.addi %13, %5 : index
          %15 = arith.addi %14, %arg4 : index
          %16 = memref.load %arg0[%15] : memref<?xf32>
          %17 = arith.subf %16, %9 : f32
          %18 = arith.subf %17, %8 : f32
          memref.store %18, %arg1[%15] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca_0 = memref.alloca() : memref<32xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca_0 = memref.alloca() : memref<32xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<32xf32, 5>
      %1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %4 = arith.index_cast %arg3 : i32 to index
      %5 = arith.muli %3, %arg3 : i32
      %6 = arith.index_cast %5 : i32 to index
      %c1_1 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_1 {
        %8 = func.call @__builtin_inff() : () -> f32
        %9 = arith.negf %8 : f32
        %10 = scf.for %arg5 = %c0 to %4 step %c1 iter_args(%arg6 = %9) -> (f32) {
          %11 = arith.addi %arg5, %6 : index
          %12 = memref.load %arg0[%11] : memref<?xf32>
          %13 = arith.maxnumf %arg6, %12 : f32
          scf.yield %13 : f32
        }
        memref.store %10, %alloca[%arg4] : memref<32xf32, 5>
      }
      %c1_2 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_2 {
        %8 = memref.load %alloca[%arg4] : memref<32xf32, 5>
        %9 = scf.for %arg5 = %c0 to %4 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %10 = arith.addi %arg5, %6 : index
          %11 = memref.load %arg0[%10] : memref<?xf32>
          %12 = arith.subf %11, %8 : f32
          %13 = math.exp %12 : f32
          %14 = arith.addf %arg6, %13 : f32
          scf.yield %14 : f32
        }
        memref.store %9, %alloca_0[%arg4] : memref<32xf32, 5>
      }
      %c32_3 = arith.constant 32 : index
      %7 = arith.subi %c32_3, %c1 : index
      %c1_4 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_4 {
        %8 = memref.load %alloca_0[%arg4] : memref<32xf32, 5>
        %9 = math.log %8 : f32
        %10 = memref.load %alloca[%arg4] : memref<32xf32, 5>
        %11 = arith.subi %4, %arg4 : index
        %12 = arith.addi %7, %11 : index
        %13 = arith.divui %12, %c32_3 : index
        scf.for %arg5 = %c0 to %13 step %c1 {
          %14 = arith.muli %arg5, %c32_3 : index
          %15 = arith.addi %14, %6 : index
          %16 = arith.addi %15, %arg4 : index
          %17 = memref.load %arg0[%16] : memref<?xf32>
          %18 = arith.subf %17, %10 : f32
          %19 = arith.subf %18, %9 : f32
          memref.store %19, %arg1[%16] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] GPUBlockIdToNPULowering: process op: 

%2 = gpu.block_id  x
[ict-debug] CastLikeOpToNPULowering: process op: 

%4 = arith.index_cast %3 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%6 = arith.index_cast %arg3 : i32 to index
[ict-debug] CastLikeOpToNPULowering: process op: 

%9 = arith.index_cast %8 : i32 to index
[ict-debug] ArithUnaryOpToNPULowering: process op: 

%18 = math.exp %17 : f32
[ict-debug] ArithUnaryOpToNPULowering: met scalar unary op, need vector help process.

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = "npu.block_id"() : () -> i64
      %5 = emitc.cast %4 : i64 to i32
      %6 = emitc.cast %arg3 : i32 to index
      %7 = arith.muli %5, %arg3 : i32
      %8 = emitc.cast %7 : i32 to index
      %c1_0 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_0 {
        %10 = func.call @__builtin_inff() : () -> f32
        %11 = arith.negf %10 : f32
        %12 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %11) -> (f32) {
          %13 = arith.addi %arg5, %8 : index
          %14 = memref.load %arg0[%13] : memref<?xf32>
          %15 = arith.maxnumf %arg6, %14 : f32
          scf.yield %15 : f32
        }
        memref.store %12, %1[%arg4] : memref<32xf32, 5>
      }
      %c1_1 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_1 {
        %10 = memref.load %1[%arg4] : memref<32xf32, 5>
        %11 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %12 = arith.addi %arg5, %8 : index
          %13 = memref.load %arg0[%12] : memref<?xf32>
          %14 = emitc.sub %13, %10 : (f32, f32) -> f32
          %15 = emitc.call "expf"(%14) : (f32) -> f32
          %16 = emitc.add %arg6, %15 : (f32, f32) -> f32
          scf.yield %16 : f32
        }
        memref.store %11, %3[%arg4] : memref<32xf32, 5>
      }
      %c32_2 = arith.constant 32 : index
      %9 = arith.subi %c32_2, %c1 : index
      %c1_3 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_3 {
        %10 = memref.load %3[%arg4] : memref<32xf32, 5>
        %11 = math.log %10 : f32
        %12 = memref.load %1[%arg4] : memref<32xf32, 5>
        %13 = arith.subi %6, %arg4 : index
        %14 = arith.addi %9, %13 : index
        %15 = arith.divui %14, %c32_2 : index
        scf.for %arg5 = %c0 to %15 step %c1 {
          %16 = arith.muli %arg5, %c32_2 : index
          %17 = arith.addi %16, %8 : index
          %18 = arith.addi %17, %arg4 : index
          %19 = memref.load %arg0[%18] : memref<?xf32>
          %20 = emitc.sub %19, %12 : (f32, f32) -> f32
          %21 = emitc.sub %20, %11 : (f32, f32) -> f32
          memref.store %21, %arg1[%18] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU: end

[ict-debug] driver.cc: Before convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %c31 = arith.constant 31 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = "npu.block_id"() : () -> i64
      %5 = emitc.cast %4 : i64 to i32
      %6 = emitc.cast %arg3 : i32 to index
      %7 = arith.muli %5, %arg3 : i32
      %8 = emitc.cast %7 : i32 to index
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = func.call @__builtin_inff() : () -> f32
        %10 = arith.negf %9 : f32
        %11 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %10) -> (f32) {
          %12 = arith.addi %arg5, %8 : index
          %13 = memref.load %arg0[%12] : memref<?xf32>
          %14 = arith.maxnumf %arg6, %13 : f32
          scf.yield %14 : f32
        }
        memref.store %11, %1[%arg4] : memref<32xf32, 5>
      }
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = memref.load %1[%arg4] : memref<32xf32, 5>
        %10 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %11 = arith.addi %arg5, %8 : index
          %12 = memref.load %arg0[%11] : memref<?xf32>
          %13 = emitc.sub %12, %9 : (f32, f32) -> f32
          %14 = emitc.call "expf"(%13) : (f32) -> f32
          %15 = emitc.add %arg6, %14 : (f32, f32) -> f32
          scf.yield %15 : f32
        }
        memref.store %10, %3[%arg4] : memref<32xf32, 5>
      }
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = memref.load %3[%arg4] : memref<32xf32, 5>
        %10 = math.log %9 : f32
        %11 = memref.load %1[%arg4] : memref<32xf32, 5>
        %12 = arith.subi %6, %arg4 : index
        %13 = arith.addi %12, %c31 : index
        %14 = arith.divui %13, %c32 : index
        scf.for %arg5 = %c0 to %14 step %c1 {
          %15 = arith.muli %arg5, %c32 : index
          %16 = arith.addi %15, %8 : index
          %17 = arith.addi %16, %arg4 : index
          %18 = memref.load %arg0[%17] : memref<?xf32>
          %19 = emitc.sub %18, %11 : (f32, f32) -> f32
          %20 = emitc.sub %19, %10 : (f32, f32) -> f32
          memref.store %20, %arg1[%17] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
[ict-debug] driver.cc: Before convert to EmitC dialect: end

[ict-debug] driver.cc: After convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18log_softmax_kernelPKfPfii_0 {
    gpu.func @_Z18log_softmax_kernelPKfPfii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32, %arg3: i32) {
      %c31 = arith.constant 31 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = "npu.block_id"() : () -> i64
      %5 = emitc.cast %4 : i64 to i32
      %6 = emitc.cast %arg3 : i32 to index
      %7 = arith.muli %5, %arg3 : i32
      %8 = emitc.cast %7 : i32 to index
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = func.call @__builtin_inff() : () -> f32
        %10 = arith.negf %9 : f32
        %11 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %10) -> (f32) {
          %12 = arith.addi %arg5, %8 : index
          %13 = memref.load %arg0[%12] : memref<?xf32>
          %14 = arith.maxnumf %arg6, %13 : f32
          scf.yield %14 : f32
        }
        memref.store %11, %1[%arg4] : memref<32xf32, 5>
      }
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = memref.load %1[%arg4] : memref<32xf32, 5>
        %10 = scf.for %arg5 = %c0 to %6 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %11 = arith.addi %arg5, %8 : index
          %12 = memref.load %arg0[%11] : memref<?xf32>
          %13 = emitc.sub %12, %9 : (f32, f32) -> f32
          %14 = emitc.call "expf"(%13) : (f32) -> f32
          %15 = emitc.add %arg6, %14 : (f32, f32) -> f32
          scf.yield %15 : f32
        }
        memref.store %10, %3[%arg4] : memref<32xf32, 5>
      }
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %9 = memref.load %3[%arg4] : memref<32xf32, 5>
        %10 = math.log %9 : f32
        %11 = memref.load %1[%arg4] : memref<32xf32, 5>
        %12 = arith.subi %6, %arg4 : index
        %13 = arith.addi %12, %c31 : index
        %14 = arith.divui %13, %c32 : index
        scf.for %arg5 = %c0 to %14 step %c1 {
          %15 = arith.muli %arg5, %c32 : index
          %16 = arith.addi %15, %8 : index
          %17 = arith.addi %16, %arg4 : index
          %18 = memref.load %arg0[%17] : memref<?xf32>
          %19 = emitc.sub %18, %11 : (f32, f32) -> f32
          %20 = emitc.sub %19, %10 : (f32, f32) -> f32
          memref.store %20, %arg1[%17] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__builtin_inff() -> f32 attributes {llvm.linkage = #llvm.linkage<external>}
  }
}
[ict-debug] driver.cc: After convert to EmitC dialect: end

loc("/CUDA2BANG/Cambricon_NaiveProfiling/cuda_ops_test/Ascend_kernels/gen_cuda_kernels/level_1_prlblem_24_sample_0_LogSoftmax.cu":6:21): error: 'arith.negf' op unable to find printer for op
[ict-debug] driver.cc: After emitc::translateToCpp:

