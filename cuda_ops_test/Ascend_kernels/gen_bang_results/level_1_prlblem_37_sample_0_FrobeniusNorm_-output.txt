warning: CUDA version 12.1 is only partially supported
warning: CUDA version 12.1 is only partially supported
[ict-debug] driver.cc: After return 5, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z36__device_stub__frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    call @_Z21frobenius_norm_kernelPKfPffi(%arg0, %arg1, %arg2, %arg3) : (memref<?xf32>, memref<?xf32>, f32, i32) -> ()
    return
  }
  func.func private @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = gpu.block_dim  x
    %3 = arith.index_cast %2 : index to i32
    %4 = arith.muli %1, %3 : i32
    %5 = gpu.thread_id  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.addi %4, %6 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi slt, %7, %arg3 : i32
    scf.if %9 {
      %10 = affine.load %arg0[symbol(%8)] : memref<?xf32>
      %11 = arith.divf %10, %arg2 : f32
      affine.store %11, %arg1[symbol(%8)] : memref<?xf32>
    }
    return
  }
  func.func private @_Z32__device_stub__square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    call @_Z17square_sum_kernelPKfPfi(%arg0, %arg1, %arg2) : (memref<?xf32>, memref<?xf32>, i32) -> ()
    return
  }
  func.func private @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %alloca = memref.alloca() : memref<1xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.cmpi eq, %1, %c0_i32 : i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = gpu.thread_id  x
    %9 = arith.index_cast %8 : index to i32
    %10 = arith.addi %7, %9 : i32
    %11 = gpu.block_dim  x
    %12 = arith.index_cast %11 : index to i32
    %13 = gpu.grid_dim  x
    %14 = arith.index_cast %13 : index to i32
    %15 = arith.muli %12, %14 : i32
    %16 = arith.index_cast %arg2 : i32 to index
    %17 = arith.index_cast %10 : i32 to index
    %18 = arith.index_cast %15 : i32 to index
    %19 = arith.index_cast %15 : i32 to index
    %20 = arith.index_cast %10 : i32 to index
    %21 = arith.subi %16, %17 : index
    %22 = arith.subi %18, %c1 : index
    %23 = arith.addi %22, %21 : index
    %24 = arith.divui %23, %18 : index
    %25 = affine.for %arg3 = 0 to %24 iter_args(%arg4 = %cst) -> (f32) {
      %30 = affine.load %arg0[%arg3 * symbol(%19) + symbol(%20)] : memref<?xf32>
      %31 = arith.mulf %30, %30 : f32
      %32 = arith.addf %arg4, %31 : f32
      affine.yield %32 : f32
    }
    affine.store %25, %alloca[symbol(%0)] : memref<1xf32, 5>
    nvvm.barrier0
    %26 = gpu.block_dim  x
    %27 = arith.index_cast %26 : index to i32
    %28 = arith.divui %27, %c2_i32 : i32
    %29 = scf.while (%arg3 = %28) : (i32) -> i32 {
      %30 = arith.cmpi sgt, %arg3, %c0_i32 : i32
      scf.condition(%30) %arg3 : i32
    } do {
    ^bb0(%arg3: i32):
      %30 = arith.cmpi slt, %1, %arg3 : i32
      scf.if %30 {
        %32 = arith.addi %1, %arg3 : i32
        %33 = arith.index_cast %32 : i32 to index
        %34 = memref.load %alloca[%33] : memref<1xf32, 5>
        %35 = affine.load %alloca[symbol(%0)] : memref<1xf32, 5>
        %36 = arith.addf %35, %34 : f32
        affine.store %36, %alloca[symbol(%0)] : memref<1xf32, 5>
      }
      nvvm.barrier0
      %31 = arith.shrsi %arg3, %c1_i32 : i32
      scf.yield %31 : i32
    }
    scf.if %2 {
      %30 = gpu.block_id  x
      %31 = affine.load %alloca[0] : memref<1xf32, 5>
      memref.store %31, %arg1[%30] : memref<?xf32>
    }
    return
  }
}
[ict-debug] driver.cc: After return 5, module: end

[ict-debug] driver.cc: After return 6, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = gpu.block_dim  x
    %3 = arith.index_cast %2 : index to i32
    %4 = arith.muli %1, %3 : i32
    %5 = gpu.thread_id  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.addi %4, %6 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi slt, %7, %arg3 : i32
    scf.if %9 {
      %10 = affine.load %arg0[symbol(%8)] : memref<?xf32>
      %11 = arith.divf %10, %arg2 : f32
      affine.store %11, %arg1[symbol(%8)] : memref<?xf32>
    }
    return
  }
  func.func private @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %alloca = memref.alloca() : memref<1xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.cmpi eq, %1, %c0_i32 : i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = arith.addi %7, %1 : i32
    %9 = gpu.grid_dim  x
    %10 = arith.index_cast %9 : index to i32
    %11 = arith.muli %6, %10 : i32
    %12 = arith.index_cast %arg2 : i32 to index
    %13 = arith.index_cast %8 : i32 to index
    %14 = arith.index_cast %11 : i32 to index
    %15 = arith.subi %12, %13 : index
    %16 = arith.subi %14, %c1 : index
    %17 = arith.addi %16, %15 : index
    %18 = arith.divui %17, %14 : index
    %19 = affine.for %arg3 = 0 to %18 iter_args(%arg4 = %cst) -> (f32) {
      %22 = affine.load %arg0[%arg3 * symbol(%14) + symbol(%13)] : memref<?xf32>
      %23 = arith.mulf %22, %22 : f32
      %24 = arith.addf %arg4, %23 : f32
      affine.yield %24 : f32
    }
    affine.store %19, %alloca[symbol(%0)] : memref<1xf32, 5>
    nvvm.barrier0
    %20 = arith.divui %6, %c2_i32 : i32
    %21 = scf.while (%arg3 = %20) : (i32) -> i32 {
      %22 = arith.cmpi sgt, %arg3, %c0_i32 : i32
      scf.condition(%22) %arg3 : i32
    } do {
    ^bb0(%arg3: i32):
      %22 = arith.cmpi slt, %1, %arg3 : i32
      scf.if %22 {
        %24 = arith.addi %1, %arg3 : i32
        %25 = arith.index_cast %24 : i32 to index
        %26 = memref.load %alloca[%25] : memref<1xf32, 5>
        %27 = affine.load %alloca[symbol(%0)] : memref<1xf32, 5>
        %28 = arith.addf %27, %26 : f32
        affine.store %28, %alloca[symbol(%0)] : memref<1xf32, 5>
      }
      nvvm.barrier0
      %23 = arith.shrsi %arg3, %c1_i32 : i32
      scf.yield %23 : i32
    }
    scf.if %2 {
      %22 = affine.load %alloca[0] : memref<1xf32, 5>
      affine.store %22, %arg1[symbol(%3)] : memref<?xf32>
    }
    return
  }
}
[ict-debug] driver.cc: After return 6, module: end

WrapAndReplaceBarrierPass::runOnOperation(): before execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = gpu.block_dim  x
    %3 = arith.index_cast %2 : index to i32
    %4 = arith.muli %1, %3 : i32
    %5 = gpu.thread_id  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.addi %4, %6 : i32
    %8 = arith.index_cast %7 : i32 to index
    %9 = arith.cmpi slt, %7, %arg3 : i32
    scf.if %9 {
      %10 = memref.load %arg0[%8] : memref<?xf32>
      %11 = arith.divf %10, %arg2 : f32
      memref.store %11, %arg1[%8] : memref<?xf32>
    }
    return
  }
  func.func private @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c2_i32 = arith.constant 2 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %alloca = memref.alloca() : memref<1xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.cmpi eq, %1, %c0_i32 : i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = arith.addi %7, %1 : i32
    %9 = gpu.grid_dim  x
    %10 = arith.index_cast %9 : index to i32
    %11 = arith.muli %6, %10 : i32
    %12 = arith.index_cast %arg2 : i32 to index
    %13 = arith.index_cast %8 : i32 to index
    %14 = arith.index_cast %11 : i32 to index
    %15 = arith.subi %12, %13 : index
    %16 = arith.subi %14, %c1 : index
    %17 = arith.addi %16, %15 : index
    %18 = arith.divui %17, %14 : index
    %19 = scf.for %arg3 = %c0 to %18 step %c1 iter_args(%arg4 = %cst) -> (f32) {
      %22 = arith.muli %arg3, %14 : index
      %23 = arith.addi %22, %13 : index
      %24 = memref.load %arg0[%23] : memref<?xf32>
      %25 = arith.mulf %24, %24 : f32
      %26 = arith.addf %arg4, %25 : f32
      scf.yield %26 : f32
    }
    memref.store %19, %alloca[%0] : memref<1xf32, 5>
    nvvm.barrier0
    %20 = arith.divui %6, %c2_i32 : i32
    %21 = scf.while (%arg3 = %20) : (i32) -> i32 {
      %22 = arith.cmpi sgt, %arg3, %c0_i32 : i32
      scf.condition(%22) %arg3 : i32
    } do {
    ^bb0(%arg3: i32):
      %22 = arith.cmpi slt, %1, %arg3 : i32
      scf.if %22 {
        %24 = arith.addi %1, %arg3 : i32
        %25 = arith.index_cast %24 : i32 to index
        %26 = memref.load %alloca[%25] : memref<1xf32, 5>
        %27 = memref.load %alloca[%0] : memref<1xf32, 5>
        %28 = arith.addf %27, %26 : f32
        memref.store %28, %alloca[%0] : memref<1xf32, 5>
      }
      nvvm.barrier0
      %23 = arith.shrsi %arg3, %c1_i32 : i32
      scf.yield %23 : i32
    }
    scf.if %2 {
      %22 = memref.load %alloca[%c0] : memref<1xf32, 5>
      memref.store %22, %arg1[%3] : memref<?xf32>
    }
    return
  }
}
WrapAndReplaceBarrierPass::runOnOperation(): before execute: end
WrapAndReplaceBarrierPass::runOnOperation(): after execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = gpu.block_dim  x
      %3 = arith.index_cast %2 : index to i32
      %4 = arith.muli %1, %3 : i32
      %5 = arith.index_cast %arg4 : index to i32
      %6 = arith.addi %4, %5 : i32
      %7 = arith.index_cast %6 : i32 to index
      %8 = arith.cmpi slt, %6, %arg3 : i32
      scf.if %8 {
        %9 = memref.load %arg0[%7] : memref<?xf32>
        %10 = arith.divf %9, %arg2 : f32
        memref.store %10, %arg1[%7] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %c1_1 = arith.constant 1 : index
      %c1_i32 = arith.constant 1 : i32
      %c0_i32 = arith.constant 0 : i32
      %c2_i32 = arith.constant 2 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %0 = arith.index_cast %arg3 : index to i32
      %1 = arith.cmpi eq, %0, %c0_i32 : i32
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %4 = gpu.block_dim  x
      %5 = arith.index_cast %4 : index to i32
      %6 = arith.muli %3, %5 : i32
      %7 = arith.addi %6, %0 : i32
      %8 = gpu.grid_dim  x
      %9 = arith.index_cast %8 : index to i32
      %10 = arith.muli %5, %9 : i32
      %11 = arith.index_cast %arg2 : i32 to index
      %12 = arith.index_cast %7 : i32 to index
      %13 = arith.index_cast %10 : i32 to index
      %14 = arith.subi %11, %12 : index
      %15 = arith.subi %13, %c1_1 : index
      %16 = arith.addi %15, %14 : index
      %17 = arith.divui %16, %13 : index
      %18 = scf.for %arg4 = %c0_0 to %17 step %c1_1 iter_args(%arg5 = %cst) -> (f32) {
        %21 = arith.muli %arg4, %13 : index
        %22 = arith.addi %21, %12 : index
        %23 = memref.load %arg0[%22] : memref<?xf32>
        %24 = arith.mulf %23, %23 : f32
        %25 = arith.addf %arg5, %24 : f32
        scf.yield %25 : f32
      }
      memref.store %18, %alloca[%arg3] : memref<1xf32, 5>
      "polygeist.barrier"(%arg3) : (index) -> ()
      %19 = arith.divui %5, %c2_i32 : i32
      %20 = scf.while (%arg4 = %19) : (i32) -> i32 {
        %21 = arith.cmpi sgt, %arg4, %c0_i32 : i32
        scf.condition(%21) %arg4 : i32
      } do {
      ^bb0(%arg4: i32):
        %21 = arith.cmpi slt, %0, %arg4 : i32
        scf.if %21 {
          %23 = arith.addi %0, %arg4 : i32
          %24 = arith.index_cast %23 : i32 to index
          %25 = memref.load %alloca[%24] : memref<1xf32, 5>
          %26 = memref.load %alloca[%arg3] : memref<1xf32, 5>
          %27 = arith.addf %26, %25 : f32
          memref.store %27, %alloca[%arg3] : memref<1xf32, 5>
        }
        "polygeist.barrier"(%arg3) : (index) -> ()
        %22 = arith.shrsi %arg4, %c1_i32 : i32
        scf.yield %22 : i32
      }
      scf.if %1 {
        %21 = memref.load %alloca[%c0_0] : memref<1xf32, 5>
        memref.store %21, %arg1[%2] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
}
WrapAndReplaceBarrierPass::runOnOperation(): after execute: end
[ict-debug] driver.cc: After return 7, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = gpu.block_dim  x
      %3 = arith.index_cast %2 : index to i32
      %4 = arith.muli %1, %3 : i32
      %5 = arith.index_cast %arg4 : index to i32
      %6 = arith.addi %4, %5 : i32
      %7 = arith.index_cast %6 : i32 to index
      %8 = arith.cmpi slt, %6, %arg3 : i32
      scf.if %8 {
        %9 = memref.load %arg0[%7] : memref<?xf32>
        %10 = arith.divf %9, %arg2 : f32
        memref.store %10, %arg1[%7] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %c1_1 = arith.constant 1 : index
      %c1_i32 = arith.constant 1 : i32
      %c0_i32 = arith.constant 0 : i32
      %c2_i32 = arith.constant 2 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %0 = arith.index_cast %arg3 : index to i32
      %1 = arith.cmpi eq, %0, %c0_i32 : i32
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %4 = gpu.block_dim  x
      %5 = arith.index_cast %4 : index to i32
      %6 = arith.muli %3, %5 : i32
      %7 = arith.addi %6, %0 : i32
      %8 = gpu.grid_dim  x
      %9 = arith.index_cast %8 : index to i32
      %10 = arith.muli %5, %9 : i32
      %11 = arith.index_cast %arg2 : i32 to index
      %12 = arith.index_cast %7 : i32 to index
      %13 = arith.index_cast %10 : i32 to index
      %14 = arith.subi %11, %12 : index
      %15 = arith.subi %13, %c1_1 : index
      %16 = arith.addi %15, %14 : index
      %17 = arith.divui %16, %13 : index
      %18 = scf.for %arg4 = %c0_0 to %17 step %c1_1 iter_args(%arg5 = %cst) -> (f32) {
        %21 = arith.muli %arg4, %13 : index
        %22 = arith.addi %21, %12 : index
        %23 = memref.load %arg0[%22] : memref<?xf32>
        %24 = arith.mulf %23, %23 : f32
        %25 = arith.addf %arg5, %24 : f32
        scf.yield %25 : f32
      }
      memref.store %18, %alloca[%arg3] : memref<1xf32, 5>
      "polygeist.barrier"(%arg3) : (index) -> ()
      %19 = arith.divui %5, %c2_i32 : i32
      %20 = scf.while (%arg4 = %19) : (i32) -> i32 {
        %21 = arith.cmpi sgt, %arg4, %c0_i32 : i32
        scf.condition(%21) %arg4 : i32
      } do {
      ^bb0(%arg4: i32):
        %21 = arith.cmpi slt, %0, %arg4 : i32
        scf.if %21 {
          %23 = arith.addi %0, %arg4 : i32
          %24 = arith.index_cast %23 : i32 to index
          %25 = memref.load %alloca[%24] : memref<1xf32, 5>
          %26 = memref.load %alloca[%arg3] : memref<1xf32, 5>
          %27 = arith.addf %26, %25 : f32
          memref.store %27, %alloca[%arg3] : memref<1xf32, 5>
        }
        "polygeist.barrier"(%arg3) : (index) -> ()
        %22 = arith.shrsi %arg4, %c1_i32 : i32
        scf.yield %22 : i32
      }
      scf.if %1 {
        %21 = memref.load %alloca[%c0_0] : memref<1xf32, 5>
        memref.store %21, %arg1[%2] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
}
[ict-debug] driver.cc: After return 7, module: end

[ict-debug] driver.cc: Before my pass process:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = gpu.block_dim  x
    %3 = arith.index_cast %2 : index to i32
    %4 = arith.muli %1, %3 : i32
    scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
      %5 = arith.index_cast %arg4 : index to i32
      %6 = arith.addi %4, %5 : i32
      %7 = arith.index_cast %6 : i32 to index
      %8 = arith.cmpi slt, %6, %arg3 : i32
      scf.if %8 {
        %9 = memref.load %arg0[%7] : memref<?xf32>
        %10 = arith.divf %9, %arg2 : f32
        memref.store %10, %arg1[%7] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.000000e+00 : f32
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xi32>
    %alloca_1 = memref.alloca() : memref<32xi32>
    %alloca_2 = memref.alloca() : memref<i1>
    %0 = gpu.block_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = gpu.block_dim  x
    %3 = arith.index_cast %2 : index to i32
    %4 = arith.muli %1, %3 : i32
    %5 = gpu.grid_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %3, %6 : i32
    %8 = arith.index_cast %arg2 : i32 to index
    %9 = arith.index_cast %7 : i32 to index
    %10 = arith.subi %9, %c1 : index
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %16 = arith.index_cast %arg3 : index to i32
      %17 = arith.addi %4, %16 : i32
      %18 = arith.index_cast %17 : i32 to index
      %19 = arith.subi %8, %18 : index
      %20 = arith.addi %10, %19 : index
      %21 = arith.divui %20, %9 : index
      %22 = scf.for %arg4 = %c0 to %21 step %c1 iter_args(%arg5 = %cst) -> (f32) {
        %23 = arith.muli %arg4, %9 : index
        %24 = arith.addi %23, %18 : index
        %25 = memref.load %arg0[%24] : memref<?xf32>
        %26 = arith.mulf %25, %25 : f32
        %27 = arith.addf %arg5, %26 : f32
        scf.yield %27 : f32
      }
      memref.store %22, %alloca[%arg3] : memref<1xf32, 5>
      scf.yield
    }
    %11 = gpu.block_dim  x
    %12 = arith.index_cast %11 : index to i32
    %13 = arith.divui %12, %c2_i32 : i32
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      memref.store %13, %alloca_0[%arg3] : memref<32xi32>
      scf.yield
    }
    scf.while : () -> () {
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %17 = memref.load %alloca_0[%arg3] : memref<32xi32>
        %18 = arith.cmpi sgt, %17, %c0_i32 : i32
        %19 = arith.cmpi eq, %arg3, %c0 : index
        scf.if %19 {
          memref.store %18, %alloca_2[] : memref<i1>
        }
        memref.store %17, %alloca_1[%arg3] : memref<32xi32>
        scf.yield
      }
      %16 = memref.load %alloca_2[] : memref<i1>
      scf.condition(%16)
    } do {
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %16 = arith.index_cast %arg3 : index to i32
        %17 = memref.load %alloca_1[%arg3] : memref<32xi32>
        %18 = arith.cmpi slt, %16, %17 : i32
        scf.if %18 {
          %19 = arith.addi %16, %17 : i32
          %20 = arith.index_cast %19 : i32 to index
          %21 = memref.load %alloca[%20] : memref<1xf32, 5>
          %22 = memref.load %alloca[%arg3] : memref<1xf32, 5>
          %23 = arith.addf %22, %21 : f32
          memref.store %23, %alloca[%arg3] : memref<1xf32, 5>
        }
        scf.yield
      }
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %16 = memref.load %alloca_1[%arg3] : memref<32xi32>
        %17 = arith.shrsi %16, %c1_i32 : i32
        memref.store %17, %alloca_0[%arg3] : memref<32xi32>
        scf.yield
      }
      scf.yield
    }
    %14 = gpu.block_id  x
    %15 = memref.load %alloca[%c0] : memref<1xf32, 5>
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %16 = arith.index_cast %arg3 : index to i32
      %17 = arith.cmpi eq, %16, %c0_i32 : i32
      scf.if %17 {
        memref.store %15, %arg1[%14] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
}
[ict-debug] driver.cc: Before my pass process: end

[ict-debug] driver.cc: vectorizeSize = 1

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z21frobenius_norm_kernelPKfPffi_0 {
    gpu.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) {
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = gpu.block_dim  x
      %3 = arith.index_cast %2 : index to i32
      %4 = arith.muli %1, %3 : i32
      scf.parallel (%arg4) = (%c0) to (%c32) step (%c1) {
        %5 = arith.index_cast %arg4 : index to i32
        %6 = arith.addi %4, %5 : i32
        %7 = arith.index_cast %6 : i32 to index
        %8 = arith.cmpi slt, %6, %arg3 : i32
        scf.if %8 {
          %9 = memref.load %arg0[%7] : memref<?xf32>
          %10 = arith.divf %9, %arg2 : f32
          memref.store %10, %arg1[%7] : memref<?xf32>
        }
        scf.yield
      }
      gpu.return
    }
  }
  gpu.module @_Z17square_sum_kernelPKfPfi_1 {
    gpu.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c2_i32 = arith.constant 2 : i32
      %c0_i32 = arith.constant 0 : i32
      %c1_i32 = arith.constant 1 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<1xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xi32>
      %alloca_1 = memref.alloca() : memref<32xi32>
      %alloca_2 = memref.alloca() : memref<i1>
      %0 = gpu.block_id  x
      %1 = arith.index_cast %0 : index to i32
      %2 = gpu.block_dim  x
      %3 = arith.index_cast %2 : index to i32
      %4 = arith.muli %1, %3 : i32
      %5 = gpu.grid_dim  x
      %6 = arith.index_cast %5 : index to i32
      %7 = arith.muli %3, %6 : i32
      %8 = arith.index_cast %arg2 : i32 to index
      %9 = arith.index_cast %7 : i32 to index
      %10 = arith.subi %9, %c1 : index
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %13 = arith.index_cast %arg3 : index to i32
        %14 = arith.addi %4, %13 : i32
        %15 = arith.index_cast %14 : i32 to index
        %16 = arith.subi %8, %15 : index
        %17 = arith.addi %10, %16 : index
        %18 = arith.divui %17, %9 : index
        %19 = scf.for %arg4 = %c0 to %18 step %c1 iter_args(%arg5 = %cst) -> (f32) {
          %20 = arith.muli %arg4, %9 : index
          %21 = arith.addi %20, %15 : index
          %22 = memref.load %arg0[%21] : memref<?xf32>
          %23 = arith.mulf %22, %22 : f32
          %24 = arith.addf %arg5, %23 : f32
          scf.yield %24 : f32
        }
        memref.store %19, %alloca[%arg3] : memref<1xf32, 5>
        scf.yield
      }
      %11 = arith.divui %3, %c2_i32 : i32
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        memref.store %11, %alloca_0[%arg3] : memref<32xi32>
        scf.yield
      }
      scf.while : () -> () {
        scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
          %14 = memref.load %alloca_0[%arg3] : memref<32xi32>
          %15 = arith.cmpi sgt, %14, %c0_i32 : i32
          %16 = arith.cmpi eq, %arg3, %c0 : index
          scf.if %16 {
            memref.store %15, %alloca_2[] : memref<i1>
          }
          memref.store %14, %alloca_1[%arg3] : memref<32xi32>
          scf.yield
        }
        %13 = memref.load %alloca_2[] : memref<i1>
        scf.condition(%13)
      } do {
        scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
          %13 = arith.index_cast %arg3 : index to i32
          %14 = memref.load %alloca_1[%arg3] : memref<32xi32>
          %15 = arith.cmpi slt, %13, %14 : i32
          scf.if %15 {
            %16 = arith.addi %13, %14 : i32
            %17 = arith.index_cast %16 : i32 to index
            %18 = memref.load %alloca[%17] : memref<1xf32, 5>
            %19 = memref.load %alloca[%arg3] : memref<1xf32, 5>
            %20 = arith.addf %19, %18 : f32
            memref.store %20, %alloca[%arg3] : memref<1xf32, 5>
          }
          scf.yield
        }
        scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
          %13 = memref.load %alloca_1[%arg3] : memref<32xi32>
          %14 = arith.shrsi %13, %c1_i32 : i32
          memref.store %14, %alloca_0[%arg3] : memref<32xi32>
          scf.yield
        }
        scf.yield
      }
      %12 = memref.load %alloca[%c0] : memref<1xf32, 5>
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %13 = arith.index_cast %arg3 : index to i32
        %14 = arith.cmpi eq, %13, %c0_i32 : i32
        scf.if %14 {
          memref.store %12, %arg1[%0] : memref<?xf32>
        }
        scf.yield
      }
      gpu.return
    }
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute: end

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.grid_dim with npu.block_num

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize:

"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, f32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32):
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index
      %1 = "arith.constant"() <{value = 1 : index}> : () -> index
      %2 = "arith.constant"() <{value = 32 : index}> : () -> index
      %3 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %4 = "arith.index_cast"(%3) : (index) -> i32
      %5 = "arith.constant"() <{value = 32 : index}> : () -> index
      %6 = "arith.index_cast"(%5) : (index) -> i32
      %7 = "arith.muli"(%4, %6) : (i32, i32) -> i32
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%0, %2, %8) ({
      ^bb0(%arg4: index):
        %9 = "arith.index_cast"(%arg4) : (index) -> i32
        %10 = "arith.addi"(%7, %9) : (i32, i32) -> i32
        %11 = "arith.index_cast"(%10) : (i32) -> index
        %12 = "arith.cmpi"(%10, %arg3) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "scf.if"(%12) ({
          %13 = "memref.load"(%arg0, %11) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %14 = "arith.divf"(%13, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "memref.store"(%14, %arg1, %11) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi_0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32):
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
      %8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32>
      %9 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32>
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1>
      %11 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %12 = "arith.index_cast"(%11) : (index) -> i32
      %13 = "arith.constant"() <{value = 32 : index}> : () -> index
      %14 = "arith.index_cast"(%13) : (index) -> i32
      %15 = "arith.muli"(%12, %14) : (i32, i32) -> i32
      %16 = "npu.block_num"() : () -> i64
      %17 = "arith.index_cast"(%16) : (i64) -> i32
      %18 = "arith.muli"(%14, %17) : (i32, i32) -> i32
      %19 = "arith.index_cast"(%arg2) : (i32) -> index
      %20 = "arith.index_cast"(%18) : (i32) -> index
      %21 = "arith.subi"(%20, %5) : (index, index) -> index
      %22 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %22) ({
      ^bb0(%arg3: index):
        %27 = "arith.index_cast"(%arg3) : (index) -> i32
        %28 = "arith.addi"(%15, %27) : (i32, i32) -> i32
        %29 = "arith.index_cast"(%28) : (i32) -> index
        %30 = "arith.subi"(%19, %29) : (index, index) -> index
        %31 = "arith.addi"(%21, %30) : (index, index) -> index
        %32 = "arith.divui"(%31, %20) : (index, index) -> index
        %33 = "scf.for"(%4, %32, %5, %0) ({
        ^bb0(%arg4: index, %arg5: f32):
          %34 = "arith.muli"(%arg4, %20) : (index, index) -> index
          %35 = "arith.addi"(%34, %29) : (index, index) -> index
          %36 = "memref.load"(%arg0, %35) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %37 = "arith.mulf"(%36, %36) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %38 = "arith.addf"(%arg5, %37) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "scf.yield"(%38) : (f32) -> ()
        }) : (index, index, index, f32) -> f32
        "memref.store"(%33, %7, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %23 = "arith.divui"(%14, %1) : (i32, i32) -> i32
      %24 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %24) ({
      ^bb0(%arg3: index):
        "memref.store"(%23, %8, %arg3) <{nontemporal = false}> : (i32, memref<32xi32>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "scf.while"() ({
        %27 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %27) ({
        ^bb0(%arg3: index):
          %29 = "memref.load"(%8, %arg3) <{nontemporal = false}> : (memref<32xi32>, index) -> i32
          %30 = "arith.cmpi"(%29, %2) <{predicate = 4 : i64}> : (i32, i32) -> i1
          %31 = "arith.cmpi"(%arg3, %4) <{predicate = 0 : i64}> : (index, index) -> i1
          "scf.if"(%31) ({
            "memref.store"(%30, %10) : (i1, memref<i1>) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "memref.store"(%29, %9, %arg3) <{nontemporal = false}> : (i32, memref<32xi32>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %28 = "memref.load"(%10) : (memref<i1>) -> i1
        "scf.condition"(%28) : (i1) -> ()
      }, {
        %27 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %27) ({
        ^bb0(%arg3: index):
          %29 = "arith.index_cast"(%arg3) : (index) -> i32
          %30 = "memref.load"(%9, %arg3) <{nontemporal = false}> : (memref<32xi32>, index) -> i32
          %31 = "arith.cmpi"(%29, %30) <{predicate = 2 : i64}> : (i32, i32) -> i1
          "scf.if"(%31) ({
            %32 = "arith.addi"(%29, %30) : (i32, i32) -> i32
            %33 = "arith.index_cast"(%32) : (i32) -> index
            %34 = "memref.load"(%7, %33) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %35 = "memref.load"(%7, %arg3) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %36 = "arith.addf"(%35, %34) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "memref.store"(%36, %7, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %28 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %28) ({
        ^bb0(%arg3: index):
          %29 = "memref.load"(%9, %arg3) <{nontemporal = false}> : (memref<32xi32>, index) -> i32
          %30 = "arith.shrsi"(%29, %3) : (i32, i32) -> i32
          "memref.store"(%30, %8, %arg3) <{nontemporal = false}> : (i32, memref<32xi32>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : () -> ()
      %25 = "memref.load"(%7, %4) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
      %26 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %26) ({
      ^bb0(%arg3: index):
        %27 = "arith.index_cast"(%arg3) : (index) -> i32
        %28 = "arith.cmpi"(%27, %2) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%28) ({
          "memref.store"(%25, %arg1, %11) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z17square_sum_kernelPKfPfi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z17square_sum_kernelPKfPfi_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize: end

[ict-debug] GPUBlockIdToNPULowering: process op: 

%0 = gpu.block_id  x
[ict-debug] CastLikeOpToNPULowering: process op: 

%2 = arith.index_cast %1 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%7 = arith.index_cast %arg4 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%10 = arith.index_cast %9 : i32 to index
[ict-debug] MemRefAllocaToNPULowering: process op: 

%7 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
MemRefAllocaToNPULowering: module: 
"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, f32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32):
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index
      %1 = "arith.constant"() <{value = 1 : index}> : () -> index
      %2 = "arith.constant"() <{value = 32 : index}> : () -> index
      %3 = "npu.block_id"() : () -> i64
      %4 = "emitc.cast"(%3) : (i64) -> i32
      %5 = "arith.constant"() <{value = 32 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %7 = "arith.muli"(%4, %6) : (i32, i32) -> i32
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%0, %2, %8) ({
      ^bb0(%arg4: index):
        %9 = "builtin.unrealized_conversion_cast"(%arg4) : (index) -> i64
        %10 = "emitc.cast"(%9) : (i64) -> i32
        %11 = "arith.addi"(%7, %10) : (i32, i32) -> i32
        %12 = "emitc.cast"(%11) : (i32) -> index
        %13 = "arith.cmpi"(%11, %arg3) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "scf.if"(%13) ({
          %14 = "memref.load"(%arg0, %12) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %15 = "emitc.div"(%14, %arg2) : (f32, f32) -> f32
          "memref.store"(%15, %arg1, %12) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi_0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32):
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
      %9 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %11 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
      %12 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %13 = "arith.index_cast"(%12) : (index) -> i32
      %14 = "arith.constant"() <{value = 32 : index}> : () -> index
      %15 = "arith.index_cast"(%14) : (index) -> i32
      %16 = "arith.muli"(%13, %15) : (i32, i32) -> i32
      %17 = "npu.block_num"() : () -> i64
      %18 = "arith.index_cast"(%17) : (i64) -> i32
      %19 = "arith.muli"(%15, %18) : (i32, i32) -> i32
      %20 = "arith.index_cast"(%arg2) : (i32) -> index
      %21 = "arith.index_cast"(%19) : (i32) -> index
      %22 = "arith.subi"(%21, %5) : (index, index) -> index
      %23 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %23) ({
      ^bb0(%arg3: index):
        %28 = "arith.index_cast"(%arg3) : (index) -> i32
        %29 = "arith.addi"(%16, %28) : (i32, i32) -> i32
        %30 = "arith.index_cast"(%29) : (i32) -> index
        %31 = "arith.subi"(%20, %30) : (index, index) -> index
        %32 = "arith.addi"(%22, %31) : (index, index) -> index
        %33 = "arith.divui"(%32, %21) : (index, index) -> index
        %34 = "scf.for"(%4, %33, %5, %0) ({
        ^bb0(%arg4: index, %arg5: f32):
          %35 = "arith.muli"(%arg4, %21) : (index, index) -> index
          %36 = "arith.addi"(%35, %30) : (index, index) -> index
          %37 = "memref.load"(%arg0, %36) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %38 = "arith.mulf"(%37, %37) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %39 = "arith.addf"(%arg5, %38) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "scf.yield"(%39) : (f32) -> ()
        }) : (index, index, index, f32) -> f32
        "memref.store"(%34, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %24 = "arith.divui"(%15, %1) : (i32, i32) -> i32
      %25 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %25) ({
      ^bb0(%arg3: index):
        "memref.store"(%24, %9, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "scf.while"() ({
        %28 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %28) ({
        ^bb0(%arg3: index):
          %30 = "memref.load"(%9, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %31 = "arith.cmpi"(%30, %2) <{predicate = 4 : i64}> : (i32, i32) -> i1
          %32 = "arith.cmpi"(%arg3, %4) <{predicate = 0 : i64}> : (index, index) -> i1
          "scf.if"(%32) ({
            "memref.store"(%31, %11) : (i1, memref<i1, 5>) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "memref.store"(%30, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %29 = "memref.load"(%11) : (memref<i1, 5>) -> i1
        "scf.condition"(%29) : (i1) -> ()
      }, {
        %28 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %28) ({
        ^bb0(%arg3: index):
          %30 = "arith.index_cast"(%arg3) : (index) -> i32
          %31 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %32 = "arith.cmpi"(%30, %31) <{predicate = 2 : i64}> : (i32, i32) -> i1
          "scf.if"(%32) ({
            %33 = "arith.addi"(%30, %31) : (i32, i32) -> i32
            %34 = "arith.index_cast"(%33) : (i32) -> index
            %35 = "memref.load"(%8, %34) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %36 = "memref.load"(%8, %arg3) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %37 = "arith.addf"(%36, %35) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "memref.store"(%37, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %29 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %29) ({
        ^bb0(%arg3: index):
          %30 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %31 = "arith.shrsi"(%30, %3) : (i32, i32) -> i32
          "memref.store"(%31, %9, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : () -> ()
      %26 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
      %27 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %27) ({
      ^bb0(%arg3: index):
        %28 = "arith.index_cast"(%arg3) : (index) -> i32
        %29 = "arith.cmpi"(%28, %2) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%29) ({
          "memref.store"(%26, %arg1, %12) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z17square_sum_kernelPKfPfi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z17square_sum_kernelPKfPfi_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%9 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
MemRefAllocaToNPULowering: module: 
"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, f32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32):
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index
      %1 = "arith.constant"() <{value = 1 : index}> : () -> index
      %2 = "arith.constant"() <{value = 32 : index}> : () -> index
      %3 = "npu.block_id"() : () -> i64
      %4 = "emitc.cast"(%3) : (i64) -> i32
      %5 = "arith.constant"() <{value = 32 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %7 = "arith.muli"(%4, %6) : (i32, i32) -> i32
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%0, %2, %8) ({
      ^bb0(%arg4: index):
        %9 = "builtin.unrealized_conversion_cast"(%arg4) : (index) -> i64
        %10 = "emitc.cast"(%9) : (i64) -> i32
        %11 = "arith.addi"(%7, %10) : (i32, i32) -> i32
        %12 = "emitc.cast"(%11) : (i32) -> index
        %13 = "arith.cmpi"(%11, %arg3) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "scf.if"(%13) ({
          %14 = "memref.load"(%arg0, %12) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %15 = "emitc.div"(%14, %arg2) : (f32, f32) -> f32
          "memref.store"(%15, %arg1, %12) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi_0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32):
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
      %9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %11 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %12 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
      %13 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %14 = "arith.index_cast"(%13) : (index) -> i32
      %15 = "arith.constant"() <{value = 32 : index}> : () -> index
      %16 = "arith.index_cast"(%15) : (index) -> i32
      %17 = "arith.muli"(%14, %16) : (i32, i32) -> i32
      %18 = "npu.block_num"() : () -> i64
      %19 = "arith.index_cast"(%18) : (i64) -> i32
      %20 = "arith.muli"(%16, %19) : (i32, i32) -> i32
      %21 = "arith.index_cast"(%arg2) : (i32) -> index
      %22 = "arith.index_cast"(%20) : (i32) -> index
      %23 = "arith.subi"(%22, %5) : (index, index) -> index
      %24 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %24) ({
      ^bb0(%arg3: index):
        %29 = "arith.index_cast"(%arg3) : (index) -> i32
        %30 = "arith.addi"(%17, %29) : (i32, i32) -> i32
        %31 = "arith.index_cast"(%30) : (i32) -> index
        %32 = "arith.subi"(%21, %31) : (index, index) -> index
        %33 = "arith.addi"(%23, %32) : (index, index) -> index
        %34 = "arith.divui"(%33, %22) : (index, index) -> index
        %35 = "scf.for"(%4, %34, %5, %0) ({
        ^bb0(%arg4: index, %arg5: f32):
          %36 = "arith.muli"(%arg4, %22) : (index, index) -> index
          %37 = "arith.addi"(%36, %31) : (index, index) -> index
          %38 = "memref.load"(%arg0, %37) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %39 = "arith.mulf"(%38, %38) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %40 = "arith.addf"(%arg5, %39) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "scf.yield"(%40) : (f32) -> ()
        }) : (index, index, index, f32) -> f32
        "memref.store"(%35, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %25 = "arith.divui"(%16, %1) : (i32, i32) -> i32
      %26 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %26) ({
      ^bb0(%arg3: index):
        "memref.store"(%25, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "scf.while"() ({
        %29 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %29) ({
        ^bb0(%arg3: index):
          %31 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %32 = "arith.cmpi"(%31, %2) <{predicate = 4 : i64}> : (i32, i32) -> i1
          %33 = "arith.cmpi"(%arg3, %4) <{predicate = 0 : i64}> : (index, index) -> i1
          "scf.if"(%33) ({
            "memref.store"(%32, %12) : (i1, memref<i1, 5>) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "memref.store"(%31, %11, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %30 = "memref.load"(%12) : (memref<i1, 5>) -> i1
        "scf.condition"(%30) : (i1) -> ()
      }, {
        %29 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %29) ({
        ^bb0(%arg3: index):
          %31 = "arith.index_cast"(%arg3) : (index) -> i32
          %32 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %33 = "arith.cmpi"(%31, %32) <{predicate = 2 : i64}> : (i32, i32) -> i1
          "scf.if"(%33) ({
            %34 = "arith.addi"(%31, %32) : (i32, i32) -> i32
            %35 = "arith.index_cast"(%34) : (i32) -> index
            %36 = "memref.load"(%8, %35) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %37 = "memref.load"(%8, %arg3) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %38 = "arith.addf"(%37, %36) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "memref.store"(%38, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %30 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %30) ({
        ^bb0(%arg3: index):
          %31 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %32 = "arith.shrsi"(%31, %3) : (i32, i32) -> i32
          "memref.store"(%32, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : () -> ()
      %27 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
      %28 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %28) ({
      ^bb0(%arg3: index):
        %29 = "arith.index_cast"(%arg3) : (index) -> i32
        %30 = "arith.cmpi"(%29, %2) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%30) ({
          "memref.store"(%27, %arg1, %13) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z17square_sum_kernelPKfPfi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z17square_sum_kernelPKfPfi_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%11 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%11 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%12 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
MemRefAllocaToNPULowering: module: 
"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, f32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32):
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index
      %1 = "arith.constant"() <{value = 1 : index}> : () -> index
      %2 = "arith.constant"() <{value = 32 : index}> : () -> index
      %3 = "npu.block_id"() : () -> i64
      %4 = "emitc.cast"(%3) : (i64) -> i32
      %5 = "arith.constant"() <{value = 32 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %7 = "arith.muli"(%4, %6) : (i32, i32) -> i32
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%0, %2, %8) ({
      ^bb0(%arg4: index):
        %9 = "builtin.unrealized_conversion_cast"(%arg4) : (index) -> i64
        %10 = "emitc.cast"(%9) : (i64) -> i32
        %11 = "arith.addi"(%7, %10) : (i32, i32) -> i32
        %12 = "emitc.cast"(%11) : (i32) -> index
        %13 = "arith.cmpi"(%11, %arg3) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "scf.if"(%13) ({
          %14 = "memref.load"(%arg0, %12) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %15 = "emitc.div"(%14, %arg2) : (f32, f32) -> f32
          "memref.store"(%15, %arg1, %12) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi_0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32):
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
      %9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %11 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %12 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %13 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
      %14 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %15 = "arith.index_cast"(%14) : (index) -> i32
      %16 = "arith.constant"() <{value = 32 : index}> : () -> index
      %17 = "arith.index_cast"(%16) : (index) -> i32
      %18 = "arith.muli"(%15, %17) : (i32, i32) -> i32
      %19 = "npu.block_num"() : () -> i64
      %20 = "arith.index_cast"(%19) : (i64) -> i32
      %21 = "arith.muli"(%17, %20) : (i32, i32) -> i32
      %22 = "arith.index_cast"(%arg2) : (i32) -> index
      %23 = "arith.index_cast"(%21) : (i32) -> index
      %24 = "arith.subi"(%23, %5) : (index, index) -> index
      %25 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %25) ({
      ^bb0(%arg3: index):
        %30 = "arith.index_cast"(%arg3) : (index) -> i32
        %31 = "arith.addi"(%18, %30) : (i32, i32) -> i32
        %32 = "arith.index_cast"(%31) : (i32) -> index
        %33 = "arith.subi"(%22, %32) : (index, index) -> index
        %34 = "arith.addi"(%24, %33) : (index, index) -> index
        %35 = "arith.divui"(%34, %23) : (index, index) -> index
        %36 = "scf.for"(%4, %35, %5, %0) ({
        ^bb0(%arg4: index, %arg5: f32):
          %37 = "arith.muli"(%arg4, %23) : (index, index) -> index
          %38 = "arith.addi"(%37, %32) : (index, index) -> index
          %39 = "memref.load"(%arg0, %38) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %40 = "arith.mulf"(%39, %39) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %41 = "arith.addf"(%arg5, %40) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "scf.yield"(%41) : (f32) -> ()
        }) : (index, index, index, f32) -> f32
        "memref.store"(%36, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %26 = "arith.divui"(%17, %1) : (i32, i32) -> i32
      %27 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %27) ({
      ^bb0(%arg3: index):
        "memref.store"(%26, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "scf.while"() ({
        %30 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %30) ({
        ^bb0(%arg3: index):
          %32 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %33 = "arith.cmpi"(%32, %2) <{predicate = 4 : i64}> : (i32, i32) -> i1
          %34 = "arith.cmpi"(%arg3, %4) <{predicate = 0 : i64}> : (index, index) -> i1
          "scf.if"(%34) ({
            "memref.store"(%33, %13) : (i1, memref<i1, 5>) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "memref.store"(%32, %12, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %31 = "memref.load"(%13) : (memref<i1, 5>) -> i1
        "scf.condition"(%31) : (i1) -> ()
      }, {
        %30 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %30) ({
        ^bb0(%arg3: index):
          %32 = "arith.index_cast"(%arg3) : (index) -> i32
          %33 = "memref.load"(%12, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %34 = "arith.cmpi"(%32, %33) <{predicate = 2 : i64}> : (i32, i32) -> i1
          "scf.if"(%34) ({
            %35 = "arith.addi"(%32, %33) : (i32, i32) -> i32
            %36 = "arith.index_cast"(%35) : (i32) -> index
            %37 = "memref.load"(%8, %36) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %38 = "memref.load"(%8, %arg3) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %39 = "arith.addf"(%38, %37) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "memref.store"(%39, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %31 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %31) ({
        ^bb0(%arg3: index):
          %32 = "memref.load"(%12, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %33 = "arith.shrsi"(%32, %3) : (i32, i32) -> i32
          "memref.store"(%33, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : () -> ()
      %28 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
      %29 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %29) ({
      ^bb0(%arg3: index):
        %30 = "arith.index_cast"(%arg3) : (index) -> i32
        %31 = "arith.cmpi"(%30, %2) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%31) ({
          "memref.store"(%28, %arg1, %14) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z17square_sum_kernelPKfPfi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z17square_sum_kernelPKfPfi_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%13 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%13 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%14 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
MemRefAllocaToNPULowering: module: 
"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, f32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32):
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index
      %1 = "arith.constant"() <{value = 1 : index}> : () -> index
      %2 = "arith.constant"() <{value = 32 : index}> : () -> index
      %3 = "npu.block_id"() : () -> i64
      %4 = "emitc.cast"(%3) : (i64) -> i32
      %5 = "arith.constant"() <{value = 32 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %7 = "arith.muli"(%4, %6) : (i32, i32) -> i32
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%0, %2, %8) ({
      ^bb0(%arg4: index):
        %9 = "builtin.unrealized_conversion_cast"(%arg4) : (index) -> i64
        %10 = "emitc.cast"(%9) : (i64) -> i32
        %11 = "arith.addi"(%7, %10) : (i32, i32) -> i32
        %12 = "emitc.cast"(%11) : (i32) -> index
        %13 = "arith.cmpi"(%11, %arg3) <{predicate = 2 : i64}> : (i32, i32) -> i1
        "scf.if"(%13) ({
          %14 = "memref.load"(%arg0, %12) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %15 = "emitc.div"(%14, %arg2) : (f32, f32) -> f32
          "memref.store"(%15, %arg1, %12) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z21frobenius_norm_kernelPKfPffi_0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32):
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<1xf32, 5>
      %9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %11 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %12 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xi32, 5>
      %13 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %14 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<i1, 5>
      %15 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
      %16 = "arith.index_cast"(%15) : (index) -> i32
      %17 = "arith.constant"() <{value = 32 : index}> : () -> index
      %18 = "arith.index_cast"(%17) : (index) -> i32
      %19 = "arith.muli"(%16, %18) : (i32, i32) -> i32
      %20 = "npu.block_num"() : () -> i64
      %21 = "arith.index_cast"(%20) : (i64) -> i32
      %22 = "arith.muli"(%18, %21) : (i32, i32) -> i32
      %23 = "arith.index_cast"(%arg2) : (i32) -> index
      %24 = "arith.index_cast"(%22) : (i32) -> index
      %25 = "arith.subi"(%24, %5) : (index, index) -> index
      %26 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %26) ({
      ^bb0(%arg3: index):
        %31 = "arith.index_cast"(%arg3) : (index) -> i32
        %32 = "arith.addi"(%19, %31) : (i32, i32) -> i32
        %33 = "arith.index_cast"(%32) : (i32) -> index
        %34 = "arith.subi"(%23, %33) : (index, index) -> index
        %35 = "arith.addi"(%25, %34) : (index, index) -> index
        %36 = "arith.divui"(%35, %24) : (index, index) -> index
        %37 = "scf.for"(%4, %36, %5, %0) ({
        ^bb0(%arg4: index, %arg5: f32):
          %38 = "arith.muli"(%arg4, %24) : (index, index) -> index
          %39 = "arith.addi"(%38, %33) : (index, index) -> index
          %40 = "memref.load"(%arg0, %39) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %41 = "arith.mulf"(%40, %40) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %42 = "arith.addf"(%arg5, %41) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "scf.yield"(%42) : (f32) -> ()
        }) : (index, index, index, f32) -> f32
        "memref.store"(%37, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %27 = "arith.divui"(%18, %1) : (i32, i32) -> i32
      %28 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %28) ({
      ^bb0(%arg3: index):
        "memref.store"(%27, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "scf.while"() ({
        %31 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %31) ({
        ^bb0(%arg3: index):
          %33 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %34 = "arith.cmpi"(%33, %2) <{predicate = 4 : i64}> : (i32, i32) -> i1
          %35 = "arith.cmpi"(%arg3, %4) <{predicate = 0 : i64}> : (index, index) -> i1
          "scf.if"(%35) ({
            "memref.store"(%34, %14) : (i1, memref<i1, 5>) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "memref.store"(%33, %12, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %32 = "memref.load"(%14) : (memref<i1, 5>) -> i1
        "scf.condition"(%32) : (i1) -> ()
      }, {
        %31 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %31) ({
        ^bb0(%arg3: index):
          %33 = "arith.index_cast"(%arg3) : (index) -> i32
          %34 = "memref.load"(%12, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %35 = "arith.cmpi"(%33, %34) <{predicate = 2 : i64}> : (i32, i32) -> i1
          "scf.if"(%35) ({
            %36 = "arith.addi"(%33, %34) : (i32, i32) -> i32
            %37 = "arith.index_cast"(%36) : (i32) -> index
            %38 = "memref.load"(%8, %37) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %39 = "memref.load"(%8, %arg3) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
            %40 = "arith.addf"(%39, %38) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "memref.store"(%40, %8, %arg3) <{nontemporal = false}> : (f32, memref<1xf32, 5>, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        %32 = "arith.constant"() <{value = 1 : index}> : () -> index
        "scf.for"(%4, %6, %32) ({
        ^bb0(%arg3: index):
          %33 = "memref.load"(%12, %arg3) <{nontemporal = false}> : (memref<32xi32, 5>, index) -> i32
          %34 = "arith.shrsi"(%33, %3) : (i32, i32) -> i32
          "memref.store"(%34, %10, %arg3) <{nontemporal = false}> : (i32, memref<32xi32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : () -> ()
      %29 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<1xf32, 5>, index) -> f32
      %30 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %30) ({
      ^bb0(%arg3: index):
        %31 = "arith.index_cast"(%arg3) : (index) -> i32
        %32 = "arith.cmpi"(%31, %2) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%32) ({
          "memref.store"(%29, %arg1, %15) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z17square_sum_kernelPKfPfi", workgroup_attributions = 0 : i64} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z17square_sum_kernelPKfPfi_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
MemRefAllocaToNPULowering: module: end
[ict-debug] GPUBlockIdToNPULowering: process op: 

%15 = "gpu.block_id"() <{dimension = #gpu<dim x>}> : () -> index
[ict-debug] CastLikeOpToNPULowering: process op: 

%17 = "arith.index_cast"(%16) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%24 = "arith.index_cast"(%23) : (i64) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%27 = "arith.index_cast"(%arg2) : (i32) -> index
[ict-debug] CastLikeOpToNPULowering: process op: 

%29 = "arith.index_cast"(%26) : (i32) -> index
[ict-debug] CastLikeOpToNPULowering: process op: 

%38 = "arith.index_cast"(%arg3) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%41 = "arith.index_cast"(%40) : (i32) -> index
[ict-debug] CastLikeOpToNPULowering: process op: 

%40 = "arith.index_cast"(%arg3) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%45 = "arith.index_cast"(%44) : (i32) -> index
[ict-debug] CastLikeOpToNPULowering: process op: 

%38 = "arith.index_cast"(%arg3) : (index) -> i32
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z21frobenius_norm_kernelPKfPffi_0 {
    gpu.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) {
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.block_id"() : () -> i64
      %1 = emitc.cast %0 : i64 to i32
      %c32_0 = arith.constant 32 : index
      %c32_i32 = arith.constant 32 : i32
      %2 = arith.muli %1, %c32_i32 : i32
      %c1_1 = arith.constant 1 : index
      scf.for %arg4 = %c0 to %c32 step %c1_1 {
        %3 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %4 = emitc.cast %3 : i64 to i32
        %5 = arith.addi %2, %4 : i32
        %6 = emitc.cast %5 : i32 to index
        %7 = arith.cmpi slt, %5, %arg3 : i32
        scf.if %7 {
          %8 = memref.load %arg0[%6] : memref<?xf32>
          %9 = emitc.div %8, %arg2 : (f32, f32) -> f32
          memref.store %9, %arg1[%6] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
  gpu.module @_Z17square_sum_kernelPKfPfi_1 {
    gpu.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 0.000000e+00 : f32
      %c2_i32 = arith.constant 2 : i32
      %c0_i32 = arith.constant 0 : i32
      %c1_i32 = arith.constant 1 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xi32, 5>
      %4 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %5 = builtin.unrealized_conversion_cast %4 : !llvm.ptr<6> to memref<32xi32, 5>
      %6 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %7 = builtin.unrealized_conversion_cast %6 : !llvm.ptr<6> to memref<i1, 5>
      %8 = "npu.block_id"() : () -> i64
      %9 = builtin.unrealized_conversion_cast %8 : i64 to index
      %10 = emitc.cast %8 : i64 to i32
      %c32_0 = arith.constant 32 : index
      %c32_i32 = arith.constant 32 : i32
      %11 = arith.muli %10, %c32_i32 : i32
      %12 = "npu.block_num"() : () -> i64
      %13 = emitc.cast %12 : i64 to i32
      %14 = arith.muli %c32_i32, %13 : i32
      %15 = emitc.cast %arg2 : i32 to index
      %16 = emitc.cast %14 : i32 to index
      %17 = arith.subi %16, %c1 : index
      %c1_1 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_1 {
        %20 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %21 = emitc.cast %20 : i64 to i32
        %22 = arith.addi %11, %21 : i32
        %23 = emitc.cast %22 : i32 to index
        %24 = arith.subi %15, %23 : index
        %25 = arith.addi %17, %24 : index
        %26 = arith.divui %25, %16 : index
        %27 = scf.for %arg4 = %c0 to %26 step %c1 iter_args(%arg5 = %cst) -> (f32) {
          %28 = arith.muli %arg4, %16 : index
          %29 = arith.addi %28, %23 : index
          %30 = memref.load %arg0[%29] : memref<?xf32>
          %31 = emitc.mul %30, %30 : (f32, f32) -> f32
          %32 = emitc.add %arg5, %31 : (f32, f32) -> f32
          scf.yield %32 : f32
        }
        memref.store %27, %1[%arg3] : memref<1xf32, 5>
      }
      %18 = arith.divui %c32_i32, %c2_i32 : i32
      %c1_2 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_2 {
        memref.store %18, %3[%arg3] : memref<32xi32, 5>
      }
      scf.while : () -> () {
        %c1_4 = arith.constant 1 : index
        scf.for %arg3 = %c0 to %c32 step %c1_4 {
          %21 = memref.load %3[%arg3] : memref<32xi32, 5>
          %22 = arith.cmpi sgt, %21, %c0_i32 : i32
          %23 = arith.cmpi eq, %arg3, %c0 : index
          scf.if %23 {
            memref.store %22, %7[] : memref<i1, 5>
          }
          memref.store %21, %5[%arg3] : memref<32xi32, 5>
        }
        %20 = memref.load %7[] : memref<i1, 5>
        scf.condition(%20)
      } do {
        %c1_4 = arith.constant 1 : index
        scf.for %arg3 = %c0 to %c32 step %c1_4 {
          %20 = builtin.unrealized_conversion_cast %arg3 : index to i64
          %21 = emitc.cast %20 : i64 to i32
          %22 = memref.load %5[%arg3] : memref<32xi32, 5>
          %23 = arith.cmpi slt, %21, %22 : i32
          scf.if %23 {
            %24 = arith.addi %21, %22 : i32
            %25 = emitc.cast %24 : i32 to index
            %26 = memref.load %1[%25] : memref<1xf32, 5>
            %27 = memref.load %1[%arg3] : memref<1xf32, 5>
            %28 = emitc.add %27, %26 : (f32, f32) -> f32
            memref.store %28, %1[%arg3] : memref<1xf32, 5>
          }
        }
        %c1_5 = arith.constant 1 : index
        scf.for %arg3 = %c0 to %c32 step %c1_5 {
          %20 = memref.load %5[%arg3] : memref<32xi32, 5>
          %21 = arith.shrsi %20, %c1_i32 : i32
          memref.store %21, %3[%arg3] : memref<32xi32, 5>
        }
        scf.yield
      }
      %19 = memref.load %1[%c0] : memref<1xf32, 5>
      %c1_3 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_3 {
        %20 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %21 = emitc.cast %20 : i64 to i32
        %22 = arith.cmpi eq, %21, %c0_i32 : i32
        scf.if %22 {
          memref.store %19, %arg1[%9] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU: end

[ict-debug] driver.cc: Before convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z21frobenius_norm_kernelPKfPffi_0 {
    gpu.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) {
      %c32_i32 = arith.constant 32 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.block_id"() : () -> i64
      %1 = emitc.cast %0 : i64 to i32
      %2 = arith.muli %1, %c32_i32 : i32
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %3 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %4 = emitc.cast %3 : i64 to i32
        %5 = arith.addi %2, %4 : i32
        %6 = emitc.cast %5 : i32 to index
        %7 = arith.cmpi slt, %5, %arg3 : i32
        scf.if %7 {
          %8 = memref.load %arg0[%6] : memref<?xf32>
          %9 = emitc.div %8, %arg2 : (f32, f32) -> f32
          memref.store %9, %arg1[%6] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
  gpu.module @_Z17square_sum_kernelPKfPfi_1 {
    gpu.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %c16_i32 = arith.constant 16 : i32
      %c32_i32 = arith.constant 32 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c0_i32 = arith.constant 0 : i32
      %c1_i32 = arith.constant 1 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xi32, 5>
      %4 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %5 = builtin.unrealized_conversion_cast %4 : !llvm.ptr<6> to memref<32xi32, 5>
      %6 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %7 = builtin.unrealized_conversion_cast %6 : !llvm.ptr<6> to memref<i1, 5>
      %8 = "npu.block_id"() : () -> i64
      %9 = builtin.unrealized_conversion_cast %8 : i64 to index
      %10 = emitc.cast %8 : i64 to i32
      %11 = arith.muli %10, %c32_i32 : i32
      %12 = "npu.block_num"() : () -> i64
      %13 = emitc.cast %12 : i64 to i32
      %14 = arith.muli %13, %c32_i32 : i32
      %15 = emitc.cast %arg2 : i32 to index
      %16 = emitc.cast %14 : i32 to index
      %17 = arith.subi %16, %c1 : index
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %20 = emitc.cast %19 : i64 to i32
        %21 = arith.addi %11, %20 : i32
        %22 = emitc.cast %21 : i32 to index
        %23 = arith.subi %15, %22 : index
        %24 = arith.addi %17, %23 : index
        %25 = arith.divui %24, %16 : index
        %26 = scf.for %arg4 = %c0 to %25 step %c1 iter_args(%arg5 = %cst) -> (f32) {
          %27 = arith.muli %arg4, %16 : index
          %28 = arith.addi %27, %22 : index
          %29 = memref.load %arg0[%28] : memref<?xf32>
          %30 = emitc.mul %29, %29 : (f32, f32) -> f32
          %31 = emitc.add %arg5, %30 : (f32, f32) -> f32
          scf.yield %31 : f32
        }
        memref.store %26, %1[%arg3] : memref<1xf32, 5>
      }
      scf.for %arg3 = %c0 to %c32 step %c1 {
        memref.store %c16_i32, %3[%arg3] : memref<32xi32, 5>
      }
      scf.while : () -> () {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %20 = memref.load %3[%arg3] : memref<32xi32, 5>
          %21 = arith.cmpi sgt, %20, %c0_i32 : i32
          %22 = arith.cmpi eq, %arg3, %c0 : index
          scf.if %22 {
            memref.store %21, %7[] : memref<i1, 5>
          }
          memref.store %20, %5[%arg3] : memref<32xi32, 5>
        }
        %19 = memref.load %7[] : memref<i1, 5>
        scf.condition(%19)
      } do {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
          %20 = emitc.cast %19 : i64 to i32
          %21 = memref.load %5[%arg3] : memref<32xi32, 5>
          %22 = arith.cmpi slt, %20, %21 : i32
          scf.if %22 {
            %23 = arith.addi %20, %21 : i32
            %24 = emitc.cast %23 : i32 to index
            %25 = memref.load %1[%24] : memref<1xf32, 5>
            %26 = memref.load %1[%arg3] : memref<1xf32, 5>
            %27 = emitc.add %26, %25 : (f32, f32) -> f32
            memref.store %27, %1[%arg3] : memref<1xf32, 5>
          }
        }
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %19 = memref.load %5[%arg3] : memref<32xi32, 5>
          %20 = arith.shrsi %19, %c1_i32 : i32
          memref.store %20, %3[%arg3] : memref<32xi32, 5>
        }
        scf.yield
      }
      %18 = memref.load %1[%c0] : memref<1xf32, 5>
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %20 = emitc.cast %19 : i64 to i32
        %21 = arith.cmpi eq, %20, %c0_i32 : i32
        scf.if %21 {
          memref.store %18, %arg1[%9] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
}
[ict-debug] driver.cc: Before convert to EmitC dialect: end

[ict-debug] driver.cc: After convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z21frobenius_norm_kernelPKfPffi_0 {
    gpu.func @_Z21frobenius_norm_kernelPKfPffi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: f32, %arg3: i32) {
      %c32_i32 = arith.constant 32 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.block_id"() : () -> i64
      %1 = emitc.cast %0 : i64 to i32
      %2 = arith.muli %1, %c32_i32 : i32
      scf.for %arg4 = %c0 to %c32 step %c1 {
        %3 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %4 = emitc.cast %3 : i64 to i32
        %5 = arith.addi %2, %4 : i32
        %6 = emitc.cast %5 : i32 to index
        %7 = arith.cmpi slt, %5, %arg3 : i32
        emitc.if %7 {
          %8 = memref.load %arg0[%6] : memref<?xf32>
          %9 = emitc.div %8, %arg2 : (f32, f32) -> f32
          memref.store %9, %arg1[%6] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
  gpu.module @_Z17square_sum_kernelPKfPfi_1 {
    gpu.func @_Z17square_sum_kernelPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %c16_i32 = arith.constant 16 : i32
      %c32_i32 = arith.constant 32 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c0_i32 = arith.constant 0 : i32
      %c1_i32 = arith.constant 1 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xi32, 5>
      %4 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %5 = builtin.unrealized_conversion_cast %4 : !llvm.ptr<6> to memref<32xi32, 5>
      %6 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %7 = builtin.unrealized_conversion_cast %6 : !llvm.ptr<6> to memref<i1, 5>
      %8 = "npu.block_id"() : () -> i64
      %9 = builtin.unrealized_conversion_cast %8 : i64 to index
      %10 = emitc.cast %8 : i64 to i32
      %11 = arith.muli %10, %c32_i32 : i32
      %12 = "npu.block_num"() : () -> i64
      %13 = emitc.cast %12 : i64 to i32
      %14 = arith.muli %13, %c32_i32 : i32
      %15 = emitc.cast %arg2 : i32 to index
      %16 = emitc.cast %14 : i32 to index
      %17 = arith.subi %16, %c1 : index
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %20 = emitc.cast %19 : i64 to i32
        %21 = arith.addi %11, %20 : i32
        %22 = emitc.cast %21 : i32 to index
        %23 = arith.subi %15, %22 : index
        %24 = arith.addi %17, %23 : index
        %25 = arith.divui %24, %16 : index
        %26 = scf.for %arg4 = %c0 to %25 step %c1 iter_args(%arg5 = %cst) -> (f32) {
          %27 = arith.muli %arg4, %16 : index
          %28 = arith.addi %27, %22 : index
          %29 = memref.load %arg0[%28] : memref<?xf32>
          %30 = emitc.mul %29, %29 : (f32, f32) -> f32
          %31 = emitc.add %arg5, %30 : (f32, f32) -> f32
          scf.yield %31 : f32
        }
        memref.store %26, %1[%arg3] : memref<1xf32, 5>
      }
      scf.for %arg3 = %c0 to %c32 step %c1 {
        memref.store %c16_i32, %3[%arg3] : memref<32xi32, 5>
      }
      scf.while : () -> () {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %20 = memref.load %3[%arg3] : memref<32xi32, 5>
          %21 = arith.cmpi sgt, %20, %c0_i32 : i32
          %22 = arith.cmpi eq, %arg3, %c0 : index
          emitc.if %22 {
            memref.store %21, %7[] : memref<i1, 5>
          }
          memref.store %20, %5[%arg3] : memref<32xi32, 5>
        }
        %19 = memref.load %7[] : memref<i1, 5>
        scf.condition(%19)
      } do {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
          %20 = emitc.cast %19 : i64 to i32
          %21 = memref.load %5[%arg3] : memref<32xi32, 5>
          %22 = arith.cmpi slt, %20, %21 : i32
          emitc.if %22 {
            %23 = arith.addi %20, %21 : i32
            %24 = emitc.cast %23 : i32 to index
            %25 = memref.load %1[%24] : memref<1xf32, 5>
            %26 = memref.load %1[%arg3] : memref<1xf32, 5>
            %27 = emitc.add %26, %25 : (f32, f32) -> f32
            memref.store %27, %1[%arg3] : memref<1xf32, 5>
          }
        }
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %19 = memref.load %5[%arg3] : memref<32xi32, 5>
          %20 = arith.shrsi %19, %c1_i32 : i32
          memref.store %20, %3[%arg3] : memref<32xi32, 5>
        }
        scf.yield
      }
      %18 = memref.load %1[%c0] : memref<1xf32, 5>
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %19 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %20 = emitc.cast %19 : i64 to i32
        %21 = arith.cmpi eq, %20, %c0_i32 : i32
        emitc.if %21 {
          memref.store %18, %arg1[%9] : memref<?xf32>
        }
      }
      gpu.return
    }
  }
}
[ict-debug] driver.cc: After convert to EmitC dialect: end

loc("/CUDA2BANG/Cambricon_NaiveProfiling/cuda_ops_test/Ascend_kernels/gen_cuda_kernels/level_1_prlblem_37_sample_0_FrobeniusNorm_.cu":24:34): error: ICT_ERROR(): cannot emit MemRef element type: 'memref<32xi32, 5>'
[ict-debug] driver.cc: After emitc::translateToCpp:

