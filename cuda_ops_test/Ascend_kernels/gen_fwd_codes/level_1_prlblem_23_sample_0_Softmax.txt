torch::Tensor softmax_cuda(torch::Tensor input) {
    // Ensure input is contiguous and on CUDA
    auto input_contiguous = input.contiguous();
    auto batch_size = input_contiguous.size(0);
    auto dim = input_contiguous.size(1);
    auto output = torch::empty_like(input_contiguous);

    int threads = 512; // Adjust as needed
    int blocks = batch_size;

    size_t shared_mem_size = 2 * threads * sizeof(float);

    softmax_kernel_batch<<<blocks, threads, shared_mem_size>>>(input_contiguous.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim);

    return output;
}
