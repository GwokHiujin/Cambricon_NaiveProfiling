torch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    // Check input tensors are on the same device
    if (!A.is_cuda() || !B.is_cuda()) {
        throw std::invalid_argument("Input tensors must be CUDA tensors.");
    }

    // Ensure input tensors are contiguous
    A = A.contiguous();
    B = B.contiguous();

    // Get dimensions
    int batch_size = A.size(0);
    int M = A.size(1);
    int K = A.size(2);
    int N = B.size(2);

    // Ensure that the batch sizes and inner dimensions match
    if (batch_size != B.size(0) || K != B.size(1)) {
        throw std::invalid_argument("Input tensor dimensions do not match for batched matrix multiplication.");
    }

    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());
    torch::Tensor C = torch::zeros({batch_size, M, N}, options);

    const int TILE_SIZE = 16;
    dim3 threads(TILE_SIZE, TILE_SIZE);
    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE,
                (M + TILE_SIZE - 1) / TILE_SIZE,
                batch_size);

    batched_matrix_multiply_kernel<<<blocks, threads>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        batch_size, M, K, N);

    return C;
}
