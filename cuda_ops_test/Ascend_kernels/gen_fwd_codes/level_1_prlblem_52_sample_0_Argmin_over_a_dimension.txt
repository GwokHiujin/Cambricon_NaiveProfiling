torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    auto batch_size = input.size(0);
    auto dim1 = input.size(1);
    auto dim2 = input.size(2);
    
    auto output = torch::empty({batch_size, dim2}, 
                             torch::TensorOptions().dtype(torch::kLong).device(input.device()));
    
    const int threads = 256;
    const int blocks = (batch_size * dim2 + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.type(), "argmin_cuda", ([&] {
        argmin_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<int64_t>(),
            batch_size, dim1, dim2
        );
    }));

    return output;
}
