warning: CUDA version 12.1 is only partially supported
warning: we failed to emit call to builtin function __nvvm_shfl_sync_down_f32
warning: CUDA version 12.1 is only partially supported
[ict-debug] driver.cc: After return 5, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z18computePartialSumsPKfiiiiiiiRfS1_(%arg0: memref<?xf32>, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: memref<?xf32>, %arg9: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = arith.muli %arg2, %arg4 : i32
    %1 = arith.muli %0, %arg5 : i32
    affine.store %cst, %arg8[0] : memref<?xf32>
    affine.store %cst, %arg9[0] : memref<?xf32>
    %2 = arith.index_cast %1 : i32 to index
    %3 = arith.index_cast %arg6 : i32 to index
    %4 = arith.index_cast %arg7 : i32 to index
    %5 = arith.index_cast %arg7 : i32 to index
    %6 = arith.index_cast %arg6 : i32 to index
    %7 = arith.muli %arg4, %arg5 : i32
    %8 = arith.muli %arg4, %arg5 : i32
    %9 = arith.subi %2, %3 : index
    %10 = arith.subi %4, %c1 : index
    %11 = arith.addi %10, %9 : index
    %12 = arith.divui %11, %4 : index
    affine.for %arg10 = 0 to %12 {
      %13 = arith.muli %arg10, %4 : index
      %14 = arith.divui %13, %4 : index
      %15 = arith.muli %14, %5 : index
      %16 = arith.addi %6, %15 : index
      %17 = arith.index_cast %16 : index to i32
      %18 = arith.divsi %17, %7 : i32
      %19 = arith.remsi %17, %8 : i32
      %20 = arith.divsi %19, %arg5 : i32
      %21 = arith.remsi %19, %arg5 : i32
      %22 = arith.muli %18, %arg3 : i32
      %23 = arith.addi %22, %arg1 : i32
      %24 = arith.muli %23, %arg4 : i32
      %25 = arith.addi %24, %20 : i32
      %26 = arith.muli %25, %arg5 : i32
      %27 = arith.addi %26, %21 : i32
      %28 = arith.index_cast %27 : i32 to index
      %29 = memref.load %arg0[%28] : memref<?xf32>
      %30 = affine.load %arg8[0] : memref<?xf32>
      %31 = arith.addf %30, %29 : f32
      affine.store %31, %arg8[0] : memref<?xf32>
      %32 = arith.mulf %29, %29 : f32
      %33 = affine.load %arg9[0] : memref<?xf32>
      %34 = arith.addf %33, %32 : f32
      affine.store %34, %arg9[0] : memref<?xf32>
    }
    return
  }
  func.func private @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c31_i32 = arith.constant 31 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i32 = arith.constant 0 : i32
    %c32_i32 = arith.constant 32 : i32
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.remui %1, %c32_i32 : i32
    %3 = arith.cmpi eq, %2, %c0_i32 : i32
    %4 = gpu.thread_id  x
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.divui %5, %c32_i32 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = affine.load %arg0[0] : memref<?xf32>
    %9 = call @_Z13warpReduceSumf(%8) : (f32) -> f32
    %10 = affine.load %arg1[0] : memref<?xf32>
    %11 = call @_Z13warpReduceSumf(%10) : (f32) -> f32
    scf.if %3 {
      affine.store %9, %alloca_0[symbol(%7)] : memref<32xf32, 5>
      affine.store %11, %alloca[symbol(%7)] : memref<32xf32, 5>
    }
    nvvm.barrier0
    %12 = gpu.thread_id  x
    %13 = arith.index_cast %12 : index to i32
    %14 = arith.cmpi eq, %13, %c0_i32 : i32
    scf.if %14 {
      %15 = gpu.block_dim  x
      %16 = arith.index_cast %15 : index to i32
      %17 = arith.addi %16, %c31_i32 : i32
      %18 = arith.divui %17, %c32_i32 : i32
      %19 = arith.index_cast %18 : i32 to index
      %20:2 = scf.for %arg2 = %c0 to %19 step %c1 iter_args(%arg3 = %cst, %arg4 = %cst) -> (f32, f32) {
        %21 = memref.load %alloca_0[%arg2] : memref<32xf32, 5>
        %22 = arith.addf %arg4, %21 : f32
        %23 = memref.load %alloca[%arg2] : memref<32xf32, 5>
        %24 = arith.addf %arg3, %23 : f32
        scf.yield %24, %22 : f32, f32
      }
      affine.store %20#1, %arg0[0] : memref<?xf32>
      affine.store %20#0, %arg1[0] : memref<?xf32>
    }
    nvvm.barrier0
    return
  }
  func.func private @_Z13warpReduceSumf(%arg0: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<linkonce_odr>, polygeist.device_only_func = "1"} {
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %0:2 = scf.while (%arg1 = %c16_i32, %arg2 = %arg0) : (i32, f32) -> (f32, i32) {
      %1 = arith.cmpi sgt, %arg1, %c0_i32 : i32
      scf.condition(%1) %arg2, %arg1 : f32, i32
    } do {
    ^bb0(%arg1: f32, %arg2: i32):
      %1 = func.call @_Z16__shfl_down_syncjfji(%c-1_i32, %arg1, %arg2, %c32_i32) : (i32, f32, i32, i32) -> f32
      %2 = arith.addf %arg1, %1 : f32
      %3 = arith.divsi %arg2, %c2_i32 : i32
      scf.yield %3, %2 : i32, f32
    }
    return %0#0 : f32
  }
  func.func private @_Z51__device_stub__adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    call @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12) : (memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, i8, f32, f32, memref<?xf32>, i32, i32, i32, i32) -> ()
    return
  }
  func.func private @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1 = arith.constant 1 : index
    %cst = arith.constant 1.000000e+00 : f32
    %c0_i8 = arith.constant 0 : i8
    %c0_i32 = arith.constant 0 : i32
    %0 = llvm.mlir.undef : f32
    %alloca = memref.alloca() : memref<2xf32, 5>
    %alloca_0 = memref.alloca() : memref<1xf32>
    %cast = memref.cast %alloca_0 : memref<1xf32> to memref<?xf32>
    affine.store %0, %alloca_0[0] : memref<1xf32>
    %alloca_1 = memref.alloca() : memref<1xf32>
    %cast_2 = memref.cast %alloca_1 : memref<1xf32> to memref<?xf32>
    affine.store %0, %alloca_1[0] : memref<1xf32>
    %1 = gpu.block_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.thread_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = arith.cmpi eq, %4, %c0_i32 : i32
    %6 = gpu.block_dim  x
    %7 = arith.index_cast %6 : index to i32
    %8 = arith.muli %arg9, %arg11 : i32
    %9 = arith.muli %8, %arg12 : i32
    call @_Z18computePartialSumsPKfiiiiiiiRfS1_(%arg0, %2, %arg9, %arg10, %arg11, %arg12, %4, %7, %cast_2, %cast) : (memref<?xf32>, i32, i32, i32, i32, i32, i32, i32, memref<?xf32>, memref<?xf32>) -> ()
    call @_Z14blockReduceSumRfS_(%cast_2, %cast) : (memref<?xf32>, memref<?xf32>) -> ()
    scf.if %5 {
      %23 = affine.load %alloca_1[0] : memref<1xf32>
      %24 = arith.sitofp %9 : i32 to f32
      %25 = arith.divf %23, %24 : f32
      %26 = affine.load %alloca_0[0] : memref<1xf32>
      %27 = arith.sitofp %9 : i32 to f32
      %28 = arith.divf %26, %27 : f32
      %29 = arith.mulf %25, %25 : f32
      %30 = arith.subf %28, %29 : f32
      %31 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %32:2 = scf.if %31 -> (f32, f32) {
        %33 = arith.subf %cst, %arg6 : f32
        %34 = affine.load %arg3[symbol(%1)] : memref<?xf32>
        %35 = arith.mulf %33, %34 : f32
        %36 = arith.mulf %arg6, %25 : f32
        %37 = arith.addf %35, %36 : f32
        affine.store %37, %arg3[symbol(%1)] : memref<?xf32>
        %38 = arith.subf %cst, %arg6 : f32
        %39 = affine.load %arg4[symbol(%1)] : memref<?xf32>
        %40 = arith.mulf %38, %39 : f32
        %41 = arith.mulf %arg6, %30 : f32
        %42 = arith.addf %40, %41 : f32
        affine.store %42, %arg4[symbol(%1)] : memref<?xf32>
        scf.yield %30, %25 : f32, f32
      } else {
        %33 = affine.load %arg3[symbol(%1)] : memref<?xf32>
        %34 = affine.load %arg4[symbol(%1)] : memref<?xf32>
        scf.yield %34, %33 : f32, f32
      }
      affine.store %32#1, %alloca[0] : memref<2xf32, 5>
      affine.store %32#0, %alloca[1] : memref<2xf32, 5>
    }
    nvvm.barrier0
    %10 = affine.load %alloca[0] : memref<2xf32, 5>
    %11 = affine.load %alloca[1] : memref<2xf32, 5>
    %12 = arith.addf %11, %arg7 : f32
    %13 = math.rsqrt %12 : f32
    %14 = affine.load %arg1[symbol(%1)] : memref<?xf32>
    %15 = affine.load %arg2[symbol(%1)] : memref<?xf32>
    %16 = arith.index_cast %9 : i32 to index
    %17 = arith.muli %arg11, %arg12 : i32
    %18 = arith.muli %arg11, %arg12 : i32
    %19 = arith.subi %16, %3 : index
    %20 = arith.subi %6, %c1 : index
    %21 = arith.addi %20, %19 : index
    %22 = arith.divui %21, %6 : index
    affine.for %arg13 = 0 to %22 {
      %23 = arith.muli %arg13, %6 : index
      %24 = arith.divui %23, %6 : index
      %25 = arith.muli %24, %6 : index
      %26 = arith.addi %3, %25 : index
      %27 = arith.index_cast %26 : index to i32
      %28 = arith.divsi %27, %17 : i32
      %29 = arith.remsi %27, %18 : i32
      %30 = arith.divsi %29, %arg12 : i32
      %31 = arith.remsi %29, %arg12 : i32
      %32 = arith.muli %28, %arg10 : i32
      %33 = arith.addi %32, %2 : i32
      %34 = arith.muli %33, %arg11 : i32
      %35 = arith.addi %34, %30 : i32
      %36 = arith.muli %35, %arg12 : i32
      %37 = arith.addi %36, %31 : i32
      %38 = arith.index_cast %37 : i32 to index
      %39 = memref.load %arg0[%38] : memref<?xf32>
      %40 = arith.index_cast %37 : i32 to index
      %41 = func.call @_Z14normalizeValuefffff(%39, %10, %13, %14, %15) : (f32, f32, f32, f32, f32) -> f32
      memref.store %41, %arg8[%40] : memref<?xf32>
    }
    return
  }
  func.func private @_Z16__shfl_down_syncjfji(%arg0: i32, %arg1: f32, %arg2: i32, %arg3: i32) -> f32 attributes {llvm.linkage = #llvm.linkage<linkonce_odr>, polygeist.device_only_func = "1"} {
    %c31_i32 = arith.constant 31 : i32
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %0 = arith.subi %c32_i32, %arg3 : i32
    %1 = arith.shli %0, %c8_i32 : i32
    %2 = arith.ori %1, %c31_i32 : i32
    %3 = call @__nvvm_shfl_sync_down_f32(%arg0, %arg1, %arg2, %2) : (i32, f32, i32, i32) -> f32
    return %3 : f32
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  func.func private @_Z14normalizeValuefffff(%arg0: f32, %arg1: f32, %arg2: f32, %arg3: f32, %arg4: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<linkonce_odr>, polygeist.device_only_func = "1"} {
    %0 = arith.subf %arg0, %arg1 : f32
    %1 = arith.mulf %0, %arg2 : f32
    %2 = arith.mulf %1, %arg3 : f32
    %3 = arith.addf %2, %arg4 : f32
    return %3 : f32
  }
}
[ict-debug] driver.cc: After return 5, module: end

[ict-debug] driver.cc: After return 6, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c16_i32 = arith.constant 16 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c31_i32 = arith.constant 31 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i32 = arith.constant 0 : i32
    %c32_i32 = arith.constant 32 : i32
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.remui %1, %c32_i32 : i32
    %3 = arith.cmpi eq, %2, %c0_i32 : i32
    %4 = arith.divui %1, %c32_i32 : i32
    %5 = arith.index_cast %4 : i32 to index
    %6 = affine.load %arg0[0] : memref<?xf32>
    %7:2 = scf.while (%arg2 = %c16_i32, %arg3 = %6) : (i32, f32) -> (f32, i32) {
      %11 = arith.cmpi sgt, %arg2, %c0_i32 : i32
      scf.condition(%11) %arg3, %arg2 : f32, i32
    } do {
    ^bb0(%arg2: f32, %arg3: i32):
      %11 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg2, %arg3, %c31_i32) : (i32, f32, i32, i32) -> f32
      %12 = arith.addf %arg2, %11 : f32
      %13 = arith.divsi %arg3, %c2_i32 : i32
      scf.yield %13, %12 : i32, f32
    }
    %8 = affine.load %arg1[0] : memref<?xf32>
    %9:2 = scf.while (%arg2 = %c16_i32, %arg3 = %8) : (i32, f32) -> (f32, i32) {
      %11 = arith.cmpi sgt, %arg2, %c0_i32 : i32
      scf.condition(%11) %arg3, %arg2 : f32, i32
    } do {
    ^bb0(%arg2: f32, %arg3: i32):
      %11 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg2, %arg3, %c31_i32) : (i32, f32, i32, i32) -> f32
      %12 = arith.addf %arg2, %11 : f32
      %13 = arith.divsi %arg3, %c2_i32 : i32
      scf.yield %13, %12 : i32, f32
    }
    scf.if %3 {
      affine.store %7#0, %alloca_0[symbol(%5)] : memref<32xf32, 5>
      affine.store %9#0, %alloca[symbol(%5)] : memref<32xf32, 5>
    }
    nvvm.barrier0
    %10 = arith.cmpi eq, %1, %c0_i32 : i32
    scf.if %10 {
      %11 = gpu.block_dim  x
      %12 = arith.index_cast %11 : index to i32
      %13 = arith.addi %12, %c31_i32 : i32
      %14 = arith.divui %13, %c32_i32 : i32
      %15 = arith.index_cast %14 : i32 to index
      %16:2 = scf.for %arg2 = %c0 to %15 step %c1 iter_args(%arg3 = %cst, %arg4 = %cst) -> (f32, f32) {
        %17 = memref.load %alloca_0[%arg2] : memref<32xf32, 5>
        %18 = arith.addf %arg4, %17 : f32
        %19 = memref.load %alloca[%arg2] : memref<32xf32, 5>
        %20 = arith.addf %arg3, %19 : f32
        scf.yield %20, %18 : f32, f32
      }
      affine.store %16#1, %arg0[0] : memref<?xf32>
      affine.store %16#0, %arg1[0] : memref<?xf32>
    }
    nvvm.barrier0
    return
  }
  func.func private @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %cst_0 = arith.constant 1.000000e+00 : f32
    %c0_i8 = arith.constant 0 : i8
    %c0_i32 = arith.constant 0 : i32
    %0 = llvm.mlir.undef : f32
    %alloca = memref.alloca() : memref<2xf32, 5>
    %alloca_1 = memref.alloca() : memref<1xf32>
    %cast = memref.cast %alloca_1 : memref<1xf32> to memref<?xf32>
    affine.store %0, %alloca_1[0] : memref<1xf32>
    %alloca_2 = memref.alloca() : memref<1xf32>
    %cast_3 = memref.cast %alloca_2 : memref<1xf32> to memref<?xf32>
    affine.store %0, %alloca_2[0] : memref<1xf32>
    %1 = gpu.block_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.thread_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = arith.cmpi eq, %4, %c0_i32 : i32
    %6 = gpu.block_dim  x
    %7 = arith.muli %arg9, %arg11 : i32
    %8 = arith.muli %7, %arg12 : i32
    affine.store %cst, %alloca_2[0] : memref<1xf32>
    affine.store %cst, %alloca_1[0] : memref<1xf32>
    %9 = arith.index_cast %8 : i32 to index
    %10 = arith.muli %arg11, %arg12 : i32
    %11 = arith.subi %9, %3 : index
    %12 = arith.subi %6, %c1 : index
    %13 = arith.addi %12, %11 : index
    %14 = arith.divui %13, %6 : index
    affine.for %arg13 = 0 to %14 {
      %21 = arith.muli %arg13, %6 : index
      %22 = arith.addi %3, %21 : index
      %23 = arith.index_cast %22 : index to i32
      %24 = arith.divsi %23, %10 : i32
      %25 = arith.remsi %23, %10 : i32
      %26 = arith.divsi %25, %arg12 : i32
      %27 = arith.remsi %25, %arg12 : i32
      %28 = arith.muli %24, %arg10 : i32
      %29 = arith.addi %28, %2 : i32
      %30 = arith.muli %29, %arg11 : i32
      %31 = arith.addi %30, %26 : i32
      %32 = arith.muli %31, %arg12 : i32
      %33 = arith.addi %32, %27 : i32
      %34 = arith.index_cast %33 : i32 to index
      %35 = memref.load %arg0[%34] : memref<?xf32>
      %36 = affine.load %alloca_2[0] : memref<1xf32>
      %37 = arith.addf %36, %35 : f32
      affine.store %37, %alloca_2[0] : memref<1xf32>
      %38 = arith.mulf %35, %35 : f32
      %39 = affine.load %alloca_1[0] : memref<1xf32>
      %40 = arith.addf %39, %38 : f32
      affine.store %40, %alloca_1[0] : memref<1xf32>
    }
    call @_Z14blockReduceSumRfS_(%cast_3, %cast) : (memref<?xf32>, memref<?xf32>) -> ()
    scf.if %5 {
      %21 = affine.load %alloca_2[0] : memref<1xf32>
      %22 = arith.sitofp %8 : i32 to f32
      %23 = arith.divf %21, %22 : f32
      %24 = affine.load %alloca_1[0] : memref<1xf32>
      %25 = arith.divf %24, %22 : f32
      %26 = arith.mulf %23, %23 : f32
      %27 = arith.subf %25, %26 : f32
      %28 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %29:2 = scf.if %28 -> (f32, f32) {
        %30 = arith.subf %cst_0, %arg6 : f32
        %31 = affine.load %arg3[symbol(%1)] : memref<?xf32>
        %32 = arith.mulf %30, %31 : f32
        %33 = arith.mulf %arg6, %23 : f32
        %34 = arith.addf %32, %33 : f32
        affine.store %34, %arg3[symbol(%1)] : memref<?xf32>
        %35 = affine.load %arg4[symbol(%1)] : memref<?xf32>
        %36 = arith.mulf %30, %35 : f32
        %37 = arith.mulf %arg6, %27 : f32
        %38 = arith.addf %36, %37 : f32
        affine.store %38, %arg4[symbol(%1)] : memref<?xf32>
        scf.yield %27, %23 : f32, f32
      } else {
        %30 = affine.load %arg3[symbol(%1)] : memref<?xf32>
        %31 = affine.load %arg4[symbol(%1)] : memref<?xf32>
        scf.yield %31, %30 : f32, f32
      }
      affine.store %29#1, %alloca[0] : memref<2xf32, 5>
      affine.store %29#0, %alloca[1] : memref<2xf32, 5>
    }
    nvvm.barrier0
    %15 = affine.load %alloca[0] : memref<2xf32, 5>
    %16 = affine.load %alloca[1] : memref<2xf32, 5>
    %17 = arith.addf %16, %arg7 : f32
    %18 = math.rsqrt %17 : f32
    %19 = affine.load %arg1[symbol(%1)] : memref<?xf32>
    %20 = affine.load %arg2[symbol(%1)] : memref<?xf32>
    affine.for %arg13 = 0 to %14 {
      %21 = arith.muli %arg13, %6 : index
      %22 = arith.addi %3, %21 : index
      %23 = arith.index_cast %22 : index to i32
      %24 = arith.divsi %23, %10 : i32
      %25 = arith.remsi %23, %10 : i32
      %26 = arith.divsi %25, %arg12 : i32
      %27 = arith.remsi %25, %arg12 : i32
      %28 = arith.muli %24, %arg10 : i32
      %29 = arith.addi %28, %2 : i32
      %30 = arith.muli %29, %arg11 : i32
      %31 = arith.addi %30, %26 : i32
      %32 = arith.muli %31, %arg12 : i32
      %33 = arith.addi %32, %27 : i32
      %34 = arith.index_cast %33 : i32 to index
      %35 = memref.load %arg0[%34] : memref<?xf32>
      %36 = arith.subf %35, %15 : f32
      %37 = arith.mulf %36, %18 : f32
      %38 = arith.mulf %37, %19 : f32
      %39 = arith.addf %38, %20 : f32
      memref.store %39, %arg8[%34] : memref<?xf32>
    }
    return
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
[ict-debug] driver.cc: After return 6, module: end

WrapAndReplaceBarrierPass::runOnOperation(): before execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c16_i32 = arith.constant 16 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c2_i32 = arith.constant 2 : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c31_i32 = arith.constant 31 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i32 = arith.constant 0 : i32
    %c32_i32 = arith.constant 32 : i32
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.remui %1, %c32_i32 : i32
    %3 = arith.cmpi eq, %2, %c0_i32 : i32
    %4 = arith.divui %1, %c32_i32 : i32
    %5 = arith.index_cast %4 : i32 to index
    %6 = memref.load %arg0[%c0] : memref<?xf32>
    %7:2 = scf.while (%arg2 = %c16_i32, %arg3 = %6) : (i32, f32) -> (f32, i32) {
      %11 = arith.cmpi sgt, %arg2, %c0_i32 : i32
      scf.condition(%11) %arg3, %arg2 : f32, i32
    } do {
    ^bb0(%arg2: f32, %arg3: i32):
      %11 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg2, %arg3, %c31_i32) : (i32, f32, i32, i32) -> f32
      %12 = arith.addf %arg2, %11 : f32
      %13 = arith.divsi %arg3, %c2_i32 : i32
      scf.yield %13, %12 : i32, f32
    }
    %8 = memref.load %arg1[%c0] : memref<?xf32>
    %9:2 = scf.while (%arg2 = %c16_i32, %arg3 = %8) : (i32, f32) -> (f32, i32) {
      %11 = arith.cmpi sgt, %arg2, %c0_i32 : i32
      scf.condition(%11) %arg3, %arg2 : f32, i32
    } do {
    ^bb0(%arg2: f32, %arg3: i32):
      %11 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg2, %arg3, %c31_i32) : (i32, f32, i32, i32) -> f32
      %12 = arith.addf %arg2, %11 : f32
      %13 = arith.divsi %arg3, %c2_i32 : i32
      scf.yield %13, %12 : i32, f32
    }
    scf.if %3 {
      memref.store %7#0, %alloca_0[%5] : memref<32xf32, 5>
      memref.store %9#0, %alloca[%5] : memref<32xf32, 5>
    }
    nvvm.barrier0
    %10 = arith.cmpi eq, %1, %c0_i32 : i32
    scf.if %10 {
      %11 = gpu.block_dim  x
      %12 = arith.index_cast %11 : index to i32
      %13 = arith.addi %12, %c31_i32 : i32
      %14 = arith.divui %13, %c32_i32 : i32
      %15 = arith.index_cast %14 : i32 to index
      %16:2 = scf.for %arg2 = %c0 to %15 step %c1 iter_args(%arg3 = %cst, %arg4 = %cst) -> (f32, f32) {
        %17 = memref.load %alloca_0[%arg2] : memref<32xf32, 5>
        %18 = arith.addf %arg4, %17 : f32
        %19 = memref.load %alloca[%arg2] : memref<32xf32, 5>
        %20 = arith.addf %arg3, %19 : f32
        scf.yield %20, %18 : f32, f32
      }
      memref.store %16#1, %arg0[%c0] : memref<?xf32>
      memref.store %16#0, %arg1[%c0] : memref<?xf32>
    }
    nvvm.barrier0
    return
  }
  func.func private @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %cst_0 = arith.constant 1.000000e+00 : f32
    %c0_i8 = arith.constant 0 : i8
    %c0_i32 = arith.constant 0 : i32
    %0 = llvm.mlir.undef : f32
    %alloca = memref.alloca() : memref<2xf32, 5>
    %alloca_1 = memref.alloca() : memref<1xf32>
    %cast = memref.cast %alloca_1 : memref<1xf32> to memref<?xf32>
    memref.store %0, %alloca_1[%c0] : memref<1xf32>
    %alloca_2 = memref.alloca() : memref<1xf32>
    %cast_3 = memref.cast %alloca_2 : memref<1xf32> to memref<?xf32>
    memref.store %0, %alloca_2[%c0] : memref<1xf32>
    %1 = gpu.block_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.thread_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = arith.cmpi eq, %4, %c0_i32 : i32
    %6 = gpu.block_dim  x
    %7 = arith.muli %arg9, %arg11 : i32
    %8 = arith.muli %7, %arg12 : i32
    memref.store %cst, %alloca_2[%c0] : memref<1xf32>
    memref.store %cst, %alloca_1[%c0] : memref<1xf32>
    %9 = arith.index_cast %8 : i32 to index
    %10 = arith.muli %arg11, %arg12 : i32
    %11 = arith.subi %9, %3 : index
    %12 = arith.subi %6, %c1 : index
    %13 = arith.addi %12, %11 : index
    %14 = arith.divui %13, %6 : index
    scf.for %arg13 = %c0 to %14 step %c1 {
      %21 = arith.muli %arg13, %6 : index
      %22 = arith.addi %3, %21 : index
      %23 = arith.index_cast %22 : index to i32
      %24 = arith.divsi %23, %10 : i32
      %25 = arith.remsi %23, %10 : i32
      %26 = arith.divsi %25, %arg12 : i32
      %27 = arith.remsi %25, %arg12 : i32
      %28 = arith.muli %24, %arg10 : i32
      %29 = arith.addi %28, %2 : i32
      %30 = arith.muli %29, %arg11 : i32
      %31 = arith.addi %30, %26 : i32
      %32 = arith.muli %31, %arg12 : i32
      %33 = arith.addi %32, %27 : i32
      %34 = arith.index_cast %33 : i32 to index
      %35 = memref.load %arg0[%34] : memref<?xf32>
      %36 = memref.load %alloca_2[%c0] : memref<1xf32>
      %37 = arith.addf %36, %35 : f32
      memref.store %37, %alloca_2[%c0] : memref<1xf32>
      %38 = arith.mulf %35, %35 : f32
      %39 = memref.load %alloca_1[%c0] : memref<1xf32>
      %40 = arith.addf %39, %38 : f32
      memref.store %40, %alloca_1[%c0] : memref<1xf32>
    }
    call @_Z14blockReduceSumRfS_(%cast_3, %cast) : (memref<?xf32>, memref<?xf32>) -> ()
    scf.if %5 {
      %21 = memref.load %alloca_2[%c0] : memref<1xf32>
      %22 = arith.sitofp %8 : i32 to f32
      %23 = arith.divf %21, %22 : f32
      %24 = memref.load %alloca_1[%c0] : memref<1xf32>
      %25 = arith.divf %24, %22 : f32
      %26 = arith.mulf %23, %23 : f32
      %27 = arith.subf %25, %26 : f32
      %28 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %29:2 = scf.if %28 -> (f32, f32) {
        %30 = arith.subf %cst_0, %arg6 : f32
        %31 = memref.load %arg3[%1] : memref<?xf32>
        %32 = arith.mulf %30, %31 : f32
        %33 = arith.mulf %arg6, %23 : f32
        %34 = arith.addf %32, %33 : f32
        memref.store %34, %arg3[%1] : memref<?xf32>
        %35 = memref.load %arg4[%1] : memref<?xf32>
        %36 = arith.mulf %30, %35 : f32
        %37 = arith.mulf %arg6, %27 : f32
        %38 = arith.addf %36, %37 : f32
        memref.store %38, %arg4[%1] : memref<?xf32>
        scf.yield %27, %23 : f32, f32
      } else {
        %30 = memref.load %arg3[%1] : memref<?xf32>
        %31 = memref.load %arg4[%1] : memref<?xf32>
        scf.yield %31, %30 : f32, f32
      }
      memref.store %29#1, %alloca[%c0] : memref<2xf32, 5>
      memref.store %29#0, %alloca[%c1] : memref<2xf32, 5>
    }
    nvvm.barrier0
    %15 = memref.load %alloca[%c0] : memref<2xf32, 5>
    %16 = memref.load %alloca[%c1] : memref<2xf32, 5>
    %17 = arith.addf %16, %arg7 : f32
    %18 = math.rsqrt %17 : f32
    %19 = memref.load %arg1[%1] : memref<?xf32>
    %20 = memref.load %arg2[%1] : memref<?xf32>
    scf.for %arg13 = %c0 to %14 step %c1 {
      %21 = arith.muli %arg13, %6 : index
      %22 = arith.addi %3, %21 : index
      %23 = arith.index_cast %22 : index to i32
      %24 = arith.divsi %23, %10 : i32
      %25 = arith.remsi %23, %10 : i32
      %26 = arith.divsi %25, %arg12 : i32
      %27 = arith.remsi %25, %arg12 : i32
      %28 = arith.muli %24, %arg10 : i32
      %29 = arith.addi %28, %2 : i32
      %30 = arith.muli %29, %arg11 : i32
      %31 = arith.addi %30, %26 : i32
      %32 = arith.muli %31, %arg12 : i32
      %33 = arith.addi %32, %27 : i32
      %34 = arith.index_cast %33 : i32 to index
      %35 = memref.load %arg0[%34] : memref<?xf32>
      %36 = arith.subf %35, %15 : f32
      %37 = arith.mulf %36, %18 : f32
      %38 = arith.mulf %37, %19 : f32
      %39 = arith.addf %38, %20 : f32
      memref.store %39, %arg8[%34] : memref<?xf32>
    }
    return
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
WrapAndReplaceBarrierPass::runOnOperation(): before execute: end
[ict-debug] WrapAndReplaceBarrierPass::runOnOperation(): Function name: __nvvm_shfl_sync_down_f32. func.getBlocks().size() == 0! this function is empty, skip it.

WrapAndReplaceBarrierPass::runOnOperation(): after execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
      %c16_i32 = arith.constant 16 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c2_i32 = arith.constant 2 : i32
      %c0_1 = arith.constant 0 : index
      %c1_2 = arith.constant 1 : index
      %c31_i32 = arith.constant 31 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c0_i32 = arith.constant 0 : i32
      %c32_i32 = arith.constant 32 : i32
      %0 = arith.index_cast %arg2 : index to i32
      %1 = arith.remui %0, %c32_i32 : i32
      %2 = arith.cmpi eq, %1, %c0_i32 : i32
      %3 = arith.divui %0, %c32_i32 : i32
      %4 = arith.index_cast %3 : i32 to index
      %5 = memref.load %arg0[%c0_1] : memref<?xf32>
      %6:2 = scf.while (%arg3 = %c16_i32, %arg4 = %5) : (i32, f32) -> (f32, i32) {
        %10 = arith.cmpi sgt, %arg3, %c0_i32 : i32
        scf.condition(%10) %arg4, %arg3 : f32, i32
      } do {
      ^bb0(%arg3: f32, %arg4: i32):
        %10 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
        %11 = arith.addf %arg3, %10 : f32
        %12 = arith.divsi %arg4, %c2_i32 : i32
        scf.yield %12, %11 : i32, f32
      }
      %7 = memref.load %arg1[%c0_1] : memref<?xf32>
      %8:2 = scf.while (%arg3 = %c16_i32, %arg4 = %7) : (i32, f32) -> (f32, i32) {
        %10 = arith.cmpi sgt, %arg3, %c0_i32 : i32
        scf.condition(%10) %arg4, %arg3 : f32, i32
      } do {
      ^bb0(%arg3: f32, %arg4: i32):
        %10 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
        %11 = arith.addf %arg3, %10 : f32
        %12 = arith.divsi %arg4, %c2_i32 : i32
        scf.yield %12, %11 : i32, f32
      }
      scf.if %2 {
        memref.store %6#0, %alloca_0[%4] : memref<32xf32, 5>
        memref.store %8#0, %alloca[%4] : memref<32xf32, 5>
      }
      "polygeist.barrier"(%arg2) : (index) -> ()
      %9 = arith.cmpi eq, %0, %c0_i32 : i32
      scf.if %9 {
        %10 = gpu.block_dim  x
        %11 = arith.index_cast %10 : index to i32
        %12 = arith.addi %11, %c31_i32 : i32
        %13 = arith.divui %12, %c32_i32 : i32
        %14 = arith.index_cast %13 : i32 to index
        %15:2 = scf.for %arg3 = %c0_1 to %14 step %c1_2 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
          %16 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
          %17 = arith.addf %arg5, %16 : f32
          %18 = memref.load %alloca[%arg3] : memref<32xf32, 5>
          %19 = arith.addf %arg4, %18 : f32
          scf.yield %19, %17 : f32, f32
        }
        memref.store %15#1, %arg0[%c0_1] : memref<?xf32>
        memref.store %15#0, %arg1[%c0_1] : memref<?xf32>
      }
      "polygeist.barrier"(%arg2) : (index) -> ()
      scf.yield
    }
    return
  }
  func.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<2xf32, 5>
    scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c1_1 = arith.constant 1 : index
      %cst_2 = arith.constant 1.000000e+00 : f32
      %c0_i8 = arith.constant 0 : i8
      %c0_i32 = arith.constant 0 : i32
      %0 = llvm.mlir.undef : f32
      %alloca_3 = memref.alloca() : memref<1xf32>
      %cast = memref.cast %alloca_3 : memref<1xf32> to memref<?xf32>
      memref.store %0, %alloca_3[%c0_0] : memref<1xf32>
      %alloca_4 = memref.alloca() : memref<1xf32>
      %cast_5 = memref.cast %alloca_4 : memref<1xf32> to memref<?xf32>
      memref.store %0, %alloca_4[%c0_0] : memref<1xf32>
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.index_cast %arg13 : index to i32
      %4 = arith.cmpi eq, %3, %c0_i32 : i32
      %5 = gpu.block_dim  x
      %6 = arith.muli %arg9, %arg11 : i32
      %7 = arith.muli %6, %arg12 : i32
      memref.store %cst, %alloca_4[%c0_0] : memref<1xf32>
      memref.store %cst, %alloca_3[%c0_0] : memref<1xf32>
      %8 = arith.index_cast %7 : i32 to index
      %9 = arith.muli %arg11, %arg12 : i32
      %10 = arith.subi %8, %arg13 : index
      %11 = arith.subi %5, %c1_1 : index
      %12 = arith.addi %11, %10 : index
      %13 = arith.divui %12, %5 : index
      scf.for %arg14 = %c0_0 to %13 step %c1_1 {
        %20 = arith.muli %arg14, %5 : index
        %21 = arith.addi %arg13, %20 : index
        %22 = arith.index_cast %21 : index to i32
        %23 = arith.divsi %22, %9 : i32
        %24 = arith.remsi %22, %9 : i32
        %25 = arith.divsi %24, %arg12 : i32
        %26 = arith.remsi %24, %arg12 : i32
        %27 = arith.muli %23, %arg10 : i32
        %28 = arith.addi %27, %2 : i32
        %29 = arith.muli %28, %arg11 : i32
        %30 = arith.addi %29, %25 : i32
        %31 = arith.muli %30, %arg12 : i32
        %32 = arith.addi %31, %26 : i32
        %33 = arith.index_cast %32 : i32 to index
        %34 = memref.load %arg0[%33] : memref<?xf32>
        %35 = memref.load %alloca_4[%c0_0] : memref<1xf32>
        %36 = arith.addf %35, %34 : f32
        memref.store %36, %alloca_4[%c0_0] : memref<1xf32>
        %37 = arith.mulf %34, %34 : f32
        %38 = memref.load %alloca_3[%c0_0] : memref<1xf32>
        %39 = arith.addf %38, %37 : f32
        memref.store %39, %alloca_3[%c0_0] : memref<1xf32>
      }
      func.call @_Z14blockReduceSumRfS_(%cast_5, %cast) : (memref<?xf32>, memref<?xf32>) -> ()
      scf.if %4 {
        %20 = memref.load %alloca_4[%c0_0] : memref<1xf32>
        %21 = arith.sitofp %7 : i32 to f32
        %22 = arith.divf %20, %21 : f32
        %23 = memref.load %alloca_3[%c0_0] : memref<1xf32>
        %24 = arith.divf %23, %21 : f32
        %25 = arith.mulf %22, %22 : f32
        %26 = arith.subf %24, %25 : f32
        %27 = arith.cmpi ne, %arg5, %c0_i8 : i8
        %28:2 = scf.if %27 -> (f32, f32) {
          %29 = arith.subf %cst_2, %arg6 : f32
          %30 = memref.load %arg3[%1] : memref<?xf32>
          %31 = arith.mulf %29, %30 : f32
          %32 = arith.mulf %arg6, %22 : f32
          %33 = arith.addf %31, %32 : f32
          memref.store %33, %arg3[%1] : memref<?xf32>
          %34 = memref.load %arg4[%1] : memref<?xf32>
          %35 = arith.mulf %29, %34 : f32
          %36 = arith.mulf %arg6, %26 : f32
          %37 = arith.addf %35, %36 : f32
          memref.store %37, %arg4[%1] : memref<?xf32>
          scf.yield %26, %22 : f32, f32
        } else {
          %29 = memref.load %arg3[%1] : memref<?xf32>
          %30 = memref.load %arg4[%1] : memref<?xf32>
          scf.yield %30, %29 : f32, f32
        }
        memref.store %28#1, %alloca[%c0_0] : memref<2xf32, 5>
        memref.store %28#0, %alloca[%c1_1] : memref<2xf32, 5>
      }
      "polygeist.barrier"(%arg13) : (index) -> ()
      %14 = memref.load %alloca[%c0_0] : memref<2xf32, 5>
      %15 = memref.load %alloca[%c1_1] : memref<2xf32, 5>
      %16 = arith.addf %15, %arg7 : f32
      %17 = math.rsqrt %16 : f32
      %18 = memref.load %arg1[%1] : memref<?xf32>
      %19 = memref.load %arg2[%1] : memref<?xf32>
      scf.for %arg14 = %c0_0 to %13 step %c1_1 {
        %20 = arith.muli %arg14, %5 : index
        %21 = arith.addi %arg13, %20 : index
        %22 = arith.index_cast %21 : index to i32
        %23 = arith.divsi %22, %9 : i32
        %24 = arith.remsi %22, %9 : i32
        %25 = arith.divsi %24, %arg12 : i32
        %26 = arith.remsi %24, %arg12 : i32
        %27 = arith.muli %23, %arg10 : i32
        %28 = arith.addi %27, %2 : i32
        %29 = arith.muli %28, %arg11 : i32
        %30 = arith.addi %29, %25 : i32
        %31 = arith.muli %30, %arg12 : i32
        %32 = arith.addi %31, %26 : i32
        %33 = arith.index_cast %32 : i32 to index
        %34 = memref.load %arg0[%33] : memref<?xf32>
        %35 = arith.subf %34, %14 : f32
        %36 = arith.mulf %35, %17 : f32
        %37 = arith.mulf %36, %18 : f32
        %38 = arith.addf %37, %19 : f32
        memref.store %38, %arg8[%33] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
WrapAndReplaceBarrierPass::runOnOperation(): after execute: end
[ict-debug] driver.cc: After return 7, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
      %c16_i32 = arith.constant 16 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c2_i32 = arith.constant 2 : i32
      %c0_1 = arith.constant 0 : index
      %c1_2 = arith.constant 1 : index
      %c31_i32 = arith.constant 31 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c0_i32 = arith.constant 0 : i32
      %c32_i32 = arith.constant 32 : i32
      %0 = arith.index_cast %arg2 : index to i32
      %1 = arith.remui %0, %c32_i32 : i32
      %2 = arith.cmpi eq, %1, %c0_i32 : i32
      %3 = arith.divui %0, %c32_i32 : i32
      %4 = arith.index_cast %3 : i32 to index
      %5 = memref.load %arg0[%c0_1] : memref<?xf32>
      %6:2 = scf.while (%arg3 = %c16_i32, %arg4 = %5) : (i32, f32) -> (f32, i32) {
        %10 = arith.cmpi sgt, %arg3, %c0_i32 : i32
        scf.condition(%10) %arg4, %arg3 : f32, i32
      } do {
      ^bb0(%arg3: f32, %arg4: i32):
        %10 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
        %11 = arith.addf %arg3, %10 : f32
        %12 = arith.divsi %arg4, %c2_i32 : i32
        scf.yield %12, %11 : i32, f32
      }
      %7 = memref.load %arg1[%c0_1] : memref<?xf32>
      %8:2 = scf.while (%arg3 = %c16_i32, %arg4 = %7) : (i32, f32) -> (f32, i32) {
        %10 = arith.cmpi sgt, %arg3, %c0_i32 : i32
        scf.condition(%10) %arg4, %arg3 : f32, i32
      } do {
      ^bb0(%arg3: f32, %arg4: i32):
        %10 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
        %11 = arith.addf %arg3, %10 : f32
        %12 = arith.divsi %arg4, %c2_i32 : i32
        scf.yield %12, %11 : i32, f32
      }
      scf.if %2 {
        memref.store %6#0, %alloca_0[%4] : memref<32xf32, 5>
        memref.store %8#0, %alloca[%4] : memref<32xf32, 5>
      }
      "polygeist.barrier"(%arg2) : (index) -> ()
      %9 = arith.cmpi eq, %0, %c0_i32 : i32
      scf.if %9 {
        %10 = gpu.block_dim  x
        %11 = arith.index_cast %10 : index to i32
        %12 = arith.addi %11, %c31_i32 : i32
        %13 = arith.divui %12, %c32_i32 : i32
        %14 = arith.index_cast %13 : i32 to index
        %15:2 = scf.for %arg3 = %c0_1 to %14 step %c1_2 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
          %16 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
          %17 = arith.addf %arg5, %16 : f32
          %18 = memref.load %alloca[%arg3] : memref<32xf32, 5>
          %19 = arith.addf %arg4, %18 : f32
          scf.yield %19, %17 : f32, f32
        }
        memref.store %15#1, %arg0[%c0_1] : memref<?xf32>
        memref.store %15#0, %arg1[%c0_1] : memref<?xf32>
      }
      "polygeist.barrier"(%arg2) : (index) -> ()
      scf.yield
    }
    return
  }
  func.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<2xf32, 5>
    scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %c1_1 = arith.constant 1 : index
      %cst_2 = arith.constant 1.000000e+00 : f32
      %c0_i8 = arith.constant 0 : i8
      %c0_i32 = arith.constant 0 : i32
      %0 = llvm.mlir.undef : f32
      %alloca_3 = memref.alloca() : memref<1xf32>
      %cast = memref.cast %alloca_3 : memref<1xf32> to memref<?xf32>
      memref.store %0, %alloca_3[%c0_0] : memref<1xf32>
      %alloca_4 = memref.alloca() : memref<1xf32>
      %cast_5 = memref.cast %alloca_4 : memref<1xf32> to memref<?xf32>
      memref.store %0, %alloca_4[%c0_0] : memref<1xf32>
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.index_cast %arg13 : index to i32
      %4 = arith.cmpi eq, %3, %c0_i32 : i32
      %5 = gpu.block_dim  x
      %6 = arith.muli %arg9, %arg11 : i32
      %7 = arith.muli %6, %arg12 : i32
      memref.store %cst, %alloca_4[%c0_0] : memref<1xf32>
      memref.store %cst, %alloca_3[%c0_0] : memref<1xf32>
      %8 = arith.index_cast %7 : i32 to index
      %9 = arith.muli %arg11, %arg12 : i32
      %10 = arith.subi %8, %arg13 : index
      %11 = arith.subi %5, %c1_1 : index
      %12 = arith.addi %11, %10 : index
      %13 = arith.divui %12, %5 : index
      scf.for %arg14 = %c0_0 to %13 step %c1_1 {
        %20 = arith.muli %arg14, %5 : index
        %21 = arith.addi %arg13, %20 : index
        %22 = arith.index_cast %21 : index to i32
        %23 = arith.divsi %22, %9 : i32
        %24 = arith.remsi %22, %9 : i32
        %25 = arith.divsi %24, %arg12 : i32
        %26 = arith.remsi %24, %arg12 : i32
        %27 = arith.muli %23, %arg10 : i32
        %28 = arith.addi %27, %2 : i32
        %29 = arith.muli %28, %arg11 : i32
        %30 = arith.addi %29, %25 : i32
        %31 = arith.muli %30, %arg12 : i32
        %32 = arith.addi %31, %26 : i32
        %33 = arith.index_cast %32 : i32 to index
        %34 = memref.load %arg0[%33] : memref<?xf32>
        %35 = memref.load %alloca_4[%c0_0] : memref<1xf32>
        %36 = arith.addf %35, %34 : f32
        memref.store %36, %alloca_4[%c0_0] : memref<1xf32>
        %37 = arith.mulf %34, %34 : f32
        %38 = memref.load %alloca_3[%c0_0] : memref<1xf32>
        %39 = arith.addf %38, %37 : f32
        memref.store %39, %alloca_3[%c0_0] : memref<1xf32>
      }
      func.call @_Z14blockReduceSumRfS_(%cast_5, %cast) : (memref<?xf32>, memref<?xf32>) -> ()
      scf.if %4 {
        %20 = memref.load %alloca_4[%c0_0] : memref<1xf32>
        %21 = arith.sitofp %7 : i32 to f32
        %22 = arith.divf %20, %21 : f32
        %23 = memref.load %alloca_3[%c0_0] : memref<1xf32>
        %24 = arith.divf %23, %21 : f32
        %25 = arith.mulf %22, %22 : f32
        %26 = arith.subf %24, %25 : f32
        %27 = arith.cmpi ne, %arg5, %c0_i8 : i8
        %28:2 = scf.if %27 -> (f32, f32) {
          %29 = arith.subf %cst_2, %arg6 : f32
          %30 = memref.load %arg3[%1] : memref<?xf32>
          %31 = arith.mulf %29, %30 : f32
          %32 = arith.mulf %arg6, %22 : f32
          %33 = arith.addf %31, %32 : f32
          memref.store %33, %arg3[%1] : memref<?xf32>
          %34 = memref.load %arg4[%1] : memref<?xf32>
          %35 = arith.mulf %29, %34 : f32
          %36 = arith.mulf %arg6, %26 : f32
          %37 = arith.addf %35, %36 : f32
          memref.store %37, %arg4[%1] : memref<?xf32>
          scf.yield %26, %22 : f32, f32
        } else {
          %29 = memref.load %arg3[%1] : memref<?xf32>
          %30 = memref.load %arg4[%1] : memref<?xf32>
          scf.yield %30, %29 : f32, f32
        }
        memref.store %28#1, %alloca[%c0_0] : memref<2xf32, 5>
        memref.store %28#0, %alloca[%c1_1] : memref<2xf32, 5>
      }
      "polygeist.barrier"(%arg13) : (index) -> ()
      %14 = memref.load %alloca[%c0_0] : memref<2xf32, 5>
      %15 = memref.load %alloca[%c1_1] : memref<2xf32, 5>
      %16 = arith.addf %15, %arg7 : f32
      %17 = math.rsqrt %16 : f32
      %18 = memref.load %arg1[%1] : memref<?xf32>
      %19 = memref.load %arg2[%1] : memref<?xf32>
      scf.for %arg14 = %c0_0 to %13 step %c1_1 {
        %20 = arith.muli %arg14, %5 : index
        %21 = arith.addi %arg13, %20 : index
        %22 = arith.index_cast %21 : index to i32
        %23 = arith.divsi %22, %9 : i32
        %24 = arith.remsi %22, %9 : i32
        %25 = arith.divsi %24, %arg12 : i32
        %26 = arith.remsi %24, %arg12 : i32
        %27 = arith.muli %23, %arg10 : i32
        %28 = arith.addi %27, %2 : i32
        %29 = arith.muli %28, %arg11 : i32
        %30 = arith.addi %29, %25 : i32
        %31 = arith.muli %30, %arg12 : i32
        %32 = arith.addi %31, %26 : i32
        %33 = arith.index_cast %32 : i32 to index
        %34 = memref.load %arg0[%33] : memref<?xf32>
        %35 = arith.subf %34, %14 : f32
        %36 = arith.mulf %35, %17 : f32
        %37 = arith.mulf %36, %18 : f32
        %38 = arith.addf %37, %19 : f32
        memref.store %38, %arg8[%33] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
[ict-debug] driver.cc: After return 7, module: end

[ict-debug] driver.cc: Before my pass process:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c31_i32 = arith.constant 31 : i32
    %c2_i32 = arith.constant 2 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c16_i32 = arith.constant 16 : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<32xf32, 5>
    %alloca_0 = memref.alloca() : memref<32xf32, 5>
    %0 = arith.cmpi slt, %c0, %c32 : index
    scf.if %0 {
      %6 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %7 = arith.index_cast %arg2 : index to i32
        %8 = arith.remui %7, %c32_i32 : i32
        %9 = arith.cmpi eq, %8, %c0_i32 : i32
        %10 = arith.divui %7, %c32_i32 : i32
        %11 = arith.index_cast %10 : i32 to index
        %12:2 = scf.while (%arg3 = %c16_i32, %arg4 = %6) : (i32, f32) -> (f32, i32) {
          %15 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%15) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %15 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %16 = arith.addf %arg3, %15 : f32
          %17 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %17, %16 : i32, f32
        }
        %13 = memref.load %arg1[%c0] : memref<?xf32>
        %14:2 = scf.while (%arg3 = %c16_i32, %arg4 = %13) : (i32, f32) -> (f32, i32) {
          %15 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%15) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %15 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %16 = arith.addf %arg3, %15 : f32
          %17 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %17, %16 : i32, f32
        }
        scf.if %9 {
          memref.store %12#0, %alloca_0[%11] : memref<32xf32, 5>
          memref.store %14#0, %alloca[%11] : memref<32xf32, 5>
        }
        scf.yield
      }
    }
    %1 = gpu.block_dim  x
    %2 = arith.index_cast %1 : index to i32
    %3 = arith.addi %2, %c31_i32 : i32
    %4 = arith.divui %3, %c32_i32 : i32
    %5 = arith.index_cast %4 : i32 to index
    scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
      %6 = arith.index_cast %arg2 : index to i32
      %7 = arith.cmpi eq, %6, %c0_i32 : i32
      scf.if %7 {
        %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
          %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
          %10 = arith.addf %arg5, %9 : f32
          %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
          %12 = arith.addf %arg4, %11 : f32
          scf.yield %12, %10 : f32, f32
        }
        memref.store %8#1, %arg0[%c0] : memref<?xf32>
        memref.store %8#0, %arg1[%c0] : memref<?xf32>
      }
      scf.yield
    }
    return
  }
  func.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0_i32 = arith.constant 0 : i32
    %c0_i8 = arith.constant 0 : i8
    %cst = arith.constant 1.000000e+00 : f32
    %cst_0 = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<2xf32, 5>
    %alloca_1 = memref.alloca() : memref<32x1xf32>
    %alloca_2 = memref.alloca() : memref<32x1xf32>
    %0 = llvm.mlir.undef : f32
    %1 = gpu.block_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.block_dim  x
    %4 = arith.muli %arg9, %arg11 : i32
    %5 = arith.muli %4, %arg12 : i32
    %6 = arith.index_cast %5 : i32 to index
    %7 = arith.muli %arg11, %arg12 : i32
    %8 = arith.subi %3, %c1 : index
    %9 = arith.sitofp %5 : i32 to f32
    %10 = arith.cmpi ne, %arg5, %c0_i8 : i8
    %11 = arith.subf %cst, %arg6 : f32
    scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
      %21 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
      memref.store %0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
      %22 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
      memref.store %0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
      %23 = arith.index_cast %arg13 : index to i32
      %24 = arith.cmpi eq, %23, %c0_i32 : i32
      memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
      memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
      %25 = arith.subi %6, %arg13 : index
      %26 = arith.addi %8, %25 : index
      %27 = arith.divui %26, %3 : index
      scf.for %arg14 = %c0 to %27 step %c1 {
        %28 = arith.muli %arg14, %3 : index
        %29 = arith.addi %arg13, %28 : index
        %30 = arith.index_cast %29 : index to i32
        %31 = arith.divsi %30, %7 : i32
        %32 = arith.remsi %30, %7 : i32
        %33 = arith.divsi %32, %arg12 : i32
        %34 = arith.remsi %32, %arg12 : i32
        %35 = arith.muli %31, %arg10 : i32
        %36 = arith.addi %35, %2 : i32
        %37 = arith.muli %36, %arg11 : i32
        %38 = arith.addi %37, %33 : i32
        %39 = arith.muli %38, %arg12 : i32
        %40 = arith.addi %39, %34 : i32
        %41 = arith.index_cast %40 : i32 to index
        %42 = memref.load %arg0[%41] : memref<?xf32>
        %43 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
        %44 = arith.addf %43, %42 : f32
        memref.store %44, %alloca_2[%arg13, %c0] : memref<32x1xf32>
        %45 = arith.mulf %42, %42 : f32
        %46 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %47 = arith.addf %46, %45 : f32
        memref.store %47, %alloca_1[%arg13, %c0] : memref<32x1xf32>
      }
      func.call @_Z14blockReduceSumRfS_(%22, %21) : (memref<?xf32>, memref<?xf32>) -> ()
      scf.if %24 {
        %28 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
        %29 = arith.divf %28, %9 : f32
        %30 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %31 = arith.divf %30, %9 : f32
        %32 = arith.mulf %29, %29 : f32
        %33 = arith.subf %31, %32 : f32
        %34:2 = scf.if %10 -> (f32, f32) {
          %35 = memref.load %arg3[%1] : memref<?xf32>
          %36 = arith.mulf %11, %35 : f32
          %37 = arith.mulf %arg6, %29 : f32
          %38 = arith.addf %36, %37 : f32
          memref.store %38, %arg3[%1] : memref<?xf32>
          %39 = memref.load %arg4[%1] : memref<?xf32>
          %40 = arith.mulf %11, %39 : f32
          %41 = arith.mulf %arg6, %33 : f32
          %42 = arith.addf %40, %41 : f32
          memref.store %42, %arg4[%1] : memref<?xf32>
          scf.yield %33, %29 : f32, f32
        } else {
          %35 = memref.load %arg3[%1] : memref<?xf32>
          %36 = memref.load %arg4[%1] : memref<?xf32>
          scf.yield %36, %35 : f32, f32
        }
        memref.store %34#1, %alloca[%c0] : memref<2xf32, 5>
        memref.store %34#0, %alloca[%c1] : memref<2xf32, 5>
      }
      scf.yield
    }
    %12 = gpu.block_dim  x
    %13 = arith.subi %12, %c1 : index
    %14 = arith.muli %arg9, %arg11 : i32
    %15 = arith.muli %14, %arg12 : i32
    %16 = arith.index_cast %15 : i32 to index
    %17 = arith.muli %arg11, %arg12 : i32
    %18 = gpu.block_id  x
    %19 = arith.index_cast %18 : index to i32
    %20 = arith.cmpi slt, %c0, %c32 : index
    scf.if %20 {
      %21 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %22 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %23 = arith.addf %22, %arg7 : f32
      %24 = math.rsqrt %23 : f32
      %25 = memref.load %arg1[%18] : memref<?xf32>
      %26 = memref.load %arg2[%18] : memref<?xf32>
      scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
        %27 = arith.subi %16, %arg13 : index
        %28 = arith.addi %13, %27 : index
        %29 = arith.divui %28, %12 : index
        scf.for %arg14 = %c0 to %29 step %c1 {
          %30 = arith.muli %arg14, %12 : index
          %31 = arith.addi %arg13, %30 : index
          %32 = arith.index_cast %31 : index to i32
          %33 = arith.divsi %32, %17 : i32
          %34 = arith.remsi %32, %17 : i32
          %35 = arith.divsi %34, %arg12 : i32
          %36 = arith.remsi %34, %arg12 : i32
          %37 = arith.muli %33, %arg10 : i32
          %38 = arith.addi %37, %19 : i32
          %39 = arith.muli %38, %arg11 : i32
          %40 = arith.addi %39, %35 : i32
          %41 = arith.muli %40, %arg12 : i32
          %42 = arith.addi %41, %36 : i32
          %43 = arith.index_cast %42 : i32 to index
          %44 = memref.load %arg0[%43] : memref<?xf32>
          %45 = arith.subf %44, %21 : f32
          %46 = arith.mulf %45, %24 : f32
          %47 = arith.mulf %46, %25 : f32
          %48 = arith.addf %47, %26 : f32
          memref.store %48, %arg8[%43] : memref<?xf32>
        }
        scf.yield
      }
    }
    return
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
[ict-debug] driver.cc: Before my pass process: end

[ict-debug] driver.cc: vectorizeSize = 1

[ict-debug] WrapAndReplaceBarrierPass::runOnOperation(): Function name: __nvvm_shfl_sync_down_f32. func.getBlocks().size() == 0! this function is empty, skip it.

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<2xf32, 5>
      %alloca_1 = memref.alloca() : memref<32x1xf32>
      %alloca_2 = memref.alloca() : memref<32x1xf32>
      %0 = llvm.mlir.undef : f32
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %3 = gpu.block_dim  x
      %4 = arith.muli %arg9, %arg11 : i32
      %5 = arith.muli %4, %arg12 : i32
      %6 = arith.index_cast %5 : i32 to index
      %7 = arith.muli %arg11, %arg12 : i32
      %8 = arith.subi %3, %c1 : index
      %9 = arith.sitofp %5 : i32 to f32
      %10 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %11 = arith.subf %cst, %arg6 : f32
      scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
        %18 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
        memref.store %0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %19 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
        memref.store %0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
        %20 = arith.index_cast %arg13 : index to i32
        %21 = arith.cmpi eq, %20, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %22 = arith.subi %6, %arg13 : index
        %23 = arith.addi %8, %22 : index
        %24 = arith.divui %23, %3 : index
        scf.for %arg14 = %c0 to %24 step %c1 {
          %25 = arith.muli %arg14, %3 : index
          %26 = arith.addi %arg13, %25 : index
          %27 = arith.index_cast %26 : index to i32
          %28 = arith.divsi %27, %7 : i32
          %29 = arith.remsi %27, %7 : i32
          %30 = arith.divsi %29, %arg12 : i32
          %31 = arith.remsi %29, %arg12 : i32
          %32 = arith.muli %28, %arg10 : i32
          %33 = arith.addi %32, %2 : i32
          %34 = arith.muli %33, %arg11 : i32
          %35 = arith.addi %34, %30 : i32
          %36 = arith.muli %35, %arg12 : i32
          %37 = arith.addi %36, %31 : i32
          %38 = arith.index_cast %37 : i32 to index
          %39 = memref.load %arg0[%38] : memref<?xf32>
          %40 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
          %41 = arith.addf %40, %39 : f32
          memref.store %41, %alloca_2[%arg13, %c0] : memref<32x1xf32>
          %42 = arith.mulf %39, %39 : f32
          %43 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
          %44 = arith.addf %43, %42 : f32
          memref.store %44, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        }
        func.call @_Z14blockReduceSumRfS_(%19, %18) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %21 {
          %25:2 = scf.if %10 -> (f32, f32) {
            %26 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
            %27 = arith.divf %26, %9 : f32
            %28 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
            %29 = arith.divf %28, %9 : f32
            %30 = arith.mulf %27, %27 : f32
            %31 = arith.subf %29, %30 : f32
            %32 = memref.load %arg3[%1] : memref<?xf32>
            %33 = arith.mulf %11, %32 : f32
            %34 = arith.mulf %arg6, %27 : f32
            %35 = arith.addf %33, %34 : f32
            memref.store %35, %arg3[%1] : memref<?xf32>
            %36 = memref.load %arg4[%1] : memref<?xf32>
            %37 = arith.mulf %11, %36 : f32
            %38 = arith.mulf %arg6, %31 : f32
            %39 = arith.addf %37, %38 : f32
            memref.store %39, %arg4[%1] : memref<?xf32>
            scf.yield %31, %27 : f32, f32
          } else {
            %26 = memref.load %arg3[%1] : memref<?xf32>
            %27 = memref.load %arg4[%1] : memref<?xf32>
            scf.yield %27, %26 : f32, f32
          }
          memref.store %25#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %25#0, %alloca[%c1] : memref<2xf32, 5>
        }
        scf.yield
      }
      %12 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %13 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %14 = arith.addf %13, %arg7 : f32
      %15 = math.rsqrt %14 : f32
      %16 = memref.load %arg1[%1] : memref<?xf32>
      %17 = memref.load %arg2[%1] : memref<?xf32>
      scf.parallel (%arg13) = (%c0) to (%c32) step (%c1) {
        %18 = arith.subi %6, %arg13 : index
        %19 = arith.addi %8, %18 : index
        %20 = arith.divui %19, %3 : index
        scf.for %arg14 = %c0 to %20 step %c1 {
          %21 = arith.muli %arg14, %3 : index
          %22 = arith.addi %arg13, %21 : index
          %23 = arith.index_cast %22 : index to i32
          %24 = arith.divsi %23, %7 : i32
          %25 = arith.remsi %23, %7 : i32
          %26 = arith.divsi %25, %arg12 : i32
          %27 = arith.remsi %25, %arg12 : i32
          %28 = arith.muli %24, %arg10 : i32
          %29 = arith.addi %28, %2 : i32
          %30 = arith.muli %29, %arg11 : i32
          %31 = arith.addi %30, %26 : i32
          %32 = arith.muli %31, %arg12 : i32
          %33 = arith.addi %32, %27 : i32
          %34 = arith.index_cast %33 : i32 to index
          %35 = memref.load %arg0[%34] : memref<?xf32>
          %36 = arith.subf %35, %12 : f32
          %37 = arith.mulf %36, %15 : f32
          %38 = arith.mulf %37, %16 : f32
          %39 = arith.addf %38, %17 : f32
          memref.store %39, %arg8[%34] : memref<?xf32>
        }
        scf.yield
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute: end

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_1 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_1 {
        %5 = arith.index_cast %arg2 : index to i32
        %6 = arith.remui %5, %c32_i32 : i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        %8 = arith.divui %5, %c32_i32 : i32
        %9 = arith.index_cast %8 : i32 to index
        %10:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %13 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%13) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %13 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %14 = arith.addf %arg3, %13 : f32
          %15 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %15, %14 : i32, f32
        }
        %11 = memref.load %arg1[%c0] : memref<?xf32>
        %12:2 = scf.while (%arg3 = %c16_i32, %arg4 = %11) : (i32, f32) -> (f32, i32) {
          %13 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%13) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %13 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %14 = arith.addf %arg3, %13 : f32
          %15 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %15, %14 : i32, f32
        }
        scf.if %7 {
          memref.store %10#0, %alloca_0[%9] : memref<32xf32, 5>
          memref.store %12#0, %alloca[%9] : memref<32xf32, 5>
        }
      }
      %c32_2 = arith.constant 32 : index
      %1 = arith.index_cast %c32_2 : index to i32
      %2 = arith.addi %1, %c31_i32 : i32
      %3 = arith.divui %2, %c32_i32 : i32
      %4 = arith.index_cast %3 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %5 = arith.index_cast %arg2 : index to i32
        %6 = arith.cmpi eq, %5, %c0_i32 : i32
        scf.if %6 {
          %7:2 = scf.for %arg3 = %c0 to %4 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %8 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %9 = arith.addf %arg5, %8 : f32
            %10 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %11 = arith.addf %arg4, %10 : f32
            scf.yield %11, %9 : f32, f32
          }
          memref.store %7#1, %arg0[%c0] : memref<?xf32>
          memref.store %7#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<2xf32, 5>
      %alloca_1 = memref.alloca() : memref<32x1xf32>
      %alloca_2 = memref.alloca() : memref<32x1xf32>
      %0 = llvm.mlir.undef : f32
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %c32_3 = arith.constant 32 : index
      %3 = arith.muli %arg9, %arg11 : i32
      %4 = arith.muli %3, %arg12 : i32
      %5 = arith.index_cast %4 : i32 to index
      %6 = arith.muli %arg11, %arg12 : i32
      %7 = arith.subi %c32_3, %c1 : index
      %8 = arith.sitofp %4 : i32 to f32
      %9 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %10 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %17 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
        memref.store %0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %18 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32>, index) -> memref<?xf32>
        memref.store %0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
        %19 = arith.index_cast %arg13 : index to i32
        %20 = arith.cmpi eq, %19, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        %21 = arith.subi %5, %arg13 : index
        %22 = arith.addi %7, %21 : index
        %23 = arith.divui %22, %c32_3 : index
        scf.for %arg14 = %c0 to %23 step %c1 {
          %24 = arith.muli %arg14, %c32_3 : index
          %25 = arith.addi %arg13, %24 : index
          %26 = arith.index_cast %25 : index to i32
          %27 = arith.divsi %26, %6 : i32
          %28 = arith.remsi %26, %6 : i32
          %29 = arith.divsi %28, %arg12 : i32
          %30 = arith.remsi %28, %arg12 : i32
          %31 = arith.muli %27, %arg10 : i32
          %32 = arith.addi %31, %2 : i32
          %33 = arith.muli %32, %arg11 : i32
          %34 = arith.addi %33, %29 : i32
          %35 = arith.muli %34, %arg12 : i32
          %36 = arith.addi %35, %30 : i32
          %37 = arith.index_cast %36 : i32 to index
          %38 = memref.load %arg0[%37] : memref<?xf32>
          %39 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
          %40 = arith.addf %39, %38 : f32
          memref.store %40, %alloca_2[%arg13, %c0] : memref<32x1xf32>
          %41 = arith.mulf %38, %38 : f32
          %42 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
          %43 = arith.addf %42, %41 : f32
          memref.store %43, %alloca_1[%arg13, %c0] : memref<32x1xf32>
        }
        func.call @_Z14blockReduceSumRfS_(%18, %17) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %20 {
          %24:2 = scf.if %9 -> (f32, f32) {
            %25 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32>
            %26 = arith.divf %25, %8 : f32
            %27 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32>
            %28 = arith.divf %27, %8 : f32
            %29 = arith.mulf %26, %26 : f32
            %30 = arith.subf %28, %29 : f32
            %31 = memref.load %arg3[%1] : memref<?xf32>
            %32 = arith.mulf %10, %31 : f32
            %33 = arith.mulf %arg6, %26 : f32
            %34 = arith.addf %32, %33 : f32
            memref.store %34, %arg3[%1] : memref<?xf32>
            %35 = memref.load %arg4[%1] : memref<?xf32>
            %36 = arith.mulf %10, %35 : f32
            %37 = arith.mulf %arg6, %30 : f32
            %38 = arith.addf %36, %37 : f32
            memref.store %38, %arg4[%1] : memref<?xf32>
            scf.yield %30, %26 : f32, f32
          } else {
            %25 = memref.load %arg3[%1] : memref<?xf32>
            %26 = memref.load %arg4[%1] : memref<?xf32>
            scf.yield %26, %25 : f32, f32
          }
          memref.store %24#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %24#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %11 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %12 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %13 = arith.addf %12, %arg7 : f32
      %14 = math.rsqrt %13 : f32
      %15 = memref.load %arg1[%1] : memref<?xf32>
      %16 = memref.load %arg2[%1] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %17 = arith.subi %5, %arg13 : index
        %18 = arith.addi %7, %17 : index
        %19 = arith.divui %18, %c32_3 : index
        scf.for %arg14 = %c0 to %19 step %c1 {
          %20 = arith.muli %arg14, %c32_3 : index
          %21 = arith.addi %arg13, %20 : index
          %22 = arith.index_cast %21 : index to i32
          %23 = arith.divsi %22, %6 : i32
          %24 = arith.remsi %22, %6 : i32
          %25 = arith.divsi %24, %arg12 : i32
          %26 = arith.remsi %24, %arg12 : i32
          %27 = arith.muli %23, %arg10 : i32
          %28 = arith.addi %27, %2 : i32
          %29 = arith.muli %28, %arg11 : i32
          %30 = arith.addi %29, %25 : i32
          %31 = arith.muli %30, %arg12 : i32
          %32 = arith.addi %31, %26 : i32
          %33 = arith.index_cast %32 : i32 to index
          %34 = memref.load %arg0[%33] : memref<?xf32>
          %35 = arith.subf %34, %11 : f32
          %36 = arith.mulf %35, %14 : f32
          %37 = arith.mulf %36, %15 : f32
          %38 = arith.addf %37, %16 : f32
          memref.store %38, %arg8[%33] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize: end

[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca = memref.alloca() : memref<32xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca = memref.alloca() : memref<32xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %1 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_1 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_1 {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %1) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
      }
      %c32_2 = arith.constant 32 : index
      %2 = arith.index_cast %c32_2 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<2xf32, 5>
      %alloca_1 = memref.alloca() : memref<32x1xf32, 5>
      %alloca_2 = memref.alloca() : memref<32x1xf32, 5>
      %0 = llvm.mlir.undef : f32
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %c32_3 = arith.constant 32 : index
      %3 = arith.muli %arg9, %arg11 : i32
      %4 = arith.muli %3, %arg12 : i32
      %5 = arith.index_cast %4 : i32 to index
      %6 = arith.muli %arg11, %arg12 : i32
      %7 = arith.subi %c32_3, %c1 : index
      %8 = arith.sitofp %4 : i32 to f32
      %9 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %10 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %17 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %18 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        %19 = arith.index_cast %arg13 : index to i32
        %20 = arith.cmpi eq, %19, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %21 = arith.subi %5, %arg13 : index
        %22 = arith.addi %7, %21 : index
        %23 = arith.divui %22, %c32_3 : index
        scf.for %arg14 = %c0 to %23 step %c1 {
          %24 = arith.muli %arg14, %c32_3 : index
          %25 = arith.addi %arg13, %24 : index
          %26 = arith.index_cast %25 : index to i32
          %27 = arith.divsi %26, %6 : i32
          %28 = arith.remsi %26, %6 : i32
          %29 = arith.divsi %28, %arg12 : i32
          %30 = arith.remsi %28, %arg12 : i32
          %31 = arith.muli %27, %arg10 : i32
          %32 = arith.addi %31, %2 : i32
          %33 = arith.muli %32, %arg11 : i32
          %34 = arith.addi %33, %29 : i32
          %35 = arith.muli %34, %arg12 : i32
          %36 = arith.addi %35, %30 : i32
          %37 = arith.index_cast %36 : i32 to index
          %38 = memref.load %arg0[%37] : memref<?xf32>
          %39 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %40 = arith.addf %39, %38 : f32
          memref.store %40, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %41 = arith.mulf %38, %38 : f32
          %42 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
          %43 = arith.addf %42, %41 : f32
          memref.store %43, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        }
        func.call @_Z14blockReduceSumRfS_(%18, %17) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %20 {
          %24:2 = scf.if %9 -> (f32, f32) {
            %25 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
            %26 = arith.divf %25, %8 : f32
            %27 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
            %28 = arith.divf %27, %8 : f32
            %29 = arith.mulf %26, %26 : f32
            %30 = arith.subf %28, %29 : f32
            %31 = memref.load %arg3[%1] : memref<?xf32>
            %32 = arith.mulf %10, %31 : f32
            %33 = arith.mulf %arg6, %26 : f32
            %34 = arith.addf %32, %33 : f32
            memref.store %34, %arg3[%1] : memref<?xf32>
            %35 = memref.load %arg4[%1] : memref<?xf32>
            %36 = arith.mulf %10, %35 : f32
            %37 = arith.mulf %arg6, %30 : f32
            %38 = arith.addf %36, %37 : f32
            memref.store %38, %arg4[%1] : memref<?xf32>
            scf.yield %30, %26 : f32, f32
          } else {
            %25 = memref.load %arg3[%1] : memref<?xf32>
            %26 = memref.load %arg4[%1] : memref<?xf32>
            scf.yield %26, %25 : f32, f32
          }
          memref.store %24#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %24#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %11 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %12 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %13 = arith.addf %12, %arg7 : f32
      %14 = math.rsqrt %13 : f32
      %15 = memref.load %arg1[%1] : memref<?xf32>
      %16 = memref.load %arg2[%1] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %17 = arith.subi %5, %arg13 : index
        %18 = arith.addi %7, %17 : index
        %19 = arith.divui %18, %c32_3 : index
        scf.for %arg14 = %c0 to %19 step %c1 {
          %20 = arith.muli %arg14, %c32_3 : index
          %21 = arith.addi %arg13, %20 : index
          %22 = arith.index_cast %21 : index to i32
          %23 = arith.divsi %22, %6 : i32
          %24 = arith.remsi %22, %6 : i32
          %25 = arith.divsi %24, %arg12 : i32
          %26 = arith.remsi %24, %arg12 : i32
          %27 = arith.muli %23, %arg10 : i32
          %28 = arith.addi %27, %2 : i32
          %29 = arith.muli %28, %arg11 : i32
          %30 = arith.addi %29, %25 : i32
          %31 = arith.muli %30, %arg12 : i32
          %32 = arith.addi %31, %26 : i32
          %33 = arith.index_cast %32 : i32 to index
          %34 = memref.load %arg0[%33] : memref<?xf32>
          %35 = arith.subf %34, %11 : f32
          %36 = arith.mulf %35, %14 : f32
          %37 = arith.mulf %36, %15 : f32
          %38 = arith.addf %37, %16 : f32
          memref.store %38, %arg8[%33] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca_0 = memref.alloca() : memref<32xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca_0 = memref.alloca() : memref<32xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<32xf32, 5>
      %1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %2 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_1 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_1 {
        %7 = arith.index_cast %arg2 : index to i32
        %8 = arith.remui %7, %c32_i32 : i32
        %9 = arith.cmpi eq, %8, %c0_i32 : i32
        %10 = arith.divui %7, %c32_i32 : i32
        %11 = arith.index_cast %10 : i32 to index
        %12:2 = scf.while (%arg3 = %c16_i32, %arg4 = %2) : (i32, f32) -> (f32, i32) {
          %15 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%15) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %15 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %16 = arith.addf %arg3, %15 : f32
          %17 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %17, %16 : i32, f32
        }
        %13 = memref.load %arg1[%c0] : memref<?xf32>
        %14:2 = scf.while (%arg3 = %c16_i32, %arg4 = %13) : (i32, f32) -> (f32, i32) {
          %15 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%15) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %15 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %16 = arith.addf %arg3, %15 : f32
          %17 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %17, %16 : i32, f32
        }
        scf.if %9 {
          memref.store %12#0, %alloca_0[%11] : memref<32xf32, 5>
          memref.store %14#0, %alloca[%11] : memref<32xf32, 5>
        }
      }
      %c32_2 = arith.constant 32 : index
      %3 = arith.index_cast %c32_2 : index to i32
      %4 = arith.addi %3, %c31_i32 : i32
      %5 = arith.divui %4, %c32_i32 : i32
      %6 = arith.index_cast %5 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %7 = arith.index_cast %arg2 : index to i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        scf.if %8 {
          %9:2 = scf.for %arg3 = %c0 to %6 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %10 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %11 = arith.addf %arg5, %10 : f32
            %12 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %13 = arith.addf %arg4, %12 : f32
            scf.yield %13, %11 : f32, f32
          }
          memref.store %9#1, %arg0[%c0] : memref<?xf32>
          memref.store %9#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<2xf32, 5>
      %alloca_1 = memref.alloca() : memref<32x1xf32, 5>
      %alloca_2 = memref.alloca() : memref<32x1xf32, 5>
      %0 = llvm.mlir.undef : f32
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %c32_3 = arith.constant 32 : index
      %3 = arith.muli %arg9, %arg11 : i32
      %4 = arith.muli %3, %arg12 : i32
      %5 = arith.index_cast %4 : i32 to index
      %6 = arith.muli %arg11, %arg12 : i32
      %7 = arith.subi %c32_3, %c1 : index
      %8 = arith.sitofp %4 : i32 to f32
      %9 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %10 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %17 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %18 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        %19 = arith.index_cast %arg13 : index to i32
        %20 = arith.cmpi eq, %19, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %21 = arith.subi %5, %arg13 : index
        %22 = arith.addi %7, %21 : index
        %23 = arith.divui %22, %c32_3 : index
        scf.for %arg14 = %c0 to %23 step %c1 {
          %24 = arith.muli %arg14, %c32_3 : index
          %25 = arith.addi %arg13, %24 : index
          %26 = arith.index_cast %25 : index to i32
          %27 = arith.divsi %26, %6 : i32
          %28 = arith.remsi %26, %6 : i32
          %29 = arith.divsi %28, %arg12 : i32
          %30 = arith.remsi %28, %arg12 : i32
          %31 = arith.muli %27, %arg10 : i32
          %32 = arith.addi %31, %2 : i32
          %33 = arith.muli %32, %arg11 : i32
          %34 = arith.addi %33, %29 : i32
          %35 = arith.muli %34, %arg12 : i32
          %36 = arith.addi %35, %30 : i32
          %37 = arith.index_cast %36 : i32 to index
          %38 = memref.load %arg0[%37] : memref<?xf32>
          %39 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %40 = arith.addf %39, %38 : f32
          memref.store %40, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %41 = arith.mulf %38, %38 : f32
          %42 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
          %43 = arith.addf %42, %41 : f32
          memref.store %43, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        }
        func.call @_Z14blockReduceSumRfS_(%18, %17) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %20 {
          %24:2 = scf.if %9 -> (f32, f32) {
            %25 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
            %26 = arith.divf %25, %8 : f32
            %27 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
            %28 = arith.divf %27, %8 : f32
            %29 = arith.mulf %26, %26 : f32
            %30 = arith.subf %28, %29 : f32
            %31 = memref.load %arg3[%1] : memref<?xf32>
            %32 = arith.mulf %10, %31 : f32
            %33 = arith.mulf %arg6, %26 : f32
            %34 = arith.addf %32, %33 : f32
            memref.store %34, %arg3[%1] : memref<?xf32>
            %35 = memref.load %arg4[%1] : memref<?xf32>
            %36 = arith.mulf %10, %35 : f32
            %37 = arith.mulf %arg6, %30 : f32
            %38 = arith.addf %36, %37 : f32
            memref.store %38, %arg4[%1] : memref<?xf32>
            scf.yield %30, %26 : f32, f32
          } else {
            %25 = memref.load %arg3[%1] : memref<?xf32>
            %26 = memref.load %arg4[%1] : memref<?xf32>
            scf.yield %26, %25 : f32, f32
          }
          memref.store %24#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %24#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %11 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %12 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %13 = arith.addf %12, %arg7 : f32
      %14 = math.rsqrt %13 : f32
      %15 = memref.load %arg1[%1] : memref<?xf32>
      %16 = memref.load %arg2[%1] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %17 = arith.subi %5, %arg13 : index
        %18 = arith.addi %7, %17 : index
        %19 = arith.divui %18, %c32_3 : index
        scf.for %arg14 = %c0 to %19 step %c1 {
          %20 = arith.muli %arg14, %c32_3 : index
          %21 = arith.addi %arg13, %20 : index
          %22 = arith.index_cast %21 : index to i32
          %23 = arith.divsi %22, %6 : i32
          %24 = arith.remsi %22, %6 : i32
          %25 = arith.divsi %24, %arg12 : i32
          %26 = arith.remsi %24, %arg12 : i32
          %27 = arith.muli %23, %arg10 : i32
          %28 = arith.addi %27, %2 : i32
          %29 = arith.muli %28, %arg11 : i32
          %30 = arith.addi %29, %25 : i32
          %31 = arith.muli %30, %arg12 : i32
          %32 = arith.addi %31, %26 : i32
          %33 = arith.index_cast %32 : i32 to index
          %34 = memref.load %arg0[%33] : memref<?xf32>
          %35 = arith.subf %34, %11 : f32
          %36 = arith.mulf %35, %14 : f32
          %37 = arith.mulf %36, %15 : f32
          %38 = arith.addf %37, %16 : f32
          memref.store %38, %arg8[%33] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] CastLikeOpToNPULowering: process op: 

%8 = arith.index_cast %arg2 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%13 = arith.index_cast %12 : i32 to index
[ict-debug] CastLikeOpToNPULowering: process op: 

%6 = arith.index_cast %5 : i32 to index
[ict-debug] CastLikeOpToNPULowering: process op: 

%9 = arith.index_cast %arg2 : index to i32
[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca = memref.alloca() : memref<2xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca = memref.alloca() : memref<2xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_0 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_0 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.remui %9, %c32_i32 : i32
        %11 = arith.cmpi eq, %10, %c0_i32 : i32
        %12 = arith.divui %9, %c32_i32 : i32
        %13 = emitc.cast %12 : i32 to index
        %14:2 = scf.while (%arg3 = %c16_i32, %arg4 = %4) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        %15 = memref.load %arg1[%c0] : memref<?xf32>
        %16:2 = scf.while (%arg3 = %c16_i32, %arg4 = %15) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        scf.if %11 {
          memref.store %14#0, %3[%13] : memref<32xf32, 5>
          memref.store %16#0, %1[%13] : memref<32xf32, 5>
        }
      }
      %c32_1 = arith.constant 32 : index
      %c32_i32_2 = arith.constant 32 : i32
      %5 = arith.addi %c32_i32_2, %c31_i32 : i32
      %6 = arith.divui %5, %c32_i32 : i32
      %7 = emitc.cast %6 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.cmpi eq, %9, %c0_i32 : i32
        scf.if %10 {
          %11:2 = scf.for %arg3 = %c0 to %7 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %12 = memref.load %3[%arg3] : memref<32xf32, 5>
            %13 = emitc.add %arg5, %12 : (f32, f32) -> f32
            %14 = memref.load %1[%arg3] : memref<32xf32, 5>
            %15 = emitc.add %arg4, %14 : (f32, f32) -> f32
            scf.yield %15, %13 : f32, f32
          }
          memref.store %11#1, %arg0[%c0] : memref<?xf32>
          memref.store %11#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<2xf32, 5>
      %alloca_1 = memref.alloca() : memref<32x1xf32, 5>
      %alloca_2 = memref.alloca() : memref<32x1xf32, 5>
      %1 = llvm.mlir.undef : f32
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %c32_3 = arith.constant 32 : index
      %4 = arith.muli %arg9, %arg11 : i32
      %5 = arith.muli %4, %arg12 : i32
      %6 = arith.index_cast %5 : i32 to index
      %7 = arith.muli %arg11, %arg12 : i32
      %8 = arith.subi %c32_3, %c1 : index
      %9 = arith.sitofp %5 : i32 to f32
      %10 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %11 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %18 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %1, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %19 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %1, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        %20 = arith.index_cast %arg13 : index to i32
        %21 = arith.cmpi eq, %20, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %22 = arith.subi %6, %arg13 : index
        %23 = arith.addi %8, %22 : index
        %24 = arith.divui %23, %c32_3 : index
        scf.for %arg14 = %c0 to %24 step %c1 {
          %25 = arith.muli %arg14, %c32_3 : index
          %26 = arith.addi %arg13, %25 : index
          %27 = arith.index_cast %26 : index to i32
          %28 = arith.divsi %27, %7 : i32
          %29 = arith.remsi %27, %7 : i32
          %30 = arith.divsi %29, %arg12 : i32
          %31 = arith.remsi %29, %arg12 : i32
          %32 = arith.muli %28, %arg10 : i32
          %33 = arith.addi %32, %3 : i32
          %34 = arith.muli %33, %arg11 : i32
          %35 = arith.addi %34, %30 : i32
          %36 = arith.muli %35, %arg12 : i32
          %37 = arith.addi %36, %31 : i32
          %38 = arith.index_cast %37 : i32 to index
          %39 = memref.load %arg0[%38] : memref<?xf32>
          %40 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %41 = arith.addf %40, %39 : f32
          memref.store %41, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %42 = arith.mulf %39, %39 : f32
          %43 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
          %44 = arith.addf %43, %42 : f32
          memref.store %44, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        }
        func.call @_Z14blockReduceSumRfS_(%19, %18) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %21 {
          %25:2 = scf.if %10 -> (f32, f32) {
            %26 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
            %27 = arith.divf %26, %9 : f32
            %28 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
            %29 = arith.divf %28, %9 : f32
            %30 = arith.mulf %27, %27 : f32
            %31 = arith.subf %29, %30 : f32
            %32 = memref.load %arg3[%2] : memref<?xf32>
            %33 = arith.mulf %11, %32 : f32
            %34 = arith.mulf %arg6, %27 : f32
            %35 = arith.addf %33, %34 : f32
            memref.store %35, %arg3[%2] : memref<?xf32>
            %36 = memref.load %arg4[%2] : memref<?xf32>
            %37 = arith.mulf %11, %36 : f32
            %38 = arith.mulf %arg6, %31 : f32
            %39 = arith.addf %37, %38 : f32
            memref.store %39, %arg4[%2] : memref<?xf32>
            scf.yield %31, %27 : f32, f32
          } else {
            %26 = memref.load %arg3[%2] : memref<?xf32>
            %27 = memref.load %arg4[%2] : memref<?xf32>
            scf.yield %27, %26 : f32, f32
          }
          memref.store %25#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %25#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %12 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %13 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %14 = arith.addf %13, %arg7 : f32
      %15 = math.rsqrt %14 : f32
      %16 = memref.load %arg1[%2] : memref<?xf32>
      %17 = memref.load %arg2[%2] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %18 = arith.subi %6, %arg13 : index
        %19 = arith.addi %8, %18 : index
        %20 = arith.divui %19, %c32_3 : index
        scf.for %arg14 = %c0 to %20 step %c1 {
          %21 = arith.muli %arg14, %c32_3 : index
          %22 = arith.addi %arg13, %21 : index
          %23 = arith.index_cast %22 : index to i32
          %24 = arith.divsi %23, %7 : i32
          %25 = arith.remsi %23, %7 : i32
          %26 = arith.divsi %25, %arg12 : i32
          %27 = arith.remsi %25, %arg12 : i32
          %28 = arith.muli %24, %arg10 : i32
          %29 = arith.addi %28, %3 : i32
          %30 = arith.muli %29, %arg11 : i32
          %31 = arith.addi %30, %26 : i32
          %32 = arith.muli %31, %arg12 : i32
          %33 = arith.addi %32, %27 : i32
          %34 = arith.index_cast %33 : i32 to index
          %35 = memref.load %arg0[%34] : memref<?xf32>
          %36 = arith.subf %35, %12 : f32
          %37 = arith.mulf %36, %15 : f32
          %38 = arith.mulf %37, %16 : f32
          %39 = arith.addf %38, %17 : f32
          memref.store %39, %arg8[%34] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca_1 = memref.alloca() : memref<32x1xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca_1 = memref.alloca() : memref<32x1xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_0 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_0 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.remui %9, %c32_i32 : i32
        %11 = arith.cmpi eq, %10, %c0_i32 : i32
        %12 = arith.divui %9, %c32_i32 : i32
        %13 = emitc.cast %12 : i32 to index
        %14:2 = scf.while (%arg3 = %c16_i32, %arg4 = %4) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        %15 = memref.load %arg1[%c0] : memref<?xf32>
        %16:2 = scf.while (%arg3 = %c16_i32, %arg4 = %15) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        scf.if %11 {
          memref.store %14#0, %3[%13] : memref<32xf32, 5>
          memref.store %16#0, %1[%13] : memref<32xf32, 5>
        }
      }
      %c32_1 = arith.constant 32 : index
      %c32_i32_2 = arith.constant 32 : i32
      %5 = arith.addi %c32_i32_2, %c31_i32 : i32
      %6 = arith.divui %5, %c32_i32 : i32
      %7 = emitc.cast %6 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.cmpi eq, %9, %c0_i32 : i32
        scf.if %10 {
          %11:2 = scf.for %arg3 = %c0 to %7 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %12 = memref.load %3[%arg3] : memref<32xf32, 5>
            %13 = emitc.add %arg5, %12 : (f32, f32) -> f32
            %14 = memref.load %1[%arg3] : memref<32xf32, 5>
            %15 = emitc.add %arg4, %14 : (f32, f32) -> f32
            scf.yield %15, %13 : f32, f32
          }
          memref.store %11#1, %arg0[%c0] : memref<?xf32>
          memref.store %11#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<2xf32, 5>
      %1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca_1 = memref.alloca() : memref<32x1xf32, 5>
      %alloca_2 = memref.alloca() : memref<32x1xf32, 5>
      %2 = llvm.mlir.undef : f32
      %3 = gpu.block_id  x
      %4 = arith.index_cast %3 : index to i32
      %c32_3 = arith.constant 32 : index
      %5 = arith.muli %arg9, %arg11 : i32
      %6 = arith.muli %5, %arg12 : i32
      %7 = arith.index_cast %6 : i32 to index
      %8 = arith.muli %arg11, %arg12 : i32
      %9 = arith.subi %c32_3, %c1 : index
      %10 = arith.sitofp %6 : i32 to f32
      %11 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %12 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %19 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %2, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %20 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %2, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        %21 = arith.index_cast %arg13 : index to i32
        %22 = arith.cmpi eq, %21, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %23 = arith.subi %7, %arg13 : index
        %24 = arith.addi %9, %23 : index
        %25 = arith.divui %24, %c32_3 : index
        scf.for %arg14 = %c0 to %25 step %c1 {
          %26 = arith.muli %arg14, %c32_3 : index
          %27 = arith.addi %arg13, %26 : index
          %28 = arith.index_cast %27 : index to i32
          %29 = arith.divsi %28, %8 : i32
          %30 = arith.remsi %28, %8 : i32
          %31 = arith.divsi %30, %arg12 : i32
          %32 = arith.remsi %30, %arg12 : i32
          %33 = arith.muli %29, %arg10 : i32
          %34 = arith.addi %33, %4 : i32
          %35 = arith.muli %34, %arg11 : i32
          %36 = arith.addi %35, %31 : i32
          %37 = arith.muli %36, %arg12 : i32
          %38 = arith.addi %37, %32 : i32
          %39 = arith.index_cast %38 : i32 to index
          %40 = memref.load %arg0[%39] : memref<?xf32>
          %41 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %42 = arith.addf %41, %40 : f32
          memref.store %42, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %43 = arith.mulf %40, %40 : f32
          %44 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
          %45 = arith.addf %44, %43 : f32
          memref.store %45, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        }
        func.call @_Z14blockReduceSumRfS_(%20, %19) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %22 {
          %26:2 = scf.if %11 -> (f32, f32) {
            %27 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
            %28 = arith.divf %27, %10 : f32
            %29 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
            %30 = arith.divf %29, %10 : f32
            %31 = arith.mulf %28, %28 : f32
            %32 = arith.subf %30, %31 : f32
            %33 = memref.load %arg3[%3] : memref<?xf32>
            %34 = arith.mulf %12, %33 : f32
            %35 = arith.mulf %arg6, %28 : f32
            %36 = arith.addf %34, %35 : f32
            memref.store %36, %arg3[%3] : memref<?xf32>
            %37 = memref.load %arg4[%3] : memref<?xf32>
            %38 = arith.mulf %12, %37 : f32
            %39 = arith.mulf %arg6, %32 : f32
            %40 = arith.addf %38, %39 : f32
            memref.store %40, %arg4[%3] : memref<?xf32>
            scf.yield %32, %28 : f32, f32
          } else {
            %27 = memref.load %arg3[%3] : memref<?xf32>
            %28 = memref.load %arg4[%3] : memref<?xf32>
            scf.yield %28, %27 : f32, f32
          }
          memref.store %26#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %26#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %13 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %14 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %15 = arith.addf %14, %arg7 : f32
      %16 = math.rsqrt %15 : f32
      %17 = memref.load %arg1[%3] : memref<?xf32>
      %18 = memref.load %arg2[%3] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %19 = arith.subi %7, %arg13 : index
        %20 = arith.addi %9, %19 : index
        %21 = arith.divui %20, %c32_3 : index
        scf.for %arg14 = %c0 to %21 step %c1 {
          %22 = arith.muli %arg14, %c32_3 : index
          %23 = arith.addi %arg13, %22 : index
          %24 = arith.index_cast %23 : index to i32
          %25 = arith.divsi %24, %8 : i32
          %26 = arith.remsi %24, %8 : i32
          %27 = arith.divsi %26, %arg12 : i32
          %28 = arith.remsi %26, %arg12 : i32
          %29 = arith.muli %25, %arg10 : i32
          %30 = arith.addi %29, %4 : i32
          %31 = arith.muli %30, %arg11 : i32
          %32 = arith.addi %31, %27 : i32
          %33 = arith.muli %32, %arg12 : i32
          %34 = arith.addi %33, %28 : i32
          %35 = arith.index_cast %34 : i32 to index
          %36 = memref.load %arg0[%35] : memref<?xf32>
          %37 = arith.subf %36, %13 : f32
          %38 = arith.mulf %37, %16 : f32
          %39 = arith.mulf %38, %17 : f32
          %40 = arith.addf %39, %18 : f32
          memref.store %40, %arg8[%35] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca_2 = memref.alloca() : memref<32x1xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca_2 = memref.alloca() : memref<32x1xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z14blockReduceSumRfS__0 {
    gpu.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<32xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %3 = builtin.unrealized_conversion_cast %2 : !llvm.ptr<6> to memref<32xf32, 5>
      %4 = memref.load %arg0[%c0] : memref<?xf32>
      %c1_0 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_0 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.remui %9, %c32_i32 : i32
        %11 = arith.cmpi eq, %10, %c0_i32 : i32
        %12 = arith.divui %9, %c32_i32 : i32
        %13 = emitc.cast %12 : i32 to index
        %14:2 = scf.while (%arg3 = %c16_i32, %arg4 = %4) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        %15 = memref.load %arg1[%c0] : memref<?xf32>
        %16:2 = scf.while (%arg3 = %c16_i32, %arg4 = %15) : (i32, f32) -> (f32, i32) {
          %17 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%17) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %17 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %18 = emitc.add %arg3, %17 : (f32, f32) -> f32
          %19 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %19, %18 : i32, f32
        }
        scf.if %11 {
          memref.store %14#0, %3[%13] : memref<32xf32, 5>
          memref.store %16#0, %1[%13] : memref<32xf32, 5>
        }
      }
      %c32_1 = arith.constant 32 : index
      %c32_i32_2 = arith.constant 32 : i32
      %5 = arith.addi %c32_i32_2, %c31_i32 : i32
      %6 = arith.divui %5, %c32_i32 : i32
      %7 = emitc.cast %6 : i32 to index
      %c1_3 = arith.constant 1 : index
      scf.for %arg2 = %c0 to %c32 step %c1_3 {
        %8 = builtin.unrealized_conversion_cast %arg2 : index to i64
        %9 = emitc.cast %8 : i64 to i32
        %10 = arith.cmpi eq, %9, %c0_i32 : i32
        scf.if %10 {
          %11:2 = scf.for %arg3 = %c0 to %7 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %12 = memref.load %3[%arg3] : memref<32xf32, 5>
            %13 = emitc.add %arg5, %12 : (f32, f32) -> f32
            %14 = memref.load %1[%arg3] : memref<32xf32, 5>
            %15 = emitc.add %arg4, %14 : (f32, f32) -> f32
            scf.yield %15, %13 : f32, f32
          }
          memref.store %11#1, %arg0[%c0] : memref<?xf32>
          memref.store %11#0, %arg1[%c0] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
  gpu.module @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1 {
    gpu.func @_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32) {
      %c0_i32 = arith.constant 0 : i32
      %c0_i8 = arith.constant 0 : i8
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<2xf32, 5>
      %1 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca_1 = memref.alloca() : memref<32x1xf32, 5>
      %2 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca_2 = memref.alloca() : memref<32x1xf32, 5>
      %3 = llvm.mlir.undef : f32
      %4 = gpu.block_id  x
      %5 = arith.index_cast %4 : index to i32
      %c32_3 = arith.constant 32 : index
      %6 = arith.muli %arg9, %arg11 : i32
      %7 = arith.muli %6, %arg12 : i32
      %8 = arith.index_cast %7 : i32 to index
      %9 = arith.muli %arg11, %arg12 : i32
      %10 = arith.subi %c32_3, %c1 : index
      %11 = arith.sitofp %7 : i32 to f32
      %12 = arith.cmpi ne, %arg5, %c0_i8 : i8
      %13 = arith.subf %cst, %arg6 : f32
      %c1_4 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_4 {
        %20 = "polygeist.subindex"(%alloca_1, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %3, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %21 = "polygeist.subindex"(%alloca_2, %arg13) : (memref<32x1xf32, 5>, index) -> memref<?xf32>
        memref.store %3, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        %22 = arith.index_cast %arg13 : index to i32
        %23 = arith.cmpi eq, %22, %c0_i32 : i32
        memref.store %cst_0, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
        memref.store %cst_0, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        %24 = arith.subi %8, %arg13 : index
        %25 = arith.addi %10, %24 : index
        %26 = arith.divui %25, %c32_3 : index
        scf.for %arg14 = %c0 to %26 step %c1 {
          %27 = arith.muli %arg14, %c32_3 : index
          %28 = arith.addi %arg13, %27 : index
          %29 = arith.index_cast %28 : index to i32
          %30 = arith.divsi %29, %9 : i32
          %31 = arith.remsi %29, %9 : i32
          %32 = arith.divsi %31, %arg12 : i32
          %33 = arith.remsi %31, %arg12 : i32
          %34 = arith.muli %30, %arg10 : i32
          %35 = arith.addi %34, %5 : i32
          %36 = arith.muli %35, %arg11 : i32
          %37 = arith.addi %36, %32 : i32
          %38 = arith.muli %37, %arg12 : i32
          %39 = arith.addi %38, %33 : i32
          %40 = arith.index_cast %39 : i32 to index
          %41 = memref.load %arg0[%40] : memref<?xf32>
          %42 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %43 = arith.addf %42, %41 : f32
          memref.store %43, %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
          %44 = arith.mulf %41, %41 : f32
          %45 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
          %46 = arith.addf %45, %44 : f32
          memref.store %46, %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
        }
        func.call @_Z14blockReduceSumRfS_(%21, %20) : (memref<?xf32>, memref<?xf32>) -> ()
        scf.if %23 {
          %27:2 = scf.if %12 -> (f32, f32) {
            %28 = memref.load %alloca_2[%arg13, %c0] : memref<32x1xf32, 5>
            %29 = arith.divf %28, %11 : f32
            %30 = memref.load %alloca_1[%arg13, %c0] : memref<32x1xf32, 5>
            %31 = arith.divf %30, %11 : f32
            %32 = arith.mulf %29, %29 : f32
            %33 = arith.subf %31, %32 : f32
            %34 = memref.load %arg3[%4] : memref<?xf32>
            %35 = arith.mulf %13, %34 : f32
            %36 = arith.mulf %arg6, %29 : f32
            %37 = arith.addf %35, %36 : f32
            memref.store %37, %arg3[%4] : memref<?xf32>
            %38 = memref.load %arg4[%4] : memref<?xf32>
            %39 = arith.mulf %13, %38 : f32
            %40 = arith.mulf %arg6, %33 : f32
            %41 = arith.addf %39, %40 : f32
            memref.store %41, %arg4[%4] : memref<?xf32>
            scf.yield %33, %29 : f32, f32
          } else {
            %28 = memref.load %arg3[%4] : memref<?xf32>
            %29 = memref.load %arg4[%4] : memref<?xf32>
            scf.yield %29, %28 : f32, f32
          }
          memref.store %27#1, %alloca[%c0] : memref<2xf32, 5>
          memref.store %27#0, %alloca[%c1] : memref<2xf32, 5>
        }
      }
      %14 = memref.load %alloca[%c0] : memref<2xf32, 5>
      %15 = memref.load %alloca[%c1] : memref<2xf32, 5>
      %16 = arith.addf %15, %arg7 : f32
      %17 = math.rsqrt %16 : f32
      %18 = memref.load %arg1[%4] : memref<?xf32>
      %19 = memref.load %arg2[%4] : memref<?xf32>
      %c1_5 = arith.constant 1 : index
      scf.for %arg13 = %c0 to %c32 step %c1_5 {
        %20 = arith.subi %8, %arg13 : index
        %21 = arith.addi %10, %20 : index
        %22 = arith.divui %21, %c32_3 : index
        scf.for %arg14 = %c0 to %22 step %c1 {
          %23 = arith.muli %arg14, %c32_3 : index
          %24 = arith.addi %arg13, %23 : index
          %25 = arith.index_cast %24 : index to i32
          %26 = arith.divsi %25, %9 : i32
          %27 = arith.remsi %25, %9 : i32
          %28 = arith.divsi %27, %arg12 : i32
          %29 = arith.remsi %27, %arg12 : i32
          %30 = arith.muli %26, %arg10 : i32
          %31 = arith.addi %30, %5 : i32
          %32 = arith.muli %31, %arg11 : i32
          %33 = arith.addi %32, %28 : i32
          %34 = arith.muli %33, %arg12 : i32
          %35 = arith.addi %34, %29 : i32
          %36 = arith.index_cast %35 : i32 to index
          %37 = memref.load %arg0[%36] : memref<?xf32>
          %38 = arith.subf %37, %14 : f32
          %39 = arith.mulf %38, %17 : f32
          %40 = arith.mulf %39, %18 : f32
          %41 = arith.addf %40, %19 : f32
          memref.store %41, %arg8[%36] : memref<?xf32>
        }
      }
      gpu.return
    }
    func.func @_Z14blockReduceSumRfS_(%arg0: memref<?xf32>, %arg1: memref<?xf32>) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
      %c32_i32 = arith.constant 32 : i32
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0.000000e+00 : f32
      %c31_i32 = arith.constant 31 : i32
      %c2_i32 = arith.constant 2 : i32
      %c-1_i32 = arith.constant -1 : i32
      %c16_i32 = arith.constant 16 : i32
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<32xf32, 5>
      %alloca_0 = memref.alloca() : memref<32xf32, 5>
      %0 = memref.load %arg0[%c0] : memref<?xf32>
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.remui %6, %c32_i32 : i32
        %8 = arith.cmpi eq, %7, %c0_i32 : i32
        %9 = arith.divui %6, %c32_i32 : i32
        %10 = arith.index_cast %9 : i32 to index
        %11:2 = scf.while (%arg3 = %c16_i32, %arg4 = %0) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        %12 = memref.load %arg1[%c0] : memref<?xf32>
        %13:2 = scf.while (%arg3 = %c16_i32, %arg4 = %12) : (i32, f32) -> (f32, i32) {
          %14 = arith.cmpi sgt, %arg3, %c0_i32 : i32
          scf.condition(%14) %arg4, %arg3 : f32, i32
        } do {
        ^bb0(%arg3: f32, %arg4: i32):
          %14 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg3, %arg4, %c31_i32) : (i32, f32, i32, i32) -> f32
          %15 = arith.addf %arg3, %14 : f32
          %16 = arith.divsi %arg4, %c2_i32 : i32
          scf.yield %16, %15 : i32, f32
        }
        scf.if %8 {
          memref.store %11#0, %alloca_0[%10] : memref<32xf32, 5>
          memref.store %13#0, %alloca[%10] : memref<32xf32, 5>
        }
        scf.yield
      }
      %1 = gpu.block_dim  x
      %2 = arith.index_cast %1 : index to i32
      %3 = arith.addi %2, %c31_i32 : i32
      %4 = arith.divui %3, %c32_i32 : i32
      %5 = arith.index_cast %4 : i32 to index
      scf.parallel (%arg2) = (%c0) to (%c32) step (%c1) {
        %6 = arith.index_cast %arg2 : index to i32
        %7 = arith.cmpi eq, %6, %c0_i32 : i32
        scf.if %7 {
          %8:2 = scf.for %arg3 = %c0 to %5 step %c1 iter_args(%arg4 = %cst, %arg5 = %cst) -> (f32, f32) {
            %9 = memref.load %alloca_0[%arg3] : memref<32xf32, 5>
            %10 = arith.addf %arg5, %9 : f32
            %11 = memref.load %alloca[%arg3] : memref<32xf32, 5>
            %12 = arith.addf %arg4, %11 : f32
            scf.yield %12, %10 : f32, f32
          }
          memref.store %8#1, %arg0[%c0] : memref<?xf32>
          memref.store %8#0, %arg1[%c0] : memref<?xf32>
        }
        scf.yield
      }
      return
    }
    func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] GPUBlockIdToNPULowering: process op: 

%4 = gpu.block_id  x
[ict-debug] CastLikeOpToNPULowering: process op: 

%6 = arith.index_cast %5 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%10 = arith.index_cast %9 : i32 to index
[ict-debug] CastLikeOpToNPULowering: process op: 

%46 = "arith.index_cast"(%arg13) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%55 = "arith.index_cast"(%53) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%67 = "arith.index_cast"(%66) : (i32) -> index
[ict-debug] ArithUnaryOpToNPULowering: process op: 

%34 = "math.rsqrt"(%33) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
[ict-debug] ArithUnaryOpToNPULowering: met scalar unary op, need vector help process.

[ict-debug] CastLikeOpToNPULowering: process op: 

%48 = "arith.index_cast"(%46) : (index) -> i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%60 = "arith.index_cast"(%59) : (i32) -> index
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU:

"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>):
      %0 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 0 : index}> : () -> index
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      %9 = "arith.constant"() <{value = 32 : index}> : () -> index
      %10 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %11 = "builtin.unrealized_conversion_cast"(%10) : (!llvm.ptr<6>) -> memref<32xf32, 5>
      %12 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %13 = "builtin.unrealized_conversion_cast"(%12) : (!llvm.ptr<6>) -> memref<32xf32, 5>
      %14 = "memref.load"(%arg0, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %15 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%7, %9, %15) ({
      ^bb0(%arg2: index):
        %22 = "builtin.unrealized_conversion_cast"(%arg2) : (index) -> i64
        %23 = "emitc.cast"(%22) : (i64) -> i32
        %24 = "arith.remui"(%23, %0) : (i32, i32) -> i32
        %25 = "arith.cmpi"(%24, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        %26 = "arith.divui"(%23, %0) : (i32, i32) -> i32
        %27 = "emitc.cast"(%26) : (i32) -> index
        %28:2 = "scf.while"(%6, %14) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %31 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%31, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %31 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %32 = "emitc.add"(%arg3, %31) : (f32, f32) -> f32
          %33 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%33, %32) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        %29 = "memref.load"(%arg1, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
        %30:2 = "scf.while"(%6, %29) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %31 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%31, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %31 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %32 = "emitc.add"(%arg3, %31) : (f32, f32) -> f32
          %33 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%33, %32) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        "scf.if"(%25) ({
          "memref.store"(%28#0, %13, %27) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "memref.store"(%30#0, %11, %27) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %16 = "arith.constant"() <{value = 32 : index}> : () -> index
      %17 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %18 = "arith.addi"(%17, %3) : (i32, i32) -> i32
      %19 = "arith.divui"(%18, %0) : (i32, i32) -> i32
      %20 = "emitc.cast"(%19) : (i32) -> index
      %21 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%7, %9, %21) ({
      ^bb0(%arg2: index):
        %22 = "builtin.unrealized_conversion_cast"(%arg2) : (index) -> i64
        %23 = "emitc.cast"(%22) : (i64) -> i32
        %24 = "arith.cmpi"(%23, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%24) ({
          %25:2 = "scf.for"(%7, %20, %8, %2, %2) ({
          ^bb0(%arg3: index, %arg4: f32, %arg5: f32):
            %26 = "memref.load"(%13, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %27 = "emitc.add"(%arg5, %26) : (f32, f32) -> f32
            %28 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %29 = "emitc.add"(%arg4, %28) : (f32, f32) -> f32
            "scf.yield"(%29, %27) : (f32, f32) -> ()
          }) : (index, index, index, f32, f32) -> (f32, f32)
          "memref.store"(%25#1, %arg0, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "memref.store"(%25#0, %arg1, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z14blockReduceSumRfS_", workgroup_attributions = 0 : i64} : () -> ()
    "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z14blockReduceSumRfS__0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, i8, f32, f32, memref<?xf32>, i32, i32, i32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32):
      %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i8}> : () -> i8
      %2 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "builtin.unrealized_conversion_cast"(%7) : (!llvm.ptr<6>) -> memref<2xf32, 5>
      %9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %10 = "builtin.unrealized_conversion_cast"(%9) : (!llvm.ptr<6>) -> memref<32x1xf32, 5>
      %11 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %12 = "builtin.unrealized_conversion_cast"(%11) : (!llvm.ptr<6>) -> memref<32x1xf32, 5>
      %13 = "llvm.mlir.undef"() : () -> f32
      %14 = "npu.block_id"() : () -> i64
      %15 = "builtin.unrealized_conversion_cast"(%14) : (i64) -> index
      %16 = "emitc.cast"(%14) : (i64) -> i32
      %17 = "arith.constant"() <{value = 32 : index}> : () -> index
      %18 = "arith.muli"(%arg9, %arg11) : (i32, i32) -> i32
      %19 = "arith.muli"(%18, %arg12) : (i32, i32) -> i32
      %20 = "emitc.cast"(%19) : (i32) -> index
      %21 = "arith.muli"(%arg11, %arg12) : (i32, i32) -> i32
      %22 = "arith.subi"(%17, %5) : (index, index) -> index
      %23 = "arith.sitofp"(%19) : (i32) -> f32
      %24 = "arith.cmpi"(%arg5, %1) <{predicate = 1 : i64}> : (i8, i8) -> i1
      %25 = "emitc.sub"(%2, %arg6) : (f32, f32) -> f32
      %26 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %26) ({
      ^bb0(%arg13: index):
        %36 = "builtin.unrealized_conversion_cast"(%arg13) : (index) -> i64
        %37 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %38 = "llvm.getelementptr"(%9, %36, %37) <{elem_type = !llvm.array<1 x f32>, rawConstantIndices = array<i32: -2147483648, -2147483648>}> : (!llvm.ptr<6>, i64, i64) -> !llvm.ptr<6>
        %39 = "llvm.bitcast"(%38) : (!llvm.ptr<6>) -> !llvm.ptr
        %40 = "builtin.unrealized_conversion_cast"(%39) : (!llvm.ptr) -> memref<?xf32>
        "memref.store"(%13, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %41 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %42 = "llvm.getelementptr"(%11, %36, %41) <{elem_type = !llvm.array<1 x f32>, rawConstantIndices = array<i32: -2147483648, -2147483648>}> : (!llvm.ptr<6>, i64, i64) -> !llvm.ptr<6>
        %43 = "llvm.bitcast"(%42) : (!llvm.ptr<6>) -> !llvm.ptr
        %44 = "builtin.unrealized_conversion_cast"(%43) : (!llvm.ptr) -> memref<?xf32>
        "memref.store"(%13, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %45 = "emitc.cast"(%36) : (i64) -> i32
        %46 = "arith.cmpi"(%45, %0) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "memref.store"(%3, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        "memref.store"(%3, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %47 = "arith.subi"(%20, %arg13) : (index, index) -> index
        %48 = "arith.addi"(%22, %47) : (index, index) -> index
        %49 = "arith.divui"(%48, %17) : (index, index) -> index
        "scf.for"(%4, %49, %5) ({
        ^bb0(%arg14: index):
          %50 = "arith.muli"(%arg14, %17) : (index, index) -> index
          %51 = "arith.addi"(%arg13, %50) : (index, index) -> index
          %52 = "builtin.unrealized_conversion_cast"(%51) : (index) -> i64
          %53 = "emitc.cast"(%52) : (i64) -> i32
          %54 = "arith.divsi"(%53, %21) : (i32, i32) -> i32
          %55 = "arith.remsi"(%53, %21) : (i32, i32) -> i32
          %56 = "arith.divsi"(%55, %arg12) : (i32, i32) -> i32
          %57 = "arith.remsi"(%55, %arg12) : (i32, i32) -> i32
          %58 = "arith.muli"(%54, %arg10) : (i32, i32) -> i32
          %59 = "arith.addi"(%58, %16) : (i32, i32) -> i32
          %60 = "arith.muli"(%59, %arg11) : (i32, i32) -> i32
          %61 = "arith.addi"(%60, %56) : (i32, i32) -> i32
          %62 = "arith.muli"(%61, %arg12) : (i32, i32) -> i32
          %63 = "arith.addi"(%62, %57) : (i32, i32) -> i32
          %64 = "emitc.cast"(%63) : (i32) -> index
          %65 = "memref.load"(%arg0, %64) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %66 = "memref.load"(%12, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
          %67 = "emitc.add"(%66, %65) : (f32, f32) -> f32
          "memref.store"(%67, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
          %68 = "emitc.mul"(%65, %65) : (f32, f32) -> f32
          %69 = "memref.load"(%10, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
          %70 = "emitc.add"(%69, %68) : (f32, f32) -> f32
          "memref.store"(%70, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "func.call"(%44, %40) <{callee = @_Z14blockReduceSumRfS_}> : (memref<?xf32>, memref<?xf32>) -> ()
        "scf.if"(%46) ({
          %50:2 = "scf.if"(%24) ({
            %51 = "memref.load"(%12, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
            %52 = "emitc.div"(%51, %23) : (f32, f32) -> f32
            %53 = "memref.load"(%10, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
            %54 = "emitc.div"(%53, %23) : (f32, f32) -> f32
            %55 = "emitc.mul"(%52, %52) : (f32, f32) -> f32
            %56 = "emitc.sub"(%54, %55) : (f32, f32) -> f32
            %57 = "memref.load"(%arg3, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %58 = "emitc.mul"(%25, %57) : (f32, f32) -> f32
            %59 = "emitc.mul"(%arg6, %52) : (f32, f32) -> f32
            %60 = "emitc.add"(%58, %59) : (f32, f32) -> f32
            "memref.store"(%60, %arg3, %15) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
            %61 = "memref.load"(%arg4, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %62 = "emitc.mul"(%25, %61) : (f32, f32) -> f32
            %63 = "emitc.mul"(%arg6, %56) : (f32, f32) -> f32
            %64 = "emitc.add"(%62, %63) : (f32, f32) -> f32
            "memref.store"(%64, %arg4, %15) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
            "scf.yield"(%56, %52) : (f32, f32) -> ()
          }, {
            %51 = "memref.load"(%arg3, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %52 = "memref.load"(%arg4, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            "scf.yield"(%52, %51) : (f32, f32) -> ()
          }) : (i1) -> (f32, f32)
          "memref.store"(%50#1, %8, %4) <{nontemporal = false}> : (f32, memref<2xf32, 5>, index) -> ()
          "memref.store"(%50#0, %8, %5) <{nontemporal = false}> : (f32, memref<2xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %27 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<2xf32, 5>, index) -> f32
      %28 = "memref.load"(%8, %5) <{nontemporal = false}> : (memref<2xf32, 5>, index) -> f32
      %29 = "emitc.add"(%28, %arg7) : (f32, f32) -> f32
      %30 = "emitc.call"(%29) <{callee = "sqrtf"}> : (f32) -> f32
      %31 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
      %32 = "emitc.div"(%31, %30) : (f32, f32) -> f32
      %33 = "memref.load"(%arg1, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %34 = "memref.load"(%arg2, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %35 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %35) ({
      ^bb0(%arg13: index):
        %36 = "arith.subi"(%20, %arg13) : (index, index) -> index
        %37 = "arith.addi"(%22, %36) : (index, index) -> index
        %38 = "arith.divui"(%37, %17) : (index, index) -> index
        "scf.for"(%4, %38, %5) ({
        ^bb0(%arg14: index):
          %39 = "arith.muli"(%arg14, %17) : (index, index) -> index
          %40 = "arith.addi"(%arg13, %39) : (index, index) -> index
          %41 = "builtin.unrealized_conversion_cast"(%40) : (index) -> i64
          %42 = "emitc.cast"(%41) : (i64) -> i32
          %43 = "arith.divsi"(%42, %21) : (i32, i32) -> i32
          %44 = "arith.remsi"(%42, %21) : (i32, i32) -> i32
          %45 = "arith.divsi"(%44, %arg12) : (i32, i32) -> i32
          %46 = "arith.remsi"(%44, %arg12) : (i32, i32) -> i32
          %47 = "arith.muli"(%43, %arg10) : (i32, i32) -> i32
          %48 = "arith.addi"(%47, %16) : (i32, i32) -> i32
          %49 = "arith.muli"(%48, %arg11) : (i32, i32) -> i32
          %50 = "arith.addi"(%49, %45) : (i32, i32) -> i32
          %51 = "arith.muli"(%50, %arg12) : (i32, i32) -> i32
          %52 = "arith.addi"(%51, %46) : (i32, i32) -> i32
          %53 = "emitc.cast"(%52) : (i32) -> index
          %54 = "memref.load"(%arg0, %53) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %55 = "emitc.sub"(%54, %27) : (f32, f32) -> f32
          %56 = "emitc.mul"(%55, %32) : (f32, f32) -> f32
          %57 = "emitc.mul"(%56, %33) : (f32, f32) -> f32
          %58 = "emitc.add"(%57, %34) : (f32, f32) -> f32
          "memref.store"(%58, %arg8, %53) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii", workgroup_attributions = 0 : i64} : () -> ()
    "func.func"() <{function_type = (memref<?xf32>, memref<?xf32>) -> (), sym_name = "_Z14blockReduceSumRfS_"}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>):
      %0 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 0 : index}> : () -> index
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      %9 = "arith.constant"() <{value = 32 : index}> : () -> index
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32, 5>
      %11 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32, 5>
      %12 = "memref.load"(%arg0, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      "scf.parallel"(%7, %9, %8) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
      ^bb0(%arg2: index):
        %18 = "arith.index_cast"(%arg2) : (index) -> i32
        %19 = "arith.remui"(%18, %0) : (i32, i32) -> i32
        %20 = "arith.cmpi"(%19, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        %21 = "arith.divui"(%18, %0) : (i32, i32) -> i32
        %22 = "arith.index_cast"(%21) : (i32) -> index
        %23:2 = "scf.while"(%6, %12) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %26 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%26, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %26 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %27 = "arith.addf"(%arg3, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %28 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%28, %27) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        %24 = "memref.load"(%arg1, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
        %25:2 = "scf.while"(%6, %24) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %26 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%26, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %26 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %27 = "arith.addf"(%arg3, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %28 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%28, %27) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        "scf.if"(%20) ({
          "memref.store"(%23#0, %11, %22) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "memref.store"(%25#0, %10, %22) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %13 = "gpu.block_dim"() <{dimension = #gpu<dim x>}> : () -> index
      %14 = "arith.index_cast"(%13) : (index) -> i32
      %15 = "arith.addi"(%14, %3) : (i32, i32) -> i32
      %16 = "arith.divui"(%15, %0) : (i32, i32) -> i32
      %17 = "arith.index_cast"(%16) : (i32) -> index
      "scf.parallel"(%7, %9, %8) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
      ^bb0(%arg2: index):
        %18 = "arith.index_cast"(%arg2) : (index) -> i32
        %19 = "arith.cmpi"(%18, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%19) ({
          %20:2 = "scf.for"(%7, %17, %8, %2, %2) ({
          ^bb0(%arg3: index, %arg4: f32, %arg5: f32):
            %21 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %22 = "arith.addf"(%arg5, %21) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            %23 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %24 = "arith.addf"(%arg4, %23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "scf.yield"(%24, %22) : (f32, f32) -> ()
          }) : (index, index, index, f32, f32) -> (f32, f32)
          "memref.store"(%20#1, %arg0, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "memref.store"(%20#0, %arg1, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "func.return"() : () -> ()
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU: end

loc("./cuda_ops/33_BatchNorm.cu":94:5): error: 'llvm.bitcast' op cannot cast pointers of different address spaces, use 'llvm.addrspacecast' instead
"builtin.module"() ({
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>):
      %0 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 0 : index}> : () -> index
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      %9 = "arith.constant"() <{value = 32 : index}> : () -> index
      %10 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %11 = "builtin.unrealized_conversion_cast"(%10) : (!llvm.ptr<6>) -> memref<32xf32, 5>
      %12 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %13 = "builtin.unrealized_conversion_cast"(%12) : (!llvm.ptr<6>) -> memref<32xf32, 5>
      %14 = "memref.load"(%arg0, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %15 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%7, %9, %15) ({
      ^bb0(%arg2: index):
        %22 = "builtin.unrealized_conversion_cast"(%arg2) : (index) -> i64
        %23 = "emitc.cast"(%22) : (i64) -> i32
        %24 = "arith.remui"(%23, %0) : (i32, i32) -> i32
        %25 = "arith.cmpi"(%24, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        %26 = "arith.divui"(%23, %0) : (i32, i32) -> i32
        %27 = "emitc.cast"(%26) : (i32) -> index
        %28:2 = "scf.while"(%6, %14) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %31 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%31, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %31 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %32 = "emitc.add"(%arg3, %31) : (f32, f32) -> f32
          %33 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%33, %32) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        %29 = "memref.load"(%arg1, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
        %30:2 = "scf.while"(%6, %29) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %31 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%31, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %31 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %32 = "emitc.add"(%arg3, %31) : (f32, f32) -> f32
          %33 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%33, %32) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        "scf.if"(%25) ({
          "memref.store"(%28#0, %13, %27) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "memref.store"(%30#0, %11, %27) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %16 = "arith.constant"() <{value = 32 : index}> : () -> index
      %17 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %18 = "arith.addi"(%17, %3) : (i32, i32) -> i32
      %19 = "arith.divui"(%18, %0) : (i32, i32) -> i32
      %20 = "emitc.cast"(%19) : (i32) -> index
      %21 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%7, %9, %21) ({
      ^bb0(%arg2: index):
        %22 = "builtin.unrealized_conversion_cast"(%arg2) : (index) -> i64
        %23 = "emitc.cast"(%22) : (i64) -> i32
        %24 = "arith.cmpi"(%23, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%24) ({
          %25:2 = "scf.for"(%7, %20, %8, %2, %2) ({
          ^bb0(%arg3: index, %arg4: f32, %arg5: f32):
            %26 = "memref.load"(%13, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %27 = "emitc.add"(%arg5, %26) : (f32, f32) -> f32
            %28 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %29 = "emitc.add"(%arg4, %28) : (f32, f32) -> f32
            "scf.yield"(%29, %27) : (f32, f32) -> ()
          }) : (index, index, index, f32, f32) -> (f32, f32)
          "memref.store"(%25#1, %arg0, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "memref.store"(%25#0, %arg1, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z14blockReduceSumRfS_", workgroup_attributions = 0 : i64} : () -> ()
    "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z14blockReduceSumRfS__0"} : () -> ()
  "gpu.module"() ({
    "gpu.func"() <{function_type = (memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, memref<?xf32>, i8, f32, f32, memref<?xf32>, i32, i32, i32, i32) -> ()}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: memref<?xf32>, %arg3: memref<?xf32>, %arg4: memref<?xf32>, %arg5: i8, %arg6: f32, %arg7: f32, %arg8: memref<?xf32>, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32):
      %0 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i8}> : () -> i8
      %2 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %4 = "arith.constant"() <{value = 0 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1 : index}> : () -> index
      %6 = "arith.constant"() <{value = 32 : index}> : () -> index
      %7 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %8 = "builtin.unrealized_conversion_cast"(%7) : (!llvm.ptr<6>) -> memref<2xf32, 5>
      %9 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %10 = "builtin.unrealized_conversion_cast"(%9) : (!llvm.ptr<6>) -> memref<32x1xf32, 5>
      %11 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %12 = "builtin.unrealized_conversion_cast"(%11) : (!llvm.ptr<6>) -> memref<32x1xf32, 5>
      %13 = "llvm.mlir.undef"() : () -> f32
      %14 = "npu.block_id"() : () -> i64
      %15 = "builtin.unrealized_conversion_cast"(%14) : (i64) -> index
      %16 = "emitc.cast"(%14) : (i64) -> i32
      %17 = "arith.constant"() <{value = 32 : index}> : () -> index
      %18 = "arith.muli"(%arg9, %arg11) : (i32, i32) -> i32
      %19 = "arith.muli"(%18, %arg12) : (i32, i32) -> i32
      %20 = "emitc.cast"(%19) : (i32) -> index
      %21 = "arith.muli"(%arg11, %arg12) : (i32, i32) -> i32
      %22 = "arith.subi"(%17, %5) : (index, index) -> index
      %23 = "arith.sitofp"(%19) : (i32) -> f32
      %24 = "arith.cmpi"(%arg5, %1) <{predicate = 1 : i64}> : (i8, i8) -> i1
      %25 = "emitc.sub"(%2, %arg6) : (f32, f32) -> f32
      %26 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %26) ({
      ^bb0(%arg13: index):
        %36 = "builtin.unrealized_conversion_cast"(%arg13) : (index) -> i64
        %37 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %38 = "llvm.getelementptr"(%9, %36, %37) <{elem_type = !llvm.array<1 x f32>, rawConstantIndices = array<i32: -2147483648, -2147483648>}> : (!llvm.ptr<6>, i64, i64) -> !llvm.ptr<6>
        %39 = "llvm.bitcast"(%38) : (!llvm.ptr<6>) -> !llvm.ptr
        %40 = "builtin.unrealized_conversion_cast"(%39) : (!llvm.ptr) -> memref<?xf32>
        "memref.store"(%13, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %41 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %42 = "llvm.getelementptr"(%11, %36, %41) <{elem_type = !llvm.array<1 x f32>, rawConstantIndices = array<i32: -2147483648, -2147483648>}> : (!llvm.ptr<6>, i64, i64) -> !llvm.ptr<6>
        %43 = "llvm.bitcast"(%42) : (!llvm.ptr<6>) -> !llvm.ptr
        %44 = "builtin.unrealized_conversion_cast"(%43) : (!llvm.ptr) -> memref<?xf32>
        "memref.store"(%13, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %45 = "emitc.cast"(%36) : (i64) -> i32
        %46 = "arith.cmpi"(%45, %0) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "memref.store"(%3, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        "memref.store"(%3, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
        %47 = "arith.subi"(%20, %arg13) : (index, index) -> index
        %48 = "arith.addi"(%22, %47) : (index, index) -> index
        %49 = "arith.divui"(%48, %17) : (index, index) -> index
        "scf.for"(%4, %49, %5) ({
        ^bb0(%arg14: index):
          %50 = "arith.muli"(%arg14, %17) : (index, index) -> index
          %51 = "arith.addi"(%arg13, %50) : (index, index) -> index
          %52 = "builtin.unrealized_conversion_cast"(%51) : (index) -> i64
          %53 = "emitc.cast"(%52) : (i64) -> i32
          %54 = "arith.divsi"(%53, %21) : (i32, i32) -> i32
          %55 = "arith.remsi"(%53, %21) : (i32, i32) -> i32
          %56 = "arith.divsi"(%55, %arg12) : (i32, i32) -> i32
          %57 = "arith.remsi"(%55, %arg12) : (i32, i32) -> i32
          %58 = "arith.muli"(%54, %arg10) : (i32, i32) -> i32
          %59 = "arith.addi"(%58, %16) : (i32, i32) -> i32
          %60 = "arith.muli"(%59, %arg11) : (i32, i32) -> i32
          %61 = "arith.addi"(%60, %56) : (i32, i32) -> i32
          %62 = "arith.muli"(%61, %arg12) : (i32, i32) -> i32
          %63 = "arith.addi"(%62, %57) : (i32, i32) -> i32
          %64 = "emitc.cast"(%63) : (i32) -> index
          %65 = "memref.load"(%arg0, %64) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %66 = "memref.load"(%12, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
          %67 = "emitc.add"(%66, %65) : (f32, f32) -> f32
          "memref.store"(%67, %12, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
          %68 = "emitc.mul"(%65, %65) : (f32, f32) -> f32
          %69 = "memref.load"(%10, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
          %70 = "emitc.add"(%69, %68) : (f32, f32) -> f32
          "memref.store"(%70, %10, %arg13, %4) <{nontemporal = false}> : (f32, memref<32x1xf32, 5>, index, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "func.call"(%44, %40) <{callee = @_Z14blockReduceSumRfS_}> : (memref<?xf32>, memref<?xf32>) -> ()
        "scf.if"(%46) ({
          %50:2 = "scf.if"(%24) ({
            %51 = "memref.load"(%12, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
            %52 = "emitc.div"(%51, %23) : (f32, f32) -> f32
            %53 = "memref.load"(%10, %arg13, %4) <{nontemporal = false}> : (memref<32x1xf32, 5>, index, index) -> f32
            %54 = "emitc.div"(%53, %23) : (f32, f32) -> f32
            %55 = "emitc.mul"(%52, %52) : (f32, f32) -> f32
            %56 = "emitc.sub"(%54, %55) : (f32, f32) -> f32
            %57 = "memref.load"(%arg3, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %58 = "emitc.mul"(%25, %57) : (f32, f32) -> f32
            %59 = "emitc.mul"(%arg6, %52) : (f32, f32) -> f32
            %60 = "emitc.add"(%58, %59) : (f32, f32) -> f32
            "memref.store"(%60, %arg3, %15) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
            %61 = "memref.load"(%arg4, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %62 = "emitc.mul"(%25, %61) : (f32, f32) -> f32
            %63 = "emitc.mul"(%arg6, %56) : (f32, f32) -> f32
            %64 = "emitc.add"(%62, %63) : (f32, f32) -> f32
            "memref.store"(%64, %arg4, %15) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
            "scf.yield"(%56, %52) : (f32, f32) -> ()
          }, {
            %51 = "memref.load"(%arg3, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            %52 = "memref.load"(%arg4, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
            "scf.yield"(%52, %51) : (f32, f32) -> ()
          }) : (i1) -> (f32, f32)
          "memref.store"(%50#1, %8, %4) <{nontemporal = false}> : (f32, memref<2xf32, 5>, index) -> ()
          "memref.store"(%50#0, %8, %5) <{nontemporal = false}> : (f32, memref<2xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %27 = "memref.load"(%8, %4) <{nontemporal = false}> : (memref<2xf32, 5>, index) -> f32
      %28 = "memref.load"(%8, %5) <{nontemporal = false}> : (memref<2xf32, 5>, index) -> f32
      %29 = "emitc.add"(%28, %arg7) : (f32, f32) -> f32
      %30 = "emitc.call"(%29) <{callee = "sqrtf"}> : (f32) -> f32
      %31 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
      %32 = "emitc.div"(%31, %30) : (f32, f32) -> f32
      %33 = "memref.load"(%arg1, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %34 = "memref.load"(%arg2, %15) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      %35 = "arith.constant"() <{value = 1 : index}> : () -> index
      "scf.for"(%4, %6, %35) ({
      ^bb0(%arg13: index):
        %36 = "arith.subi"(%20, %arg13) : (index, index) -> index
        %37 = "arith.addi"(%22, %36) : (index, index) -> index
        %38 = "arith.divui"(%37, %17) : (index, index) -> index
        "scf.for"(%4, %38, %5) ({
        ^bb0(%arg14: index):
          %39 = "arith.muli"(%arg14, %17) : (index, index) -> index
          %40 = "arith.addi"(%arg13, %39) : (index, index) -> index
          %41 = "builtin.unrealized_conversion_cast"(%40) : (index) -> i64
          %42 = "emitc.cast"(%41) : (i64) -> i32
          %43 = "arith.divsi"(%42, %21) : (i32, i32) -> i32
          %44 = "arith.remsi"(%42, %21) : (i32, i32) -> i32
          %45 = "arith.divsi"(%44, %arg12) : (i32, i32) -> i32
          %46 = "arith.remsi"(%44, %arg12) : (i32, i32) -> i32
          %47 = "arith.muli"(%43, %arg10) : (i32, i32) -> i32
          %48 = "arith.addi"(%47, %16) : (i32, i32) -> i32
          %49 = "arith.muli"(%48, %arg11) : (i32, i32) -> i32
          %50 = "arith.addi"(%49, %45) : (i32, i32) -> i32
          %51 = "arith.muli"(%50, %arg12) : (i32, i32) -> i32
          %52 = "arith.addi"(%51, %46) : (i32, i32) -> i32
          %53 = "emitc.cast"(%52) : (i32) -> index
          %54 = "memref.load"(%arg0, %53) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
          %55 = "emitc.sub"(%54, %27) : (f32, f32) -> f32
          %56 = "emitc.mul"(%55, %32) : (f32, f32) -> f32
          %57 = "emitc.mul"(%56, %33) : (f32, f32) -> f32
          %58 = "emitc.add"(%57, %34) : (f32, f32) -> f32
          "memref.store"(%58, %arg8, %53) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }) : (index, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "gpu.return"() : () -> ()
    }) {sym_name = "_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii", workgroup_attributions = 0 : i64} : () -> ()
    "func.func"() <{function_type = (memref<?xf32>, memref<?xf32>) -> (), sym_name = "_Z14blockReduceSumRfS_"}> ({
    ^bb0(%arg0: memref<?xf32>, %arg1: memref<?xf32>):
      %0 = "arith.constant"() <{value = 32 : i32}> : () -> i32
      %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 2 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 0 : index}> : () -> index
      %8 = "arith.constant"() <{value = 1 : index}> : () -> index
      %9 = "arith.constant"() <{value = 32 : index}> : () -> index
      %10 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32, 5>
      %11 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32, 5>
      %12 = "memref.load"(%arg0, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
      "scf.parallel"(%7, %9, %8) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
      ^bb0(%arg2: index):
        %18 = "arith.index_cast"(%arg2) : (index) -> i32
        %19 = "arith.remui"(%18, %0) : (i32, i32) -> i32
        %20 = "arith.cmpi"(%19, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        %21 = "arith.divui"(%18, %0) : (i32, i32) -> i32
        %22 = "arith.index_cast"(%21) : (i32) -> index
        %23:2 = "scf.while"(%6, %12) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %26 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%26, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %26 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %27 = "arith.addf"(%arg3, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %28 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%28, %27) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        %24 = "memref.load"(%arg1, %7) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
        %25:2 = "scf.while"(%6, %24) ({
        ^bb0(%arg3: i32, %arg4: f32):
          %26 = "arith.cmpi"(%arg3, %1) <{predicate = 4 : i64}> : (i32, i32) -> i1
          "scf.condition"(%26, %arg4, %arg3) : (i1, f32, i32) -> ()
        }, {
        ^bb0(%arg3: f32, %arg4: i32):
          %26 = "func.call"(%5, %arg3, %arg4, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
          %27 = "arith.addf"(%arg3, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %28 = "arith.divsi"(%arg4, %4) : (i32, i32) -> i32
          "scf.yield"(%28, %27) : (i32, f32) -> ()
        }) : (i32, f32) -> (f32, i32)
        "scf.if"(%20) ({
          "memref.store"(%23#0, %11, %22) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "memref.store"(%25#0, %10, %22) <{nontemporal = false}> : (f32, memref<32xf32, 5>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      %13 = "gpu.block_dim"() <{dimension = #gpu<dim x>}> : () -> index
      %14 = "arith.index_cast"(%13) : (index) -> i32
      %15 = "arith.addi"(%14, %3) : (i32, i32) -> i32
      %16 = "arith.divui"(%15, %0) : (i32, i32) -> i32
      %17 = "arith.index_cast"(%16) : (i32) -> index
      "scf.parallel"(%7, %9, %8) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
      ^bb0(%arg2: index):
        %18 = "arith.index_cast"(%arg2) : (index) -> i32
        %19 = "arith.cmpi"(%18, %1) <{predicate = 0 : i64}> : (i32, i32) -> i1
        "scf.if"(%19) ({
          %20:2 = "scf.for"(%7, %17, %8, %2, %2) ({
          ^bb0(%arg3: index, %arg4: f32, %arg5: f32):
            %21 = "memref.load"(%11, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %22 = "arith.addf"(%arg5, %21) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            %23 = "memref.load"(%10, %arg3) <{nontemporal = false}> : (memref<32xf32, 5>, index) -> f32
            %24 = "arith.addf"(%arg4, %23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
            "scf.yield"(%24, %22) : (f32, f32) -> ()
          }) : (index, index, index, f32, f32) -> (f32, f32)
          "memref.store"(%20#1, %arg0, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "memref.store"(%20#0, %arg1, %7) <{nontemporal = false}> : (f32, memref<?xf32>, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index) -> ()
      "func.return"() : () -> ()
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
    }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
    "gpu.module_end"() : () -> ()
  }) {sym_name = "_Z36adaptive_blocksize_batch_norm_kernelPKfS0_S0_PfS1_bffS1_iiii_1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
