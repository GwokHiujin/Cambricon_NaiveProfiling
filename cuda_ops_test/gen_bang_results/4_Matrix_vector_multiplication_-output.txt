warning: CUDA version 12.1 is only partially supported
warning: we failed to emit call to builtin function __nvvm_shfl_sync_down_f32
warning: CUDA version 12.1 is only partially supported
[ict-debug] driver.cc: After return 5, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z15warp_reduce_sumf(%arg0: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %0:2 = scf.while (%arg1 = %c16_i32, %arg2 = %arg0) : (i32, f32) -> (f32, i32) {
      %1 = arith.cmpi sgt, %arg1, %c0_i32 : i32
      scf.condition(%1) %arg2, %arg1 : f32, i32
    } do {
    ^bb0(%arg1: f32, %arg2: i32):
      %1 = func.call @_Z16__shfl_down_syncjfji(%c-1_i32, %arg1, %arg2, %c32_i32) : (i32, f32, i32, i32) -> f32
      %2 = arith.addf %arg1, %1 : f32
      %3 = arith.shrsi %arg2, %c1_i32 : i32
      scf.yield %3, %2 : i32, f32
    }
    return %0#0 : f32
  }
  func.func private @_Z16__shfl_down_syncjfji(%arg0: i32, %arg1: f32, %arg2: i32, %arg3: i32) -> f32 attributes {llvm.linkage = #llvm.linkage<linkonce_odr>, polygeist.device_only_func = "1"} {
    %c31_i32 = arith.constant 31 : i32
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %0 = arith.subi %c32_i32, %arg3 : i32
    %1 = arith.shli %0, %c8_i32 : i32
    %2 = arith.ori %1, %c31_i32 : i32
    %3 = call @__nvvm_shfl_sync_down_f32(%arg0, %arg1, %arg2, %2) : (i32, f32, i32, i32) -> f32
    return %3 : f32
  }
  func.func private @_Z16block_reduce_sumf(%arg0: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.000000e+00 : f32
    %c16_i32 = arith.constant 16 : i32
    %c0_i32 = arith.constant 0 : i32
    %c32_i32 = arith.constant 32 : i32
    %alloca = memref.alloca() : memref<16xf32, 5>
    %0 = gpu.thread_id  x
    %1 = arith.index_cast %0 : index to i32
    %2 = arith.remui %1, %c32_i32 : i32
    %3 = arith.cmpi eq, %2, %c0_i32 : i32
    %4 = gpu.thread_id  x
    %5 = arith.index_cast %4 : index to i32
    %6 = arith.divui %5, %c32_i32 : i32
    %7 = arith.index_cast %6 : i32 to index
    %8 = arith.cmpi eq, %6, %c0_i32 : i32
    %9 = call @_Z15warp_reduce_sumf(%arg0) : (f32) -> f32
    scf.if %3 {
      affine.store %9, %alloca[symbol(%7)] : memref<16xf32, 5>
    }
    nvvm.barrier0
    %10 = gpu.thread_id  x
    %11 = arith.index_cast %10 : index to i32
    %12 = arith.cmpi ult, %11, %c16_i32 : i32
    %13 = scf.if %12 -> (f32) {
      %15 = affine.load %alloca[symbol(%0) mod 32] : memref<16xf32, 5>
      scf.yield %15 : f32
    } else {
      scf.yield %cst : f32
    }
    %14 = scf.if %8 -> (f32) {
      %15 = func.call @_Z15warp_reduce_sumf(%13) : (f32) -> f32
      scf.yield %15 : f32
    } else {
      scf.yield %13 : f32
    }
    return %14 : f32
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
[ict-debug] driver.cc: After return 5, module: end

[ict-debug] driver.cc: After return 6, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z15warp_reduce_sumf(%arg0: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c31_i32 = arith.constant 31 : i32
    %c1_i32 = arith.constant 1 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %0:2 = scf.while (%arg1 = %c16_i32, %arg2 = %arg0) : (i32, f32) -> (f32, i32) {
      %1 = arith.cmpi sgt, %arg1, %c0_i32 : i32
      scf.condition(%1) %arg2, %arg1 : f32, i32
    } do {
    ^bb0(%arg1: f32, %arg2: i32):
      %1 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg1, %arg2, %c31_i32) : (i32, f32, i32, i32) -> f32
      %2 = arith.addf %arg1, %1 : f32
      %3 = arith.shrsi %arg2, %c1_i32 : i32
      scf.yield %3, %2 : i32, f32
    }
    return %0#0 : f32
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
[ict-debug] driver.cc: After return 6, module: end

WrapAndReplaceBarrierPass::runOnOperation(): before execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z15warp_reduce_sumf(%arg0: f32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c31_i32 = arith.constant 31 : i32
    %c1_i32 = arith.constant 1 : i32
    %c-1_i32 = arith.constant -1 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %0:2 = scf.while (%arg1 = %c16_i32, %arg2 = %arg0) : (i32, f32) -> (f32, i32) {
      %1 = arith.cmpi sgt, %arg1, %c0_i32 : i32
      scf.condition(%1) %arg2, %arg1 : f32, i32
    } do {
    ^bb0(%arg1: f32, %arg2: i32):
      %1 = func.call @__nvvm_shfl_sync_down_f32(%c-1_i32, %arg1, %arg2, %c31_i32) : (i32, f32, i32, i32) -> f32
      %2 = arith.addf %arg1, %1 : f32
      %3 = arith.shrsi %arg2, %c1_i32 : i32
      scf.yield %3, %2 : i32, f32
    }
    return %0#0 : f32
  }
  func.func private @__nvvm_shfl_sync_down_f32(i32, f32, i32, i32) -> f32 attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"}
}
WrapAndReplaceBarrierPass::runOnOperation(): before execute: end
[ict-debug] WrapAndReplaceBarrierPass::runOnOperation(): Function name: __nvvm_shfl_sync_down_f32. func.getBlocks().size() == 0! this function is empty, skip it.

WrapAndReplaceBarrierPass::runOnOperation(): after execute: 
"builtin.module"() ({
  "func.func"() <{function_type = (f32) -> f32, sym_name = "_Z15warp_reduce_sumf"}> ({
  ^bb0(%arg0: f32):
    %0 = "arith.constant"() <{value = 0 : index}> : () -> index
    %1 = "arith.constant"() <{value = 1 : index}> : () -> index
    %2 = "arith.constant"() <{value = 32 : index}> : () -> index
    "scf.parallel"(%0, %2, %1) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
    ^bb0(%arg1: index):
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %8:2 = "scf.while"(%7, %arg0) ({
      ^bb0(%arg2: i32, %arg3: f32):
        %9 = "arith.cmpi"(%arg2, %6) <{predicate = 4 : i64}> : (i32, i32) -> i1
        "scf.condition"(%9, %arg3, %arg2) : (i1, f32, i32) -> ()
      }, {
      ^bb0(%arg2: f32, %arg3: i32):
        %9 = "func.call"(%5, %arg2, %arg3, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
        %10 = "arith.addf"(%arg2, %9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %11 = "arith.shrsi"(%arg3, %4) : (i32, i32) -> i32
        "scf.yield"(%11, %10) : (i32, f32) -> ()
      }) : (i32, f32) -> (f32, i32)
      "scf.yield"() : () -> ()
    }) : (index, index, index) -> ()
    "func.return"(<<UNKNOWN SSA VALUE>>) : (f32) -> ()
  }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
  "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
  }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
WrapAndReplaceBarrierPass::runOnOperation(): after execute: end
loc("./cuda_ops/4_Matrix_vector_multiplication_.cu":16:1): error: operation's operand is unlinked
"builtin.module"() ({
  "func.func"() <{function_type = (f32) -> f32, sym_name = "_Z15warp_reduce_sumf"}> ({
  ^bb0(%arg0: f32):
    %0 = "arith.constant"() <{value = 0 : index}> : () -> index
    %1 = "arith.constant"() <{value = 1 : index}> : () -> index
    %2 = "arith.constant"() <{value = 32 : index}> : () -> index
    "scf.parallel"(%0, %2, %1) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>}> ({
    ^bb0(%arg1: index):
      %3 = "arith.constant"() <{value = 31 : i32}> : () -> i32
      %4 = "arith.constant"() <{value = 1 : i32}> : () -> i32
      %5 = "arith.constant"() <{value = -1 : i32}> : () -> i32
      %6 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %7 = "arith.constant"() <{value = 16 : i32}> : () -> i32
      %8:2 = "scf.while"(%7, %arg0) ({
      ^bb0(%arg2: i32, %arg3: f32):
        %9 = "arith.cmpi"(%arg2, %6) <{predicate = 4 : i64}> : (i32, i32) -> i1
        "scf.condition"(%9, %arg3, %arg2) : (i1, f32, i32) -> ()
      }, {
      ^bb0(%arg2: f32, %arg3: i32):
        %9 = "func.call"(%5, %arg2, %arg3, %3) <{callee = @__nvvm_shfl_sync_down_f32}> : (i32, f32, i32, i32) -> f32
        %10 = "arith.addf"(%arg2, %9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %11 = "arith.shrsi"(%arg3, %4) : (i32, i32) -> i32
        "scf.yield"(%11, %10) : (i32, f32) -> ()
      }) : (i32, f32) -> (f32, i32)
      "scf.yield"() : () -> ()
    }) : (index, index, index) -> ()
    "func.return"(<<UNKNOWN SSA VALUE>>) : (f32) -> ()
  }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
  "func.func"() <{function_type = (i32, f32, i32, i32) -> f32, sym_name = "__nvvm_shfl_sync_down_f32", sym_visibility = "private"}> ({
  }) {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} : () -> ()
}) {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>, #dlti.dl_entry<"dlti.endianness", "little">>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} : () -> ()
