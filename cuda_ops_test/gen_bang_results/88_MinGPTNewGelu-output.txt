warning: CUDA version 12.1 is only partially supported
warning: CUDA version 12.1 is only partially supported
[ict-debug] driver.cc: After return 5, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z33__device_stub__gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    call @_Z18gelu_kernel_sharedPKfPfi(%arg0, %arg1, %arg2) : (memref<?xf32>, memref<?xf32>, i32) -> ()
    return
  }
  func.func private @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.797884583 : f32
    %cst_0 = arith.constant 4.471500e-02 : f32
    %c4_i32 = arith.constant 4 : i32
    %cst_1 = arith.constant 5.000000e-01 : f32
    %cst_2 = arith.constant 1.000000e+00 : f32
    %0 = arith.index_cast %arg2 : i32 to index
    %alloca = memref.alloca() : memref<1xf32, 5>
    %1 = gpu.thread_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = arith.muli %7, %c4_i32 : i32
    %9 = arith.addi %8, %2 : i32
    %10 = arith.index_cast %9 : i32 to index
    %11 = arith.index_cast %9 : i32 to index
    %12 = arith.index_cast %9 : i32 to index
    %13 = arith.index_cast %9 : i32 to index
    %14 = gpu.block_dim  x
    %15 = gpu.block_dim  x
    affine.for %arg3 = 0 to 4 {
      affine.if affine_set<(d0)[s0, s1, s2] : (-(d0 * s1) - s0 + s2 - 1 >= 0)>(%arg3)[%10, %14, %0] {
        %18 = affine.load %arg0[%arg3 * symbol(%14) + symbol(%11)] : memref<?xf32>
        affine.store %18, %alloca[%arg3 * symbol(%15) + symbol(%1)] : memref<1xf32, 5>
      }
    }
    nvvm.barrier0
    %16 = gpu.block_dim  x
    %17 = gpu.block_dim  x
    affine.for %arg3 = 0 to 4 {
      affine.if affine_set<(d0)[s0, s1, s2] : (-(d0 * s1) - s0 + s2 - 1 >= 0)>(%arg3)[%12, %16, %0] {
        %18 = affine.load %alloca[%arg3 * symbol(%17) + symbol(%1)] : memref<1xf32, 5>
        %19 = arith.mulf %18, %18 : f32
        %20 = arith.mulf %19, %18 : f32
        %21 = arith.mulf %20, %cst_0 : f32
        %22 = arith.addf %18, %21 : f32
        %23 = arith.mulf %22, %cst : f32
        %24 = math.tanh %23 : f32
        %25 = arith.mulf %18, %cst_1 : f32
        %26 = arith.addf %24, %cst_2 : f32
        %27 = arith.mulf %25, %26 : f32
        affine.store %27, %arg1[%arg3 * symbol(%16) + symbol(%13)] : memref<?xf32>
      }
    }
    return
  }
}
[ict-debug] driver.cc: After return 5, module: end

[ict-debug] driver.cc: After return 6, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 0.797884583 : f32
    %cst_0 = arith.constant 4.471500e-02 : f32
    %c4_i32 = arith.constant 4 : i32
    %cst_1 = arith.constant 5.000000e-01 : f32
    %cst_2 = arith.constant 1.000000e+00 : f32
    %0 = arith.index_cast %arg2 : i32 to index
    %alloca = memref.alloca() : memref<1xf32, 5>
    %1 = gpu.thread_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = arith.muli %7, %c4_i32 : i32
    %9 = arith.addi %8, %2 : i32
    %10 = arith.index_cast %9 : i32 to index
    affine.for %arg3 = 0 to 4 {
      affine.if affine_set<(d0)[s0, s1, s2] : (-(d0 * s0) - s1 + s2 - 1 >= 0)>(%arg3)[%5, %10, %0] {
        %11 = affine.load %arg0[%arg3 * symbol(%5) + symbol(%10)] : memref<?xf32>
        affine.store %11, %alloca[%arg3 * symbol(%5) + symbol(%1)] : memref<1xf32, 5>
      }
    }
    nvvm.barrier0
    affine.for %arg3 = 0 to 4 {
      affine.if affine_set<(d0)[s0, s1, s2] : (-(d0 * s0) - s1 + s2 - 1 >= 0)>(%arg3)[%5, %10, %0] {
        %11 = affine.load %alloca[%arg3 * symbol(%5) + symbol(%1)] : memref<1xf32, 5>
        %12 = arith.mulf %11, %11 : f32
        %13 = arith.mulf %12, %11 : f32
        %14 = arith.mulf %13, %cst_0 : f32
        %15 = arith.addf %11, %14 : f32
        %16 = arith.mulf %15, %cst : f32
        %17 = math.tanh %16 : f32
        %18 = arith.mulf %11, %cst_1 : f32
        %19 = arith.addf %17, %cst_2 : f32
        %20 = arith.mulf %18, %19 : f32
        affine.store %20, %arg1[%arg3 * symbol(%5) + symbol(%10)] : memref<?xf32>
      }
    }
    return
  }
}
[ict-debug] driver.cc: After return 6, module: end

WrapAndReplaceBarrierPass::runOnOperation(): before execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func private @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.797884583 : f32
    %cst_0 = arith.constant 4.471500e-02 : f32
    %c4_i32 = arith.constant 4 : i32
    %cst_1 = arith.constant 5.000000e-01 : f32
    %cst_2 = arith.constant 1.000000e+00 : f32
    %0 = arith.index_cast %arg2 : i32 to index
    %alloca = memref.alloca() : memref<1xf32, 5>
    %1 = gpu.thread_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.block_id  x
    %4 = arith.index_cast %3 : index to i32
    %5 = gpu.block_dim  x
    %6 = arith.index_cast %5 : index to i32
    %7 = arith.muli %4, %6 : i32
    %8 = arith.muli %7, %c4_i32 : i32
    %9 = arith.addi %8, %2 : i32
    %10 = arith.index_cast %9 : i32 to index
    scf.for %arg3 = %c0 to %c4 step %c1 {
      %11 = arith.muli %arg3, %5 : index
      %12 = arith.muli %11, %c-1 : index
      %13 = arith.subi %12, %10 : index
      %14 = arith.addi %13, %0 : index
      %15 = arith.addi %14, %c-1 : index
      %16 = arith.cmpi sge, %15, %c0 : index
      scf.if %16 {
        %17 = arith.addi %11, %10 : index
        %18 = memref.load %arg0[%17] : memref<?xf32>
        %19 = arith.addi %11, %1 : index
        memref.store %18, %alloca[%19] : memref<1xf32, 5>
      }
    }
    nvvm.barrier0
    scf.for %arg3 = %c0 to %c4 step %c1 {
      %11 = arith.muli %arg3, %5 : index
      %12 = arith.muli %11, %c-1 : index
      %13 = arith.subi %12, %10 : index
      %14 = arith.addi %13, %0 : index
      %15 = arith.addi %14, %c-1 : index
      %16 = arith.cmpi sge, %15, %c0 : index
      scf.if %16 {
        %17 = arith.addi %11, %1 : index
        %18 = memref.load %alloca[%17] : memref<1xf32, 5>
        %19 = arith.mulf %18, %18 : f32
        %20 = arith.mulf %19, %18 : f32
        %21 = arith.mulf %20, %cst_0 : f32
        %22 = arith.addf %18, %21 : f32
        %23 = arith.mulf %22, %cst : f32
        %24 = math.tanh %23 : f32
        %25 = arith.mulf %18, %cst_1 : f32
        %26 = arith.addf %24, %cst_2 : f32
        %27 = arith.mulf %25, %26 : f32
        %28 = arith.addi %11, %10 : index
        memref.store %27, %arg1[%28] : memref<?xf32>
      }
    }
    return
  }
}
WrapAndReplaceBarrierPass::runOnOperation(): before execute: end
WrapAndReplaceBarrierPass::runOnOperation(): after execute: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %c-1 = arith.constant -1 : index
      %c1_0 = arith.constant 1 : index
      %c4 = arith.constant 4 : index
      %c0_1 = arith.constant 0 : index
      %cst = arith.constant 0.797884583 : f32
      %cst_2 = arith.constant 4.471500e-02 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_3 = arith.constant 5.000000e-01 : f32
      %cst_4 = arith.constant 1.000000e+00 : f32
      %0 = arith.index_cast %arg2 : i32 to index
      %1 = arith.index_cast %arg3 : index to i32
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %4 = gpu.block_dim  x
      %5 = arith.index_cast %4 : index to i32
      %6 = arith.muli %3, %5 : i32
      %7 = arith.muli %6, %c4_i32 : i32
      %8 = arith.addi %7, %1 : i32
      %9 = arith.index_cast %8 : i32 to index
      scf.for %arg4 = %c0_1 to %c4 step %c1_0 {
        %10 = arith.muli %arg4, %4 : index
        %11 = arith.muli %10, %c-1 : index
        %12 = arith.subi %11, %9 : index
        %13 = arith.addi %12, %0 : index
        %14 = arith.addi %13, %c-1 : index
        %15 = arith.cmpi sge, %14, %c0_1 : index
        scf.if %15 {
          %16 = arith.addi %10, %9 : index
          %17 = memref.load %arg0[%16] : memref<?xf32>
          %18 = arith.addi %10, %arg3 : index
          memref.store %17, %alloca[%18] : memref<1xf32, 5>
        }
      }
      "polygeist.barrier"(%arg3) : (index) -> ()
      scf.for %arg4 = %c0_1 to %c4 step %c1_0 {
        %10 = arith.muli %arg4, %4 : index
        %11 = arith.muli %10, %c-1 : index
        %12 = arith.subi %11, %9 : index
        %13 = arith.addi %12, %0 : index
        %14 = arith.addi %13, %c-1 : index
        %15 = arith.cmpi sge, %14, %c0_1 : index
        scf.if %15 {
          %16 = arith.addi %10, %arg3 : index
          %17 = memref.load %alloca[%16] : memref<1xf32, 5>
          %18 = arith.mulf %17, %17 : f32
          %19 = arith.mulf %18, %17 : f32
          %20 = arith.mulf %19, %cst_2 : f32
          %21 = arith.addf %17, %20 : f32
          %22 = arith.mulf %21, %cst : f32
          %23 = math.tanh %22 : f32
          %24 = arith.mulf %17, %cst_3 : f32
          %25 = arith.addf %23, %cst_4 : f32
          %26 = arith.mulf %24, %25 : f32
          %27 = arith.addi %10, %9 : index
          memref.store %26, %arg1[%27] : memref<?xf32>
        }
      }
      scf.yield
    }
    return
  }
}
WrapAndReplaceBarrierPass::runOnOperation(): after execute: end
[ict-debug] driver.cc: After return 7, module:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %c-1 = arith.constant -1 : index
      %c1_0 = arith.constant 1 : index
      %c4 = arith.constant 4 : index
      %c0_1 = arith.constant 0 : index
      %cst = arith.constant 0.797884583 : f32
      %cst_2 = arith.constant 4.471500e-02 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_3 = arith.constant 5.000000e-01 : f32
      %cst_4 = arith.constant 1.000000e+00 : f32
      %0 = arith.index_cast %arg2 : i32 to index
      %1 = arith.index_cast %arg3 : index to i32
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %4 = gpu.block_dim  x
      %5 = arith.index_cast %4 : index to i32
      %6 = arith.muli %3, %5 : i32
      %7 = arith.muli %6, %c4_i32 : i32
      %8 = arith.addi %7, %1 : i32
      %9 = arith.index_cast %8 : i32 to index
      scf.for %arg4 = %c0_1 to %c4 step %c1_0 {
        %10 = arith.muli %arg4, %4 : index
        %11 = arith.muli %10, %c-1 : index
        %12 = arith.subi %11, %9 : index
        %13 = arith.addi %12, %0 : index
        %14 = arith.addi %13, %c-1 : index
        %15 = arith.cmpi sge, %14, %c0_1 : index
        scf.if %15 {
          %16 = arith.addi %10, %9 : index
          %17 = memref.load %arg0[%16] : memref<?xf32>
          %18 = arith.addi %10, %arg3 : index
          memref.store %17, %alloca[%18] : memref<1xf32, 5>
        }
      }
      "polygeist.barrier"(%arg3) : (index) -> ()
      scf.for %arg4 = %c0_1 to %c4 step %c1_0 {
        %10 = arith.muli %arg4, %4 : index
        %11 = arith.muli %10, %c-1 : index
        %12 = arith.subi %11, %9 : index
        %13 = arith.addi %12, %0 : index
        %14 = arith.addi %13, %c-1 : index
        %15 = arith.cmpi sge, %14, %c0_1 : index
        scf.if %15 {
          %16 = arith.addi %10, %arg3 : index
          %17 = memref.load %alloca[%16] : memref<1xf32, 5>
          %18 = arith.mulf %17, %17 : f32
          %19 = arith.mulf %18, %17 : f32
          %20 = arith.mulf %19, %cst_2 : f32
          %21 = arith.addf %17, %20 : f32
          %22 = arith.mulf %21, %cst : f32
          %23 = math.tanh %22 : f32
          %24 = arith.mulf %17, %cst_3 : f32
          %25 = arith.addf %23, %cst_4 : f32
          %26 = arith.mulf %24, %25 : f32
          %27 = arith.addi %10, %9 : index
          memref.store %26, %arg1[%27] : memref<?xf32>
        }
      }
      scf.yield
    }
    return
  }
}
[ict-debug] driver.cc: After return 7, module: end

[ict-debug] driver.cc: Before my pass process:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  func.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) attributes {llvm.linkage = #llvm.linkage<external>, polygeist.device_only_func = "1"} {
    %cst = arith.constant 1.000000e+00 : f32
    %cst_0 = arith.constant 5.000000e-01 : f32
    %c4_i32 = arith.constant 4 : i32
    %cst_1 = arith.constant 4.471500e-02 : f32
    %cst_2 = arith.constant 0.797884583 : f32
    %c4 = arith.constant 4 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c32 = arith.constant 32 : index
    %alloca = memref.alloca() : memref<1xf32, 5>
    %0 = arith.index_cast %arg2 : i32 to index
    %1 = gpu.block_id  x
    %2 = arith.index_cast %1 : index to i32
    %3 = gpu.block_dim  x
    %4 = arith.index_cast %3 : index to i32
    %5 = arith.muli %2, %4 : i32
    %6 = arith.muli %5, %c4_i32 : i32
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %14 = arith.index_cast %arg3 : index to i32
      %15 = arith.addi %6, %14 : i32
      %16 = arith.index_cast %15 : i32 to index
      scf.for %arg4 = %c0 to %c4 step %c1 {
        %17 = arith.muli %arg4, %3 : index
        %18 = arith.muli %17, %c-1 : index
        %19 = arith.subi %18, %16 : index
        %20 = arith.addi %19, %0 : index
        %21 = arith.addi %20, %c-1 : index
        %22 = arith.cmpi sge, %21, %c0 : index
        scf.if %22 {
          %23 = arith.addi %17, %16 : index
          %24 = memref.load %arg0[%23] : memref<?xf32>
          %25 = arith.addi %17, %arg3 : index
          memref.store %24, %alloca[%25] : memref<1xf32, 5>
        }
      }
      scf.yield
    }
    %7 = gpu.block_id  x
    %8 = arith.index_cast %7 : index to i32
    %9 = gpu.block_dim  x
    %10 = arith.index_cast %9 : index to i32
    %11 = arith.muli %8, %10 : i32
    %12 = arith.muli %11, %c4_i32 : i32
    %13 = arith.index_cast %arg2 : i32 to index
    scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
      %14 = arith.index_cast %arg3 : index to i32
      %15 = arith.addi %12, %14 : i32
      %16 = arith.index_cast %15 : i32 to index
      scf.for %arg4 = %c0 to %c4 step %c1 {
        %17 = arith.muli %arg4, %9 : index
        %18 = arith.muli %17, %c-1 : index
        %19 = arith.subi %18, %16 : index
        %20 = arith.addi %19, %13 : index
        %21 = arith.addi %20, %c-1 : index
        %22 = arith.cmpi sge, %21, %c0 : index
        scf.if %22 {
          %23 = arith.addi %17, %arg3 : index
          %24 = memref.load %alloca[%23] : memref<1xf32, 5>
          %25 = arith.mulf %24, %24 : f32
          %26 = arith.mulf %25, %24 : f32
          %27 = arith.mulf %26, %cst_1 : f32
          %28 = arith.addf %24, %27 : f32
          %29 = arith.mulf %28, %cst_2 : f32
          %30 = math.tanh %29 : f32
          %31 = arith.mulf %24, %cst_0 : f32
          %32 = arith.addf %30, %cst : f32
          %33 = arith.mulf %31, %32 : f32
          %34 = arith.addi %17, %16 : index
          memref.store %33, %arg1[%34] : memref<?xf32>
        }
      }
      scf.yield
    }
    return
  }
}
[ict-debug] driver.cc: Before my pass process: end

[ict-debug] driver.cc: vectorizeSize = 1

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<1xf32, 5>
      %0 = arith.index_cast %arg2 : i32 to index
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %3 = gpu.block_dim  x
      %4 = arith.index_cast %3 : index to i32
      %5 = arith.muli %2, %4 : i32
      %6 = arith.muli %5, %c4_i32 : i32
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %7 = arith.index_cast %arg3 : index to i32
        %8 = arith.addi %6, %7 : i32
        %9 = arith.index_cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %3 : index
          %11 = arith.muli %10, %c-1 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %0 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %9 : index
            %17 = memref.load %arg0[%16] : memref<?xf32>
            %18 = arith.addi %10, %arg3 : index
            memref.store %17, %alloca[%18] : memref<1xf32, 5>
          }
        }
        scf.yield
      }
      scf.parallel (%arg3) = (%c0) to (%c32) step (%c1) {
        %7 = arith.index_cast %arg3 : index to i32
        %8 = arith.addi %6, %7 : i32
        %9 = arith.index_cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %3 : index
          %11 = arith.muli %10, %c-1 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %0 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %arg3 : index
            %17 = memref.load %alloca[%16] : memref<1xf32, 5>
            %18 = arith.mulf %17, %17 : f32
            %19 = arith.mulf %18, %17 : f32
            %20 = arith.mulf %19, %cst_1 : f32
            %21 = arith.addf %17, %20 : f32
            %22 = arith.mulf %21, %cst_2 : f32
            %23 = math.tanh %22 : f32
            %24 = arith.mulf %17, %cst_0 : f32
            %25 = arith.addf %23, %cst : f32
            %26 = arith.mulf %24, %25 : f32
            %27 = arith.addi %10, %9 : index
            memref.store %26, %arg1[%27] : memref<?xf32>
          }
        }
        scf.yield
      }
      gpu.return
    }
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): Before execute: end

[ict-debug] ConvertPolygeistToNPU:convertScfParallelToScfFor(): replace gpu.block_dim op with thread loop bound

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %alloca = memref.alloca() : memref<1xf32, 5>
      %0 = arith.index_cast %arg2 : i32 to index
      %1 = gpu.block_id  x
      %2 = arith.index_cast %1 : index to i32
      %c32_3 = arith.constant 32 : index
      %3 = arith.index_cast %c32_3 : index to i32
      %4 = arith.muli %2, %3 : i32
      %5 = arith.muli %4, %c4_i32 : i32
      %c1_4 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_4 {
        %6 = arith.index_cast %arg3 : index to i32
        %7 = arith.addi %5, %6 : i32
        %8 = arith.index_cast %7 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %9 = arith.muli %arg4, %c32_3 : index
          %10 = arith.muli %9, %c-1 : index
          %11 = arith.subi %10, %8 : index
          %12 = arith.addi %11, %0 : index
          %13 = arith.addi %12, %c-1 : index
          %14 = arith.cmpi sge, %13, %c0 : index
          scf.if %14 {
            %15 = arith.addi %9, %8 : index
            %16 = memref.load %arg0[%15] : memref<?xf32>
            %17 = arith.addi %9, %arg3 : index
            memref.store %16, %alloca[%17] : memref<1xf32, 5>
          }
        }
      }
      %c1_5 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_5 {
        %6 = arith.index_cast %arg3 : index to i32
        %7 = arith.addi %5, %6 : i32
        %8 = arith.index_cast %7 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %9 = arith.muli %arg4, %c32_3 : index
          %10 = arith.muli %9, %c-1 : index
          %11 = arith.subi %10, %8 : index
          %12 = arith.addi %11, %0 : index
          %13 = arith.addi %12, %c-1 : index
          %14 = arith.cmpi sge, %13, %c0 : index
          scf.if %14 {
            %15 = arith.addi %9, %arg3 : index
            %16 = memref.load %alloca[%15] : memref<1xf32, 5>
            %17 = arith.mulf %16, %16 : f32
            %18 = arith.mulf %17, %16 : f32
            %19 = arith.mulf %18, %cst_1 : f32
            %20 = arith.addf %16, %19 : f32
            %21 = arith.mulf %20, %cst_2 : f32
            %22 = math.tanh %21 : f32
            %23 = arith.mulf %16, %cst_0 : f32
            %24 = arith.addf %22, %cst : f32
            %25 = arith.mulf %23, %24 : f32
            %26 = arith.addi %9, %8 : index
            memref.store %25, %arg1[%26] : memref<?xf32>
          }
        }
      }
      gpu.return
    }
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After vectorize: end

[ict-debug] MemRefAllocaToNPULowering: process op: 

%alloca = memref.alloca() : memref<1xf32, 5>
[ict-debug] MemRefAllocaToNPULowering: memory space is 5

MemRefAllocaToNPULowering: newAllocaOp: 
%0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
MemRefAllocaToNPULowering: old allocaOp: 
%alloca = memref.alloca() : memref<1xf32, 5>
MemRefAllocaToNPULowering: module: 
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %alloca = memref.alloca() : memref<1xf32, 5>
      %1 = arith.index_cast %arg2 : i32 to index
      %2 = gpu.block_id  x
      %3 = arith.index_cast %2 : index to i32
      %c32_3 = arith.constant 32 : index
      %4 = arith.index_cast %c32_3 : index to i32
      %5 = arith.muli %3, %4 : i32
      %6 = arith.muli %5, %c4_i32 : i32
      %c1_4 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_4 {
        %7 = arith.index_cast %arg3 : index to i32
        %8 = arith.addi %6, %7 : i32
        %9 = arith.index_cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32_3 : index
          %11 = arith.muli %10, %c-1 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %1 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %9 : index
            %17 = memref.load %arg0[%16] : memref<?xf32>
            %18 = arith.addi %10, %arg3 : index
            memref.store %17, %alloca[%18] : memref<1xf32, 5>
          }
        }
      }
      %c1_5 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_5 {
        %7 = arith.index_cast %arg3 : index to i32
        %8 = arith.addi %6, %7 : i32
        %9 = arith.index_cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32_3 : index
          %11 = arith.muli %10, %c-1 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %1 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %arg3 : index
            %17 = memref.load %alloca[%16] : memref<1xf32, 5>
            %18 = arith.mulf %17, %17 : f32
            %19 = arith.mulf %18, %17 : f32
            %20 = arith.mulf %19, %cst_1 : f32
            %21 = arith.addf %17, %20 : f32
            %22 = arith.mulf %21, %cst_2 : f32
            %23 = math.tanh %22 : f32
            %24 = arith.mulf %17, %cst_0 : f32
            %25 = arith.addf %23, %cst : f32
            %26 = arith.mulf %24, %25 : f32
            %27 = arith.addi %10, %9 : index
            memref.store %26, %arg1[%27] : memref<?xf32>
          }
        }
      }
      gpu.return
    }
  }
}
MemRefAllocaToNPULowering: module: end
[ict-debug] CastLikeOpToNPULowering: process op: 

%1 = arith.index_cast %arg2 : i32 to index
[ict-debug] GPUBlockIdToNPULowering: process op: 

%3 = gpu.block_id  x
[ict-debug] CastLikeOpToNPULowering: process op: 

%5 = arith.index_cast %4 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%11 = arith.index_cast %arg3 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%14 = arith.index_cast %13 : i32 to index
[ict-debug] CastLikeOpToNPULowering: process op: 

%11 = arith.index_cast %arg3 : index to i32
[ict-debug] CastLikeOpToNPULowering: process op: 

%14 = arith.index_cast %13 : i32 to index
[ict-debug] ArithUnaryOpToNPULowering: process op: 

%34 = math.tanh %33 : f32
[ict-debug] ArithUnaryOpToNPULowering: met scalar unary op, need vector help process.

[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %c4_i32 = arith.constant 4 : i32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = emitc.cast %arg2 : i32 to index
      %3 = "npu.block_id"() : () -> i64
      %4 = emitc.cast %3 : i64 to i32
      %c32_3 = arith.constant 32 : index
      %c32_i32 = arith.constant 32 : i32
      %5 = arith.muli %4, %c32_i32 : i32
      %6 = arith.muli %5, %c4_i32 : i32
      %c1_4 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_4 {
        %7 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %8 = emitc.cast %7 : i64 to i32
        %9 = arith.addi %6, %8 : i32
        %10 = emitc.cast %9 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %11 = arith.muli %arg4, %c32_3 : index
          %12 = arith.muli %11, %c-1 : index
          %13 = arith.subi %12, %10 : index
          %14 = arith.addi %13, %2 : index
          %15 = arith.addi %14, %c-1 : index
          %16 = arith.cmpi sge, %15, %c0 : index
          scf.if %16 {
            %17 = arith.addi %11, %10 : index
            %18 = memref.load %arg0[%17] : memref<?xf32>
            %19 = arith.addi %11, %arg3 : index
            memref.store %18, %1[%19] : memref<1xf32, 5>
          }
        }
      }
      %c1_5 = arith.constant 1 : index
      scf.for %arg3 = %c0 to %c32 step %c1_5 {
        %7 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %8 = emitc.cast %7 : i64 to i32
        %9 = arith.addi %6, %8 : i32
        %10 = emitc.cast %9 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %11 = arith.muli %arg4, %c32_3 : index
          %12 = arith.muli %11, %c-1 : index
          %13 = arith.subi %12, %10 : index
          %14 = arith.addi %13, %2 : index
          %15 = arith.addi %14, %c-1 : index
          %16 = arith.cmpi sge, %15, %c0 : index
          scf.if %16 {
            %17 = arith.addi %11, %arg3 : index
            %18 = memref.load %1[%17] : memref<1xf32, 5>
            %19 = emitc.mul %18, %18 : (f32, f32) -> f32
            %20 = emitc.mul %19, %18 : (f32, f32) -> f32
            %21 = emitc.mul %20, %cst_1 : (f32, f32) -> f32
            %22 = emitc.add %18, %21 : (f32, f32) -> f32
            %23 = emitc.mul %22, %cst_2 : (f32, f32) -> f32
            %24 = emitc.call "tanhf"(%23) : (f32) -> f32
            %25 = emitc.mul %18, %cst_0 : (f32, f32) -> f32
            %26 = emitc.add %24, %cst : (f32, f32) -> f32
            %27 = emitc.mul %25, %26 : (f32, f32) -> f32
            %28 = arith.addi %11, %10 : index
            memref.store %27, %arg1[%28] : memref<?xf32>
          }
        }
      }
      gpu.return
    }
  }
}
[ict-debug] ConvertPolygeistToNPUPass::runOnOperation(): After convert to NPU: end

[ict-debug] driver.cc: Before convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %c128_i32 = arith.constant 128 : i32
      %c-32 = arith.constant -32 : index
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = emitc.cast %arg2 : i32 to index
      %3 = "npu.block_id"() : () -> i64
      %4 = emitc.cast %3 : i64 to i32
      %5 = arith.muli %4, %c128_i32 : i32
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %6 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %7 = emitc.cast %6 : i64 to i32
        %8 = arith.addi %5, %7 : i32
        %9 = emitc.cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32 : index
          %11 = arith.muli %arg4, %c-32 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %2 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %9 : index
            %17 = memref.load %arg0[%16] : memref<?xf32>
            %18 = arith.addi %10, %arg3 : index
            memref.store %17, %1[%18] : memref<1xf32, 5>
          }
        }
      }
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %6 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %7 = emitc.cast %6 : i64 to i32
        %8 = arith.addi %5, %7 : i32
        %9 = emitc.cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32 : index
          %11 = arith.muli %arg4, %c-32 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %2 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          scf.if %15 {
            %16 = arith.addi %10, %arg3 : index
            %17 = memref.load %1[%16] : memref<1xf32, 5>
            %18 = emitc.mul %17, %17 : (f32, f32) -> f32
            %19 = emitc.mul %18, %17 : (f32, f32) -> f32
            %20 = emitc.mul %19, %cst_1 : (f32, f32) -> f32
            %21 = emitc.add %17, %20 : (f32, f32) -> f32
            %22 = emitc.mul %21, %cst_2 : (f32, f32) -> f32
            %23 = emitc.call "tanhf"(%22) : (f32) -> f32
            %24 = emitc.mul %17, %cst_0 : (f32, f32) -> f32
            %25 = emitc.add %23, %cst : (f32, f32) -> f32
            %26 = emitc.mul %24, %25 : (f32, f32) -> f32
            %27 = arith.addi %10, %9 : index
            memref.store %26, %arg1[%27] : memref<?xf32>
          }
        }
      }
      gpu.return
    }
  }
}
[ict-debug] driver.cc: Before convert to EmitC dialect: end

[ict-debug] driver.cc: After convert to EmitC dialect:

module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi32>>, #dlti.dl_entry<f128, dense<128> : vector<2xi32>>, #dlti.dl_entry<f64, dense<64> : vector<2xi32>>, #dlti.dl_entry<f16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i32, dense<32> : vector<2xi32>>, #dlti.dl_entry<i16, dense<16> : vector<2xi32>>, #dlti.dl_entry<i8, dense<8> : vector<2xi32>>, #dlti.dl_entry<i1, dense<8> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi32>>, #dlti.dl_entry<f80, dense<128> : vector<2xi32>>, #dlti.dl_entry<i64, dense<64> : vector<2xi32>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi32>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi32>>, #dlti.dl_entry<"dlti.endianness", "little">, #dlti.dl_entry<"dlti.stack_alignment", 128 : i32>>, llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", polygeist.gpu_module.llvm.data_layout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64", polygeist.gpu_module.llvm.target_triple = "nvptx64-nvidia-cuda", "polygeist.target-cpu" = "x86-64", "polygeist.target-features" = "+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87", "polygeist.tune-cpu" = "generic"} {
  gpu.module @_Z18gelu_kernel_sharedPKfPfi_0 {
    gpu.func @_Z18gelu_kernel_sharedPKfPfi(%arg0: memref<?xf32>, %arg1: memref<?xf32>, %arg2: i32) {
      %c128_i32 = arith.constant 128 : i32
      %c-32 = arith.constant -32 : index
      %cst = arith.constant 1.000000e+00 : f32
      %cst_0 = arith.constant 5.000000e-01 : f32
      %cst_1 = arith.constant 4.471500e-02 : f32
      %cst_2 = arith.constant 0.797884583 : f32
      %c4 = arith.constant 4 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c32 = arith.constant 32 : index
      %0 = "npu.alloca"() <{numElems = 32 : i32}> : () -> !llvm.ptr<6>
      %1 = builtin.unrealized_conversion_cast %0 : !llvm.ptr<6> to memref<1xf32, 5>
      %2 = emitc.cast %arg2 : i32 to index
      %3 = "npu.block_id"() : () -> i64
      %4 = emitc.cast %3 : i64 to i32
      %5 = arith.muli %4, %c128_i32 : i32
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %6 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %7 = emitc.cast %6 : i64 to i32
        %8 = arith.addi %5, %7 : i32
        %9 = emitc.cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32 : index
          %11 = arith.muli %arg4, %c-32 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %2 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          emitc.if %15 {
            %16 = arith.addi %10, %9 : index
            %17 = memref.load %arg0[%16] : memref<?xf32>
            %18 = arith.addi %10, %arg3 : index
            memref.store %17, %1[%18] : memref<1xf32, 5>
          }
        }
      }
      scf.for %arg3 = %c0 to %c32 step %c1 {
        %6 = builtin.unrealized_conversion_cast %arg3 : index to i64
        %7 = emitc.cast %6 : i64 to i32
        %8 = arith.addi %5, %7 : i32
        %9 = emitc.cast %8 : i32 to index
        scf.for %arg4 = %c0 to %c4 step %c1 {
          %10 = arith.muli %arg4, %c32 : index
          %11 = arith.muli %arg4, %c-32 : index
          %12 = arith.subi %11, %9 : index
          %13 = arith.addi %12, %2 : index
          %14 = arith.addi %13, %c-1 : index
          %15 = arith.cmpi sge, %14, %c0 : index
          emitc.if %15 {
            %16 = arith.addi %10, %arg3 : index
            %17 = memref.load %1[%16] : memref<1xf32, 5>
            %18 = emitc.mul %17, %17 : (f32, f32) -> f32
            %19 = emitc.mul %18, %17 : (f32, f32) -> f32
            %20 = emitc.mul %19, %cst_1 : (f32, f32) -> f32
            %21 = emitc.add %17, %20 : (f32, f32) -> f32
            %22 = emitc.mul %21, %cst_2 : (f32, f32) -> f32
            %23 = emitc.call "tanhf"(%22) : (f32) -> f32
            %24 = emitc.mul %17, %cst_0 : (f32, f32) -> f32
            %25 = emitc.add %23, %cst : (f32, f32) -> f32
            %26 = emitc.mul %24, %25 : (f32, f32) -> f32
            %27 = arith.addi %10, %9 : index
            memref.store %26, %arg1[%27] : memref<?xf32>
          }
        }
      }
      gpu.return
    }
  }
}
[ict-debug] driver.cc: After convert to EmitC dialect: end

[ict-debug] driver.cc: After emitc::translateToCpp:

